<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/"><channel><title>Publications on RL4AA Collaboration | Homepage</title><link>https://RL4aa.github.io/categories/publications/</link><description>Recent content in Publications on RL4AA Collaboration | Homepage</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Tue, 06 Jun 2023 00:00:00 +0000</lastBuildDate><atom:link href="https://RL4aa.github.io/categories/publications/index.xml" rel="self" type="application/rss+xml"/><item><title>Learning to Do or Learning While Doing: Reinforcement Learning and Bayesian Optimisation for Online Continuous Tuning</title><link>https://RL4aa.github.io/posts/publications/kaiser2023learning/</link><pubDate>Tue, 06 Jun 2023 00:00:00 +0000</pubDate><guid>https://RL4aa.github.io/posts/publications/kaiser2023learning/</guid><description>J. Kaiser1, C. Xu2, A. Eichler1, A. Santamaria Garcia2, O. Stein1, E. Bründermann2, W. Kuropka1, H. Dinter1, F. Mayet1, T. Vinatier1, F. Burkart1, H. Schlarb1
1Deutsches Elektronen-Synchrotron DESY, 2 Karlsruhe Institute of Technology KIT
arXiv
Abstract Online tuning of real-world plants is a complex optimisation problem that continues to require manual intervention by experienced human operators. Autonomous tuning is a rapidly expanding field of research, where learning-based methods, such as Reinforcement Learning-trained Optimisation (RLO) and Bayesian optimisation (BO), hold great promise for achieving outstanding plant performance and reducing tuning times.</description></item><item><title>Trend-Based SAC Beam Control Method with Zero-Shot in Superconducting Linear Accelerator</title><link>https://RL4aa.github.io/posts/publications/chen2023trendbased/</link><pubDate>Tue, 23 May 2023 00:00:00 +0000</pubDate><guid>https://RL4aa.github.io/posts/publications/chen2023trendbased/</guid><description>X. Chen, X. Qi, C. Su, Y. He, Z. Wang, K. Sun, C. Jin, W. Chen, S. Liu, X. Zhao, D. Jia, M. Yi
Chinese Academy of Sciences
arXiv
Abstract The superconducting linear accelerator is a highly flexiable facility for modern scientific discoveries, necessitating weekly reconfiguration and tuning. Accordingly, minimizing setup time proves essential in affording users with ample experimental time. We propose a trend-based soft actor-critic(TBSAC) beam control method with strong robustness, allowing the agents to be trained in a simulated environment and applied to the real accelerator directly with zero-shot.</description></item><item><title>Towards automatic setup of 18 MeV electron beamline using machine learning</title><link>https://RL4aa.github.io/posts/publications/velotti2023towards/</link><pubDate>Thu, 27 Apr 2023 00:00:00 +0000</pubDate><guid>https://RL4aa.github.io/posts/publications/velotti2023towards/</guid><description>F. M. Velotti1, B. Goddard1, V. Kain1, R. Ramjiawan1, G. Z. Della Porta1 and S. Hirlaender2
1CERN, 2University of Salzburg
Machine Learning: Science and Technology
Abstract To improve the performance-critical stability and brightness of the electron bunch at injection into the proton-driven plasma wakefield at the AWAKE CERN experiment, automation approaches based on unsupervised machine learning (ML) were developed and deployed. Numerical optimisers were tested together with different model-free reinforcement learning (RL) agents.</description></item><item><title>Orbit Correction Based on Improved Reinforcement Learning Algorithm</title><link>https://RL4aa.github.io/posts/publications/chen2023orbit/</link><pubDate>Thu, 13 Apr 2023 00:00:00 +0000</pubDate><guid>https://RL4aa.github.io/posts/publications/chen2023orbit/</guid><description>X. Chen, Y. Jia, X. Qi, Z. Wang, Y. He
Chinese Academy of Sciences
Physical Review Accelerators and Beams
Abstract Recently, reinforcement learning (RL) algorithms have been applied to a wide range of control problems in accelerator commissioning. In order to achieve efficient and fast control, these algorithms need to be highly efficient, so as to minimize the online training time. In this paper, we incorporated the beam position monitor trend into the observation space of the twin delayed deep deterministic policy gradient (TD3) algorithm and trained two different structure agents, one based on physical prior knowledge and the other using the original TD3 network architecture.</description></item><item><title>Optimizing a superconducting radio-frequency gun using deep reinforcement learning</title><link>https://RL4aa.github.io/posts/publications/meier2022optimizing/</link><pubDate>Fri, 28 Oct 2022 00:00:00 +0000</pubDate><guid>https://RL4aa.github.io/posts/publications/meier2022optimizing/</guid><description>D. Meier1, L. V. Ramirez1, J. Völker1, J. Viefhaus1, B. Sick2, G. Hartmann1
1Helmholtz-Zentrum Berlin, 2University of Kassel
Physical Review Accelerators and Beams
Abstract Superconducting photoelectron injectors are promising for generating highly brilliant pulsed electron beams with high repetition rates and low emittances. Experiments such as ultrafast electron diffraction, experiments at the Terahertz scale, and energy recovery linac applications require such properties. However, optimizing the beam properties is challenging due to the high number of possible machine parameter combinations.</description></item><item><title>Application of reinforcement learning in the LHC tune feedback</title><link>https://RL4aa.github.io/posts/publications/grech2022application/</link><pubDate>Wed, 07 Sep 2022 00:00:00 +0000</pubDate><guid>https://RL4aa.github.io/posts/publications/grech2022application/</guid><description>L. Grech1, G. Valentino1, D. Alves2 and Simon Hirlaender3
1University of Malta, 2CERN, 3University of Salzburg
Frontiers in Physics
Abstract The Beam-Based Feedback System (BBFS) was primarily responsible for correcting the beam energy, orbit and tune in the CERN Large Hadron Collider (LHC). A major code renovation of the BBFS was planned and carried out during the LHC Long Shutdown 2 (LS2). This work consists of an explorative study to solve a beam-based control problem, the tune feedback (QFB), utilising state-of-the-art Reinforcement Learning (RL).</description></item><item><title>Learning-based Optimisation of Particle Accelerators Under Partial Observability Without Real-World Training</title><link>https://RL4aa.github.io/posts/publications/kaiser2022learningbased/</link><pubDate>Fri, 22 Jul 2022 00:00:00 +0000</pubDate><guid>https://RL4aa.github.io/posts/publications/kaiser2022learningbased/</guid><description>J. Kaiser, O. Stein, A. Eichler
Deutsches Elektronen-Synchrotron DESY
39th International Conference on Machine Learning
Abstract In recent work, it has been shown that reinforcement learning (RL) is capable of solving a variety of problems at sometimes super-human performance levels. But despite continued advances in the field, applying RL to complex real-world control and optimisation problems has proven difficult. In this contribution, we demonstrate how to successfully apply RL to the optimisation of a highly complex real-world machine – specifically a linear particle accelerator – in an only partially observable setting and without requiring training on the real machine.</description></item><item><title>Automated Intensity Optimisation Using Reinforcement Learning at LEIR</title><link>https://RL4aa.github.io/posts/publications/madysa2022automated/</link><pubDate>Sun, 12 Jun 2022 00:00:00 +0000</pubDate><guid>https://RL4aa.github.io/posts/publications/madysa2022automated/</guid><description>N. Madysa, V. Kain, R. Alemany Fernandez, N. Biancacci, B. Goddard, F. M. Velotti
CERN
13th Particle Accelerator Conference
Abstract High intensities in the Low Energy Ion Ring (LEIR) at CERN are achieved by stacking several multi-turn injec- tions from the pre-accelerator Linac3. Up to seven consec- utive 200 μs long, 200 ms spaced pulses are injected from Linac3 into LEIR. Two inclined septa, one magnetic and one electrostatic, combined with a collapsing horizontal or- bit bump allows a 6-D phase space painting via a linearly ramped mean momentum along the Linac3 pulse and in- jection at high dispersion.</description></item><item><title>Real-time artificial intelligence for accelerator control: A study at the Fermilab Booster</title><link>https://RL4aa.github.io/posts/publications/stjohn2021realtime/</link><pubDate>Mon, 18 Oct 2021 00:00:00 +0000</pubDate><guid>https://RL4aa.github.io/posts/publications/stjohn2021realtime/</guid><description>J. St. John1, C. Herwig1, D. Kafkes1, J. Mitrevski1, W. A. Pellico1, G. N. Perdue1, A. Quintero-Parra1, B. A. Schupbach1, K. Seiya1, N. Tran1, M. Schram2, J. M. Duarte3, Y. Huang4, R. Keller5
1Fermi National Accelerator Laboratory, 2Thomas Jefferson National Accelerator Laboratory, 3University of California San Diego, 4Pacific Northwest National Laboratory, 5Columbia University
Physical Review Accelerators and Beams
Abstract We describe a method for precisely regulating the gradient magnet power supply (GMPS) at the Fermilab Booster accelerator complex using a neural network trained via reinforcement learning.</description></item><item><title>Test of Machine Learning at the CERN LINAC4</title><link>https://RL4aa.github.io/posts/publications/kain2021test/</link><pubDate>Mon, 04 Oct 2021 00:00:00 +0000</pubDate><guid>https://RL4aa.github.io/posts/publications/kain2021test/</guid><description>V. Kain1, N. Bruchon1, S. Hirlander1, N. Madysa1, I. Vojskovic1, P. Skowronski1, G. Valentino2
1CERN, 2University of Malta
61st ICFA ABDW on High-Intensity and High-Brightness Hadron Beams
Abstract The CERN H−linear accelerator, LINAC4, served as atest bed for advanced algorithms during the CERN LongShutdown 2 in the years 2019/20. One of the main goals wasto show that reinforcement learning with all its benefits canbe used as a replacement for numerical optimization and asa complement to classical control in the accelerator controlcontext.</description></item><item><title>Renovation of the beam-based feedback systems in the LHC</title><link>https://RL4aa.github.io/posts/publications/grech2021renovation/</link><pubDate>Wed, 01 Sep 2021 00:00:00 +0000</pubDate><guid>https://RL4aa.github.io/posts/publications/grech2021renovation/</guid><description>L. Grech
University of Malta
PhD thesis
Abstract The Large Hadron Collider (LHC) at the European Organization for Nuclear Research (CERN) is the largest synchrotron built to date, having a circumference of approx- imately 27km. The LHC is able to accelerate two counter-rotating proton and/or heavy-ion beams up to 7 TeV per charge. These highly energetic beams are contained inside a vacuum chamber with an inner diameter of 80 mm by means of strong mag- netic fields produced by superconducting magnets.</description></item><item><title>First Steps Toward an Autonomous Accelerator, A Common Project Between DESY and KIT</title><link>https://RL4aa.github.io/posts/publications/eichler2021first/</link><pubDate>Mon, 24 May 2021 00:00:00 +0000</pubDate><guid>https://RL4aa.github.io/posts/publications/eichler2021first/</guid><description>A. Eichler1, F. Burkart1, J. Kaiser1, W. Kuropka1, O. Stein1, E. Bründermann2, A. Santamaria Garcia2, C. Xu2
1Deutsches Elektronen-Synchrotron DESY, 2Karlsruhe Institute of Technology KIT
12th International Particle Accelerator Conference
Abstract Reinforcement learning algorithms have risen in pop-ularity in the accelerator physics community in recentyears, showing potential in beam control and in the opti-mization and automation of tasks in accelerator operation.The Helmholtz AI project “Machine Learning Toward Au-tonomous Accelerators” is a collaboration between DESYand KIT that works on investigating and developing rein-forcement learning applications for the automatic start-upof electron linear accelerators.</description></item><item><title>Physics-Enhanced Reinforcement Learning for Optimal Control</title><link>https://RL4aa.github.io/posts/publications/ivanov2021physicsenhanced/</link><pubDate>Fri, 21 May 2021 00:00:00 +0000</pubDate><guid>https://RL4aa.github.io/posts/publications/ivanov2021physicsenhanced/</guid><description>A. Ivanov, I. Agapov, A. Eichler, S. Tomin
Deutsches Elektronen Synchrotron DESY
12th International Particle Accelerator Conference
Abstract We propose an approach for incorporating acceleratorphysics models into reinforcement learning agents. The proposed approach is based on the Taylor mapping technique for the simulation of particle dynamics. The resulting computational graph is represented as a polynomial neural network and embedded into the traditional reinforcement learning agents. The application of the model is demonstrated in a nonlinear simulation model of beam transmission.</description></item><item><title>Feasibility Investigation on Several Reinforcement Learning Techniques to Improve the Performance of the FERMI Free-Electron Laser</title><link>https://RL4aa.github.io/posts/publications/bruchon2021feasiblity/</link><pubDate>Thu, 18 Mar 2021 00:00:00 +0000</pubDate><guid>https://RL4aa.github.io/posts/publications/bruchon2021feasiblity/</guid><description>N. Bruchon
University of Trieste
PhD thesis
Abstract The research carried out in particle accelerator facilities does not concern only particle and condensed matter physics, although these are the main topics covered in the field. Indeed, since a particle accelerator is composed of many different sub-systems, its proper functioning depends both on each of these parts and their interconnection. It follows that the study, implementation, and improvement of the various sub-systems are fundamental points of investigation too.</description></item><item><title>Policy gradient methods for free-electron laser and terahertz source optimization and stabilization at the FERMI free-electron laser at Elettra</title><link>https://RL4aa.github.io/posts/publications/oshea202policy/</link><pubDate>Mon, 21 Dec 2020 00:00:00 +0000</pubDate><guid>https://RL4aa.github.io/posts/publications/oshea202policy/</guid><description>F. H. O&amp;rsquo;Shea1, N. Bruchon2, G. Gaio1
1Elettra Sincrotrone Trieste, 2University of Trieste
Physical Review Accelerators and Beams
Abstract In this article we report on the application of a model-free reinforcement learning method to the optimization of accelerator systems. We simplify a policy gradient algorithm to accelerator control from sophisticated algorithms that have recently been demonstrated to solve complex dynamic problems. After outlining a theoretical basis for the functioning of the algorithm, we explore the small hyperparameter space to develop intuition about said parameters using a simple number-guess environment.</description></item><item><title>Model-free and Bayesian Ensembling Model-based Deep Reinforcement Learning for Particle Accelerator Control Demonstrated on the FERMI FEL</title><link>https://RL4aa.github.io/posts/publications/hirlaender2020modelfree/</link><pubDate>Thu, 17 Dec 2020 00:00:00 +0000</pubDate><guid>https://RL4aa.github.io/posts/publications/hirlaender2020modelfree/</guid><description>S. Hirlaender, N. Bruchon
University of Salzburg, University of Trieste
arXiv
Abstract Reinforcement learning holds tremendous promise in accelerator controls. The primary goal of this paper is to show how this approach can be utilised on an operational level on accelerator physics problems. Despite the success of model-free reinforcement learning in several domains, sample-efficiency still is a bottle-neck, which might be encompassed by model-based methods. We compare well-suited purely model-based to model-free reinforcement learning applied to the intensity optimisation on the FERMI FEL system.</description></item><item><title>Autonomous Control of a Particle Accelerator using Deep Reinforcement Learning</title><link>https://RL4aa.github.io/posts/publications/pang2020autonomous/</link><pubDate>Sat, 12 Dec 2020 00:00:00 +0000</pubDate><guid>https://RL4aa.github.io/posts/publications/pang2020autonomous/</guid><description>X. Pang1, S. Thulasidasan2, L. Rybarcyk2
1Apple, 2Los Alamos National Laboratory
Machine Learning for Engineering Modeling, Simulation, and Design Workshop at Neural Information Processing Systems 2020
Abstract We describe an approach to learning optimal control policies for a large, linear particle accelerator using deep reinforcement learning coupled with a high-fidelity physics engine. The framework consists of an AI controller that uses deep neural networks for state and action-space representation and learns optimal policies using reward signals that are provided by the physics simulator.</description></item><item><title>Sample-efficient reinforcement learning for CERN accelerator control</title><link>https://RL4aa.github.io/posts/publications/kain2020sampleefficient/</link><pubDate>Tue, 01 Dec 2020 00:00:00 +0000</pubDate><guid>https://RL4aa.github.io/posts/publications/kain2020sampleefficient/</guid><description>V. Kain1, S. Hirlander1, B. Goddard1, F. M. Velotti1, G. Z. Della Porta1, N. Bruchon2, G. Valentino3
1CERN, 2University of Trieste, 3University of Malta
Physical Review Accelerators and Beams
Abstract Numerical optimization algorithms are already established tools to increase and stabilize the performance of particle accelerators. These algorithms have many advantages, are available out of the box, and can be adapted to a wide range of optimization problems in accelerator operation.</description></item><item><title>Basic Reinforcement Learning Techniques to Control the Intensity of a Seeded Free-Electron Laser</title><link>https://RL4aa.github.io/posts/publications/bruchon2020basic/</link><pubDate>Sat, 09 May 2020 00:00:00 +0000</pubDate><guid>https://RL4aa.github.io/posts/publications/bruchon2020basic/</guid><description>N. Bruchon1, G. Fenu1, G. Gaio2, M. Lonza2, F. H. O’Shea2, F. A. Pellegrino1, E. Salvato1
1University of Trieste, 2Elettra Sincrotrone Trieste
Electronics
Abstract Optimal tuning of particle accelerators is a challenging task. Many different approaches have been proposed in the past to solve two main problems—attainment of an optimal working point and performance recovery after machine drifts. The most classical model-free techniques (e.g., Gradient Ascent or Extremum Seeking algorithms) have some intrinsic limitations.</description></item><item><title>Toward the Application of Reinforcement Learning to the Intensity Control of a Seeded Free-Electron Laser</title><link>https://RL4aa.github.io/posts/publications/bruchon2019toward/</link><pubDate>Wed, 23 Oct 2019 00:00:00 +0000</pubDate><guid>https://RL4aa.github.io/posts/publications/bruchon2019toward/</guid><description>N. Bruchon, G. Fenu, G. Gaio, M. Lonza, F. A. Pellegrino, E. Salvato
University of Trieste
23rd International Conference on Mechatronics Technology
Abstract The optimization of particle accelerators is a challenging task, and many different approaches have been proposed in years, to obtain an optimal tuning of the plant and to keep it optimally tuned despite drifts or disturbances. Indeed, the classical model-free approaches (such as Gradient Ascent or Extremum Seeking algorithms) have intrinsic limitations.</description></item><item><title>Feedback Design for Control of the Micro-Bunching Instability Based on Reinforcement Learning</title><link>https://RL4aa.github.io/posts/publications/boltz2019feedback/</link><pubDate>Sun, 19 May 2019 00:00:00 +0000</pubDate><guid>https://RL4aa.github.io/posts/publications/boltz2019feedback/</guid><description>T. Boltz, M. Brosi, E. Bründermann, B. Haerer, P. Kaiser, C. Pohl, P. Schreiber, M. Yan,T. Asfour, A.-S. Müller
Karlsruhe Insitute of Technology KIT
10th International Particle Accelerator Conference
Abstract The operation of ring-based synchrotron light sourceswith short electron bunches increases the emission of co-herent synchrotron radiation (CSR) in the THz frequencyrange. However, the micro-bunching instability resultingfrom self-interaction of the bunch with its own radiationfield limits stable operation with constant intensity of CSRemission to a particular threshold current.</description></item><item><title>Using a neural network control policy for rapid switching between beam parameters in an FEL</title><link>https://RL4aa.github.io/posts/publications/edelen2017using/</link><pubDate>Fri, 25 Aug 2017 00:00:00 +0000</pubDate><guid>https://RL4aa.github.io/posts/publications/edelen2017using/</guid><description>A. L. Edelen1, S. G. Biedron2, J. P. Edelen3, S. V. Milton4, P. J. M. van der Slot5
1Colorado State University, 2Element Aero, 3Fermi National Accelerator Laboratory, 4Los Alamos National Laboratory, 5University of Twente
38th International Free Electron Laser Conference
Abstract FEL user facilities often must accommodate requests for a variety of beam parameters. This usually requires skilled operators to tune the machine, reducing the amount of available time for users.</description></item></channel></rss>