<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Publications | RL4AA Collaboration | Homepage</title><meta name=keywords content><meta name=description content="Reinforcement Learning for Autonomous Accelerator Collaboration - https://github.com/RL4AA"><meta name=author content="RL4AA Collaboration"><link rel=canonical href=https://RL4aa.github.io/categories/publications/><link crossorigin=anonymous href=/assets/css/stylesheet.a090830a421002426baafbd314e38f149d77b4c48a12ee9312700d770b27fb26.css integrity="sha256-oJCDCkIQAkJrqvvTFOOPFJ13tMSKEu6TEnANdwsn+yY=" rel="preload stylesheet" as=style><link rel=icon href=https://RL4aa.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://RL4aa.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://RL4aa.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://RL4aa.github.io/apple-touch-icon.png><link rel=mask-icon href=https://RL4aa.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate type=application/rss+xml href=https://RL4aa.github.io/categories/publications/index.xml title=rss><link rel=alternate hreflang=en href=https://RL4aa.github.io/categories/publications/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><meta property="og:url" content="https://RL4aa.github.io/categories/publications/"><meta property="og:site_name" content="RL4AA Collaboration | Homepage"><meta property="og:title" content="Publications"><meta property="og:description" content="Reinforcement Learning for Autonomous Accelerator Collaboration - https://github.com/RL4AA"><meta property="og:locale" content="en-us"><meta property="og:type" content="website"><meta name=twitter:card content="summary"><meta name=twitter:title content="Publications"><meta name=twitter:description content="Reinforcement Learning for Autonomous Accelerator Collaboration - https://github.com/RL4AA"></head><body class=list id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://RL4aa.github.io/ accesskey=h title="RL4AA Collaboration | Homepage (Alt + H)"><img src=https://RL4aa.github.io/imgs/rl4aa_logo.png alt aria-label=logo height=30>RL4AA Collaboration | Homepage</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://RL4aa.github.io/categories/announcements title=Announcements><span>Announcements</span></a></li><li><a href=https://RL4aa.github.io/categories/publications title=Publications><span class=active>Publications</span></a></li><li><a href=https://RL4aa.github.io/tags/ title=Tags><span>Tags</span></a></li><li><a href=https://RL4aa.github.io/archives/ title=Archive><span>Archive</span></a></li><li><a href=https://RL4aa.github.io/contact/ title=Contact><span>Contact</span></a></li></ul></nav></header><main class=main><header class=page-header><div class=breadcrumbs><a href=https://RL4aa.github.io/>Home</a>&nbsp;»&nbsp;<a href=https://RL4aa.github.io/categories/>Categories</a></div><h1>Publications</h1></header><article class="post-entry tag-entry"><figure class=entry-cover><img loading=lazy src=https://RL4aa.github.io/imgs/kain2020sampleefficient.png alt="The RL paradigm as applied to particle accelerator control, showing the example of trajectory correction."></figure><header class=entry-header><h2 class=entry-hint-parent>Sample-efficient reinforcement learning for CERN accelerator control</h2></header><div class=entry-content><p>V. Kain1, S. Hirlander1, B. Goddard1, F. M. Velotti1, G. Z. Della Porta1, N. Bruchon2, G. Valentino3
1CERN, 2University of Trieste, 3University of Malta
Physical Review Accelerators and Beams
Abstract Numerical optimization algorithms are already established tools to increase and stabilize the performance of particle accelerators. These algorithms have many advantages, are available out of the box, and can be adapted to a wide range of optimization problems in accelerator operation. The next boost in efficiency is expected to come from reinforcement learning algorithms that learn the optimal policy for a certain control problem and hence, once trained, can do without the time-consuming exploration phase needed for numerical optimizers. To investigate this approach, continuous model-free reinforcement learning with up to 16 degrees of freedom was developed and successfully tested at various facilities at CERN. The approach and algorithms used are discussed and the results obtained for trajectory steering at the AWAKE electron line and LINAC4 are presented. The necessary next steps, such as uncertainty aware model-based approaches, and the potential for future applications at particle accelerators are addressed.
...</p></div><footer class=entry-footer><span title='2020-12-01 00:00:00 +0000 UTC'>December 1, 2020</span>&nbsp;·&nbsp;<span>185 words</span>&nbsp;·&nbsp;<span>RL4AA Collaboration</span></footer><a class=entry-link aria-label="post link to Sample-efficient reinforcement learning for CERN accelerator control" href=https://RL4aa.github.io/posts/publications/kain2020sampleefficient/></a></article><article class="post-entry tag-entry"><figure class=entry-cover><img loading=lazy src=https://RL4aa.github.io/imgs/edelen2020neural.png alt="Schematic for neural network control policy updates."></figure><header class=entry-header><h2 class=entry-hint-parent>Neural Networks for Modeling and Control of Particle Accelerators</h2></header><div class=entry-content><p>A. L. Edelen
Colorado State University
PhD thesis
Abstract Charged particle accelerators support a wide variety of scientific, industrial, and medical applications. They range in scale and complexity from systems with just a few components for beam acceleration and manipulation, to large scientific user facilities that span many kilometers and have hundreds-to-thousands of individually-controllable components. Specific operational requirements must be met by adjusting the many controllable variables of the accelerator. Meeting these requirements can be challenging, both in terms of the ability to achieve specific beam quality metrics in a reliable fashion and in terms of the time needed to set up and maintain the optimal operating conditions. One avenue toward addressing this challenge is to incorporate techniques from the fields of machine learning (ML) and artificial intelligence (AI) into the way particle accelerators are modeled and controlled. While many promising approaches within AI/ML could beused for particle accelerators, this dissertation focuses on approaches based on neural networks. Neural networks are particularly well-suited to modeling, control, and diagnostic analysis of non-linear systems, as well as systems with large parameter spaces. They are also very appealing for their ability to process high-dimensional data types, such as images and time series (both of which are ubiquitous in particle accelerators). In this work, key studies that demonstrated the potential utility of modern neural network-based approaches to modeling and control of particle accelerators are presented. The context for this work is important: at the start of this work in 2012, there was little interest in AI/ML in the particle accelerator community, and many of the advances in neural networks and deep learning that enabled its present success had not yet been made at that time. As such, this work was both an exploration of possible application areas and a generator of initial demonstrations in these areas, including some of the first applications of modern deep neural networks in particle accelerators.
...</p></div><footer class=entry-footer><span title='2020-07-01 00:00:00 +0000 UTC'>July 1, 2020</span>&nbsp;·&nbsp;<span>322 words</span>&nbsp;·&nbsp;<span>RL4AA Collaboration</span></footer><a class=entry-link aria-label="post link to Neural Networks for Modeling and Control of Particle Accelerators" href=https://RL4aa.github.io/posts/publications/edelen2020neural/></a></article><article class="post-entry tag-entry"><figure class=entry-cover><img loading=lazy src=https://RL4aa.github.io/imgs/bruchon2020basic.png alt="Simple scheme of the FERMI FEL seed laser alignment set up."></figure><header class=entry-header><h2 class=entry-hint-parent>Basic Reinforcement Learning Techniques to Control the Intensity of a Seeded Free-Electron Laser</h2></header><div class=entry-content><p>N. Bruchon1, G. Fenu1, G. Gaio2, M. Lonza2, F. H. O’Shea2, F. A. Pellegrino1, E. Salvato1
1University of Trieste, 2Elettra Sincrotrone Trieste
Electronics
Abstract Optimal tuning of particle accelerators is a challenging task. Many different approaches have been proposed in the past to solve two main problems—attainment of an optimal working point and performance recovery after machine drifts. The most classical model-free techniques (e.g., Gradient Ascent or Extremum Seeking algorithms) have some intrinsic limitations. To overcome those limitations, Machine Learning tools, in particular Reinforcement Learning (RL), are attracting more and more attention in the particle accelerator community. We investigate the feasibility of RL model-free approaches to align the seed laser, as well as other service lasers, at FERMI, the free-electron laser facility at Elettra Sincrotrone Trieste. We apply two different techniques—the first, based on the episodic Q-learning with linear function approximation, for performance optimization; the second, based on the continuous Natural Policy Gradient REINFORCE algorithm, for performance recovery. Despite the simplicity of these approaches, we report satisfactory preliminary results, that represent the first step toward a new fully automatic procedure for the alignment of the seed laser to the electron beam. Such an alignment is, at present, performed manually.
...</p></div><footer class=entry-footer><span title='2020-05-09 00:00:00 +0000 UTC'>May 9, 2020</span>&nbsp;·&nbsp;<span>206 words</span>&nbsp;·&nbsp;<span>RL4AA Collaboration</span></footer><a class=entry-link aria-label="post link to Basic Reinforcement Learning Techniques to Control the Intensity of a Seeded Free-Electron Laser" href=https://RL4aa.github.io/posts/publications/bruchon2020basic/></a></article><article class="post-entry tag-entry"><figure class=entry-cover><img loading=lazy src=https://RL4aa.github.io/imgs/bruchon2019toward.png alt="Simple scheme of the EOS laser alignment set up."></figure><header class=entry-header><h2 class=entry-hint-parent>Toward the Application of Reinforcement Learning to the Intensity Control of a Seeded Free-Electron Laser</h2></header><div class=entry-content><p>N. Bruchon, G. Fenu, G. Gaio, M. Lonza, F. A. Pellegrino, E. Salvato
University of Trieste
23rd International Conference on Mechatronics Technology
Abstract The optimization of particle accelerators is a challenging task, and many different approaches have been proposed in years, to obtain an optimal tuning of the plant and to keep it optimally tuned despite drifts or disturbances. Indeed, the classical model-free approaches (such as Gradient Ascent or Extremum Seeking algorithms) have intrinsic limitations. To overcome those limitations, Machine Learning techniques, in particular, the Reinforcement Learning, are attracting more and more attention in the particle accelerator community. The purpose of this paper is to apply a Reinforcement Learning model-free approach to the alignment of a seed laser, based on a rather general target function depending on the laser trajectory. The study focuses on the alignment of the lasers at FERMI, the free-electron laser facility at Elettra Sincrotrone Trieste. In particular, we employ Q-learning with linear function approximation and report experimental results obtained in two setups, which are the actual setups where the final application has to be deployed. Despite the simplicity of the approach, we report satisfactory preliminary results, that represent the first step toward a fully automatic procedure for seed laser to the electron beam. Such a superimposition is, at present, performed manually.
...</p></div><footer class=entry-footer><span title='2019-10-23 00:00:00 +0000 UTC'>October 23, 2019</span>&nbsp;·&nbsp;<span>222 words</span>&nbsp;·&nbsp;<span>RL4AA Collaboration</span></footer><a class=entry-link aria-label="post link to Toward the Application of Reinforcement Learning to the Intensity Control of a Seeded Free-Electron Laser" href=https://RL4aa.github.io/posts/publications/bruchon2019toward/></a></article><article class="post-entry tag-entry"><figure class=entry-cover><img loading=lazy src=https://RL4aa.github.io/imgs/bruchon2019freeelectron.png alt="Scheme of the FERMI FEL seed laser alignmentset up"></figure><header class=entry-header><h2 class=entry-hint-parent>Free-electron Laser Optimization with Reinforcement Learning</h2></header><div class=entry-content><p>N. Bruchon1, G. Gaio2, G. Fenu1, M. Lonza2, F. A. Pellegrino1, E. Salvato1
1University of Trieste, 2Elettra Sincrotrone Trieste
17th International Conference on Accelerator and Large Experimental Physics Control Systems
Abstract Reinforcement Learning (RL) is one of the most promis-ing techniques in Machine Learning because of its modestcomputational requirements with respect to other algorithms.RL uses an agent that takes actions within its environmentto maximize a reward related to the goal it is designed toachieve. We have recently used RL as a model-free approachto improve the performance of the FERMI Free ElectronLaser. A number of machine parameters are adjusted tofind the optimum FEL output in terms of intensity and spec-tral quality. In particular we focus on the problem of thealignment of the seed laser with the electron beam, initiallyusing a simplified model and then applying the developedalgorithm on the real machine. This paper reports the resultsobtained and discusses pros and cons of this approach withplans for future applications.
...</p></div><footer class=entry-footer><span title='2019-10-05 00:00:00 +0000 UTC'>October 5, 2019</span>&nbsp;·&nbsp;<span>164 words</span>&nbsp;·&nbsp;<span>RL4AA Collaboration</span></footer><a class=entry-link aria-label="post link to Free-electron Laser Optimization with Reinforcement Learning" href=https://RL4aa.github.io/posts/publications/bruchon2019freeelectron/></a></article><article class="post-entry tag-entry"><figure class=entry-cover><img loading=lazy src=https://RL4aa.github.io/imgs/boltz2019feedback.png alt="General feedback scheme using the CSR powersignal to construct both, the state and reward signal of the Markov decision process (MDP)."></figure><header class=entry-header><h2 class=entry-hint-parent>Feedback Design for Control of the Micro-Bunching Instability Based on Reinforcement Learning</h2></header><div class=entry-content><p>T. Boltz, M. Brosi, E. Bründermann, B. Haerer, P. Kaiser, C. Pohl, P. Schreiber, M. Yan,T. Asfour, A.-S. Müller
Karlsruhe Insitute of Technology KIT
10th International Particle Accelerator Conference
Abstract The operation of ring-based synchrotron light sourceswith short electron bunches increases the emission of co-herent synchrotron radiation (CSR) in the THz frequencyrange. However, the micro-bunching instability resultingfrom self-interaction of the bunch with its own radiationfield limits stable operation with constant intensity of CSRemission to a particular threshold current. Above this thresh-old, the longitudinal charge distribution and thus the emittedradiation vary rapidly and continuously. Therefore, a fastand adaptive feedback system is the appropriate approach tostabilize the dynamics and to overcome the limitations givenby the instability. In this contribution, we discuss first effortstowards a longitudinal feedback design that acts on the RFsystem of the KIT storage ring KARA (Karlsruhe ResearchAccelerator) and aims for stabilization of the emitted THzradiation. Our approach is based on methods of adaptive con-trol that were developed in the field of reinforcement learningand have seen great success in other fields of research overthe past decade. We motivate this particular approach andcomment on different aspects of its implementation.
...</p></div><footer class=entry-footer><span title='2019-05-19 00:00:00 +0000 UTC'>May 19, 2019</span>&nbsp;·&nbsp;<span>195 words</span>&nbsp;·&nbsp;<span>RL4AA Collaboration</span></footer><a class=entry-link aria-label="post link to Feedback Design for Control of the Micro-Bunching Instability Based on Reinforcement Learning" href=https://RL4aa.github.io/posts/publications/boltz2019feedback/></a></article><article class="post-entry tag-entry"><figure class=entry-cover><img loading=lazy src=https://RL4aa.github.io/imgs/edelen2017using.png alt="Layout of the accelerator."></figure><header class=entry-header><h2 class=entry-hint-parent>Using a neural network control policy for rapid switching between beam parameters in an FEL</h2></header><div class=entry-content><p>A. L. Edelen1, S. G. Biedron2, J. P. Edelen3, S. V. Milton4, P. J. M. van der Slot5
1Colorado State University, 2Element Aero, 3Fermi National Accelerator Laboratory, 4Los Alamos National Laboratory, 5University of Twente
38th International Free Electron Laser Conference
Abstract FEL user facilities often must accommodate requests for a variety of beam parameters. This usually requires skilled operators to tune the machine, reducing the amount of available time for users. In principle, a neural network control policy that is trained on a broad range of operating states could be used to quickly switch between these requests without substantial need for human inter-vention. We present preliminary results from an ongoing study in which a neural network control policy is investi-gated for rapid switching between beam parameters in a compact THz FEL.
...</p></div><footer class=entry-footer><span title='2017-08-25 00:00:00 +0000 UTC'>August 25, 2017</span>&nbsp;·&nbsp;<span>138 words</span>&nbsp;·&nbsp;<span>RL4AA Collaboration</span></footer><a class=entry-link aria-label="post link to Using a neural network control policy for rapid switching between beam parameters in an FEL" href=https://RL4aa.github.io/posts/publications/edelen2017using/></a></article><article class="post-entry tag-entry"><figure class=entry-cover><img loading=lazy src=https://RL4aa.github.io/imgs/edelen2017using2.png alt="General setup for the neural network model."></figure><header class=entry-header><h2 class=entry-hint-parent>Using Neural Network Control Policies For Rapid Switching Between Beam Parameters in a Free Electron Laser</h2></header><div class=entry-content><p>A. L. Edelen1, S. G. Biedron2, J. P. Edelen3, S. V. Milton4, P. J. M. van der Slot5
1Colorado State University, 2Element Aero, 3Fermi National Accelerator Laboratory, 4Los Alamos National Laboratory, 5University of Twente
Workshop on Deep Learning for Physical Sciences at the Conference on Neural Information Processing Systems 2017
Abstract Free Electron Laser (FEL) facilities often must accommodate requests for a varietyof electron beam parameters in order to supply scientific users with appropriatephoton beam characteristics. This usually requires skilled human operators to tunethe machine. In principle, a neural network control policy that is trained on a broadrange of machine operating states could be used to quickly switch between theserequests without substantial need for human intervention. We present preliminaryresults from an ongoing simulation study in which a neural network control policyis investigated for rapid switching between beam parameters in a compact THzFEL that exhibits nonlinear electron beam dynamics. To accomplish this, we firsttrain a feed-forward neural network to mimic a physics-based simulation of theFEL. We then train a neural network control policy by first pre-training it as aninverse model (using supervised learning with a subset of the simulation data) andthen training it more extensively with reinforcement learning. In this case, thereinforcement learning component consists of letting the policy network interactwith the learned system model and backpropagating the cost through the modelnetwork to the controller network.
...</p></div><footer class=entry-footer><span title='2017-08-25 00:00:00 +0000 UTC'>August 25, 2017</span>&nbsp;·&nbsp;<span>232 words</span>&nbsp;·&nbsp;<span>RL4AA Collaboration</span></footer><a class=entry-link aria-label="post link to Using Neural Network Control Policies For Rapid Switching Between Beam Parameters in a Free Electron Laser" href=https://RL4aa.github.io/posts/publications/edelen2017using2/></a></article><article class="post-entry tag-entry"><figure class=entry-cover><img loading=lazy src=https://RL4aa.github.io/imgs/meier2012orbit.png alt="Example of a simulation run."></figure><header class=entry-header><h2 class=entry-hint-parent>Orbit Correction Studies Using Neural Networks</h2></header><div class=entry-content><p>E. Meier, Y.-R. E. Tan, G. S. LeBlanc
Australian Synchrotron
3rd International Particle Accelerator Conference
Abstract This paper reports the use of neural networks for orbitcorrection at the Australian Synchrotron Storage Ring. Theproposed system uses two neural networks in an actor-criticscheme to model a long term cost function and computeappropriate corrections. The system is entirely based onthe history of the beam position and the actuators, i.e. thecorrector magnets, in the storage ring. This makes the sys-tem auto-tuneable, which has the advantage of avoiding themeasure of a response matrix. The controller will automat-ically maintain an updated BPM corrector response matrix.In future if coupled with some form of orbit response anal-ysis, the system will have the potential to track drifts orchanges to the lattice functions in ”real time”. As a genericand robust orbit correction program it can be used duringcommissioning and in slow orbit feedback. In this study,we present positive initial results of the simulations of thestorage ring in Matlab.
...</p></div><footer class=entry-footer><span title='2012-05-20 00:00:00 +0000 UTC'>May 20, 2012</span>&nbsp;·&nbsp;<span>165 words</span>&nbsp;·&nbsp;<span>RL4AA Collaboration</span></footer><a class=entry-link aria-label="post link to Orbit Correction Studies Using Neural Networks" href=https://RL4aa.github.io/posts/publications/meier2012orbit/></a></article><footer class=page-footer><nav class=pagination><a class=prev href=https://RL4aa.github.io/categories/publications/page/2/>«&nbsp;Prev&nbsp;</a></nav></footer></main><footer class=footer><span>&copy; 2025 <a href=https://RL4aa.github.io/>RL4AA Collaboration | Homepage</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>