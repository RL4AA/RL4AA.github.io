<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Publications | RL4AA Collaboration | Homepage</title><meta name=keywords content><meta name=description content="Reinforcement Learning for Autonomous Accelerator Collaboration - https://github.com/RL4AA"><meta name=author content="RL4AA Collaboration"><link rel=canonical href=https://RL4aa.github.io/categories/publications/><link crossorigin=anonymous href=/assets/css/stylesheet.3613efbd0b1772781e8f49935e973cae632a7f61471c05b17be155505ccf87b5.css integrity="sha256-NhPvvQsXcngej0mTXpc8rmMqf2FHHAWxe+FVUFzPh7U=" rel="preload stylesheet" as=style><link rel=icon href=https://RL4aa.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://RL4aa.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://RL4aa.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://RL4aa.github.io/apple-touch-icon.png><link rel=mask-icon href=https://RL4aa.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate type=application/rss+xml href=https://RL4aa.github.io/categories/publications/index.xml><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><meta property="og:title" content="Publications"><meta property="og:description" content="Reinforcement Learning for Autonomous Accelerator Collaboration - https://github.com/RL4AA"><meta property="og:type" content="website"><meta property="og:url" content="https://RL4aa.github.io/categories/publications/"><meta name=twitter:card content="summary"><meta name=twitter:title content="Publications"><meta name=twitter:description content="Reinforcement Learning for Autonomous Accelerator Collaboration - https://github.com/RL4AA"></head><body class=list id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://RL4aa.github.io/ accesskey=h title="RL4AA Collaboration | Homepage (Alt + H)">RL4AA Collaboration | Homepage</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://RL4aa.github.io/categories/announcements title=Announcements><span>Announcements</span></a></li><li><a href=https://RL4aa.github.io/categories/publications title=Publications><span class=active>Publications</span></a></li><li><a href=https://RL4aa.github.io/tags/ title=Tags><span>Tags</span></a></li><li><a href=https://RL4aa.github.io/archives/ title=Archive><span>Archive</span></a></li><li><a href=https://RL4aa.github.io/contact/ title=Contact><span>Contact</span></a></li></ul></nav></header><main class=main><header class=page-header><div class=breadcrumbs><a href=https://RL4aa.github.io/>Home</a>&nbsp;»&nbsp;<a href=https://RL4aa.github.io/categories/>Categories</a></div><h1>Publications</h1></header><article class="post-entry tag-entry"><figure class=entry-cover><img loading=lazy src=https://RL4aa.github.io/imgs/bruchon2021feasibility.png alt="Simple scheme of the FERMI FEL seed laser alignment set up."></figure><header class=entry-header><h2>Feasibility Investigation on Several Reinforcement Learning Techniques to Improve the Performance of the FERMI Free-Electron Laser</h2></header><div class=entry-content><p>N. Bruchon
University of Trieste
PhD thesis
Abstract The research carried out in particle accelerator facilities does not concern only particle and condensed matter physics, although these are the main topics covered in the field. Indeed, since a particle accelerator is composed of many different sub-systems, its proper functioning depends both on each of these parts and their interconnection. It follows that the study, implementation, and improvement of the various sub-systems are fundamental points of investigation too....</p></div><footer class=entry-footer><span title='2021-03-18 00:00:00 +0000 UTC'>March 18, 2021</span>&nbsp;·&nbsp;322 words&nbsp;·&nbsp;RL4AA Collaboration</footer><a class=entry-link aria-label="post link to Feasibility Investigation on Several Reinforcement Learning Techniques to Improve the Performance of the FERMI Free-Electron Laser" href=https://RL4aa.github.io/posts/publications/bruchon2021feasiblity/></a></article><article class="post-entry tag-entry"><figure class=entry-cover><img loading=lazy src=https://RL4aa.github.io/imgs/oshea2020policy.png alt="Plot of the reward received by the agent versus step number."></figure><header class=entry-header><h2>Policy gradient methods for free-electron laser and terahertz source optimization and stabilization at the FERMI free-electron laser at Elettra</h2></header><div class=entry-content><p>F. H. O’Shea1, N. Bruchon2, G. Gaio1
1Elettra Sincrotrone Trieste, 2University of Trieste
Physical Review Accelerators and Beams
Abstract In this article we report on the application of a model-free reinforcement learning method to the optimization of accelerator systems. We simplify a policy gradient algorithm to accelerator control from sophisticated algorithms that have recently been demonstrated to solve complex dynamic problems. After outlining a theoretical basis for the functioning of the algorithm, we explore the small hyperparameter space to develop intuition about said parameters using a simple number-guess environment....</p></div><footer class=entry-footer><span title='2020-12-21 00:00:00 +0000 UTC'>December 21, 2020</span>&nbsp;·&nbsp;160 words&nbsp;·&nbsp;RL4AA Collaboration</footer><a class=entry-link aria-label="post link to Policy gradient methods for free-electron laser and terahertz source optimization and stabilization at the FERMI free-electron laser at Elettra" href=https://RL4aa.github.io/posts/publications/oshea202policy/></a></article><article class="post-entry tag-entry"><figure class=entry-cover><img loading=lazy src=https://RL4aa.github.io/imgs/hirlaender2020modelfree.png alt="A schematic overview of theAE-DYNAapproach used in this paper."></figure><header class=entry-header><h2>Model-free and Bayesian Ensembling Model-based Deep Reinforcement Learning for Particle Accelerator Control Demonstrated on the FERMI FEL</h2></header><div class=entry-content><p>S. Hirlaender, N. Bruchon
University of Salzburg, University of Trieste
arXiv
Abstract Reinforcement learning holds tremendous promise in accelerator controls. The primary goal of this paper is to show how this approach can be utilised on an operational level on accelerator physics problems. Despite the success of model-free reinforcement learning in several domains, sample-efficiency still is a bottle-neck, which might be encompassed by model-based methods. We compare well-suited purely model-based to model-free reinforcement learning applied to the intensity optimisation on the FERMI FEL system....</p></div><footer class=entry-footer><span title='2020-12-17 00:00:00 +0000 UTC'>December 17, 2020</span>&nbsp;·&nbsp;158 words&nbsp;·&nbsp;RL4AA Collaboration</footer><a class=entry-link aria-label="post link to Model-free and Bayesian Ensembling Model-based Deep Reinforcement Learning for Particle Accelerator Control Demonstrated on the FERMI FEL" href=https://RL4aa.github.io/posts/publications/hirlaender2020modelfree/></a></article><article class="post-entry tag-entry"><figure class=entry-cover><img loading=lazy src=https://RL4aa.github.io/imgs/pang2020autonomous.png alt="Policy network maps states to actions."></figure><header class=entry-header><h2>Autonomous Control of a Particle Accelerator using Deep Reinforcement Learning</h2></header><div class=entry-content><p>X. Pang1, S. Thulasidasan2, L. Rybarcyk2
1Apple, 2Los Alamos National Laboratory
Machine Learning for Engineering Modeling, Simulation, and Design Workshop at Neural Information Processing Systems 2020
Abstract We describe an approach to learning optimal control policies for a large, linear particle accelerator using deep reinforcement learning coupled with a high-fidelity physics engine. The framework consists of an AI controller that uses deep neural networks for state and action-space representation and learns optimal policies using reward signals that are provided by the physics simulator....</p></div><footer class=entry-footer><span title='2020-12-12 00:00:00 +0000 UTC'>December 12, 2020</span>&nbsp;·&nbsp;150 words&nbsp;·&nbsp;RL4AA Collaboration</footer><a class=entry-link aria-label="post link to Autonomous Control of a Particle Accelerator using Deep Reinforcement Learning" href=https://RL4aa.github.io/posts/publications/pang2020autonomous/></a></article><article class="post-entry tag-entry"><figure class=entry-cover><img loading=lazy src=https://RL4aa.github.io/imgs/kain2020sampleefficient.png alt="The RL paradigm as applied to particle accelerator control, showing the example of trajectory correction."></figure><header class=entry-header><h2>Sample-efficient reinforcement learning for CERN accelerator control</h2></header><div class=entry-content><p>V. Kain1, S. Hirlander1, B. Goddard1, F. M. Velotti1, G. Z. Della Porta1, N. Bruchon2, G. Valentino3
1CERN, 2University of Trieste, 3University of Malta
Physical Review Accelerators and Beams
Abstract Numerical optimization algorithms are already established tools to increase and stabilize the performance of particle accelerators. These algorithms have many advantages, are available out of the box, and can be adapted to a wide range of optimization problems in accelerator operation....</p></div><footer class=entry-footer><span title='2020-12-01 00:00:00 +0000 UTC'>December 1, 2020</span>&nbsp;·&nbsp;185 words&nbsp;·&nbsp;RL4AA Collaboration</footer><a class=entry-link aria-label="post link to Sample-efficient reinforcement learning for CERN accelerator control" href=https://RL4aa.github.io/posts/publications/kain2020sampleefficient/></a></article><article class="post-entry tag-entry"><figure class=entry-cover><img loading=lazy src=https://RL4aa.github.io/imgs/bruchon2020basic.png alt="Simple scheme of the FERMI FEL seed laser alignment set up."></figure><header class=entry-header><h2>Basic Reinforcement Learning Techniques to Control the Intensity of a Seeded Free-Electron Laser</h2></header><div class=entry-content><p>N. Bruchon1, G. Fenu1, G. Gaio2, M. Lonza2, F. H. O’Shea2, F. A. Pellegrino1, E. Salvato1
1University of Trieste, 2Elettra Sincrotrone Trieste
Electronics
Abstract Optimal tuning of particle accelerators is a challenging task. Many different approaches have been proposed in the past to solve two main problems—attainment of an optimal working point and performance recovery after machine drifts. The most classical model-free techniques (e.g., Gradient Ascent or Extremum Seeking algorithms) have some intrinsic limitations....</p></div><footer class=entry-footer><span title='2020-05-09 00:00:00 +0000 UTC'>May 9, 2020</span>&nbsp;·&nbsp;206 words&nbsp;·&nbsp;RL4AA Collaboration</footer><a class=entry-link aria-label="post link to Basic Reinforcement Learning Techniques to Control the Intensity of a Seeded Free-Electron Laser" href=https://RL4aa.github.io/posts/publications/bruchon2020basic/></a></article><article class="post-entry tag-entry"><figure class=entry-cover><img loading=lazy src=https://RL4aa.github.io/imgs/bruchon2019toward.png alt="Simple scheme of the EOS laser alignment set up."></figure><header class=entry-header><h2>Toward the Application of Reinforcement Learning to the Intensity Control of a Seeded Free-Electron Laser</h2></header><div class=entry-content><p>N. Bruchon, G. Fenu, G. Gaio, M. Lonza, F. A. Pellegrino, E. Salvato
University of Trieste
23rd International Conference on Mechatronics Technology
Abstract The optimization of particle accelerators is a challenging task, and many different approaches have been proposed in years, to obtain an optimal tuning of the plant and to keep it optimally tuned despite drifts or disturbances. Indeed, the classical model-free approaches (such as Gradient Ascent or Extremum Seeking algorithms) have intrinsic limitations....</p></div><footer class=entry-footer><span title='2019-10-23 00:00:00 +0000 UTC'>October 23, 2019</span>&nbsp;·&nbsp;222 words&nbsp;·&nbsp;RL4AA Collaboration</footer><a class=entry-link aria-label="post link to Toward the Application of Reinforcement Learning to the Intensity Control of a Seeded Free-Electron Laser" href=https://RL4aa.github.io/posts/publications/bruchon2019toward/></a></article><article class="post-entry tag-entry"><figure class=entry-cover><img loading=lazy src=https://RL4aa.github.io/imgs/boltz2019feedback.png alt="General feedback scheme using the CSR powersignal to construct both, the state and reward signal of the Markov decision process (MDP)."></figure><header class=entry-header><h2>Feedback Design for Control of the Micro-Bunching Instability Based on Reinforcement Learning</h2></header><div class=entry-content><p>T. Boltz, M. Brosi, E. Bründermann, B. Haerer, P. Kaiser, C. Pohl, P. Schreiber, M. Yan,T. Asfour, A.-S. Müller
Karlsruhe Insitute of Technology KIT
10th International Particle Accelerator Conference
Abstract The operation of ring-based synchrotron light sourceswith short electron bunches increases the emission of co-herent synchrotron radiation (CSR) in the THz frequencyrange. However, the micro-bunching instability resultingfrom self-interaction of the bunch with its own radiationfield limits stable operation with constant intensity of CSRemission to a particular threshold current....</p></div><footer class=entry-footer><span title='2019-05-19 00:00:00 +0000 UTC'>May 19, 2019</span>&nbsp;·&nbsp;195 words&nbsp;·&nbsp;RL4AA Collaboration</footer><a class=entry-link aria-label="post link to Feedback Design for Control of the Micro-Bunching Instability Based on Reinforcement Learning" href=https://RL4aa.github.io/posts/publications/boltz2019feedback/></a></article><article class="post-entry tag-entry"><figure class=entry-cover><img loading=lazy src=https://RL4aa.github.io/imgs/edelen2017using.png alt="Layout of the accelerator."></figure><header class=entry-header><h2>Using a neural network control policy for rapid switching between beam parameters in an FEL</h2></header><div class=entry-content><p>A. L. Edelen1, S. G. Biedron2, J. P. Edelen3, S. V. Milton4, P. J. M. van der Slot5
1Colorado State University, 2Element Aero, 3Fermi National Accelerator Laboratory, 4Los Alamos National Laboratory, 5University of Twente
38th International Free Electron Laser Conference
Abstract FEL user facilities often must accommodate requests for a variety of beam parameters. This usually requires skilled operators to tune the machine, reducing the amount of available time for users....</p></div><footer class=entry-footer><span title='2017-08-25 00:00:00 +0000 UTC'>August 25, 2017</span>&nbsp;·&nbsp;138 words&nbsp;·&nbsp;RL4AA Collaboration</footer><a class=entry-link aria-label="post link to Using a neural network control policy for rapid switching between beam parameters in an FEL" href=https://RL4aa.github.io/posts/publications/edelen2017using/></a></article><footer class=page-footer><nav class=pagination><a class=prev href=https://RL4aa.github.io/categories/publications/>«&nbsp;Prev&nbsp;</a></nav></footer></main><footer class=footer><span>&copy; 2023 <a href=https://RL4aa.github.io/>RL4AA Collaboration | Homepage</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>