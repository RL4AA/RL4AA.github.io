<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>conference proceedings | RL4AA Collaboration | Homepage</title><meta name=keywords content><meta name=description content="Reinforcement Learning for Autonomous Accelerator Collaboration - https://github.com/RL4AA"><meta name=author content="RL4AA Collaboration"><link rel=canonical href=https://RL4aa.github.io/tags/conference-proceedings/><link crossorigin=anonymous href=/assets/css/stylesheet.5cfc680b1eeaeef9efbced92d46c2a9e876b72ee14fba85846afc4cff9e6e6f8.css integrity="sha256-XPxoCx7q7vnvvO2S1Gwqnodrcu4U+6hYRq/Ez/nm5vg=" rel="preload stylesheet" as=style><link rel=icon href=https://RL4aa.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://RL4aa.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://RL4aa.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://RL4aa.github.io/apple-touch-icon.png><link rel=mask-icon href=https://RL4aa.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate type=application/rss+xml href=https://RL4aa.github.io/tags/conference-proceedings/index.xml><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><meta property="og:title" content="conference proceedings"><meta property="og:description" content="Reinforcement Learning for Autonomous Accelerator Collaboration - https://github.com/RL4AA"><meta property="og:type" content="website"><meta property="og:url" content="https://RL4aa.github.io/tags/conference-proceedings/"><meta name=twitter:card content="summary"><meta name=twitter:title content="conference proceedings"><meta name=twitter:description content="Reinforcement Learning for Autonomous Accelerator Collaboration - https://github.com/RL4AA"></head><body class=list id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://RL4aa.github.io/ accesskey=h title="RL4AA Collaboration | Homepage (Alt + H)"><img src=https://RL4aa.github.io/imgs/rl4aa_logo.png alt aria-label=logo height=30>RL4AA Collaboration | Homepage</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://RL4aa.github.io/categories/announcements title=Announcements><span>Announcements</span></a></li><li><a href=https://RL4aa.github.io/categories/publications title=Publications><span>Publications</span></a></li><li><a href=https://RL4aa.github.io/tags/ title=Tags><span>Tags</span></a></li><li><a href=https://RL4aa.github.io/archives/ title=Archive><span>Archive</span></a></li><li><a href=https://RL4aa.github.io/contact/ title=Contact><span>Contact</span></a></li></ul></nav></header><main class=main><header class=page-header><div class=breadcrumbs><a href=https://RL4aa.github.io/>Home</a>&nbsp;»&nbsp;<a href=https://RL4aa.github.io/tags/>Tags</a></div><h1>conference proceedings</h1></header><article class="post-entry tag-entry"><figure class=entry-cover><img loading=lazy src=https://RL4aa.github.io/imgs/kaiser2022learningbased.png alt="Reinforcement learning loop for the ARES experimental area."></figure><header class=entry-header><h2>Learning-based Optimisation of Particle Accelerators Under Partial Observability Without Real-World Training</h2></header><div class=entry-content><p>J. Kaiser, O. Stein, A. Eichler
Deutsches Elektronen-Synchrotron DESY
39th International Conference on Machine Learning
Abstract In recent work, it has been shown that reinforcement learning (RL) is capable of solving a variety of problems at sometimes super-human performance levels. But despite continued advances in the field, applying RL to complex real-world control and optimisation problems has proven difficult. In this contribution, we demonstrate how to successfully apply RL to the optimisation of a highly complex real-world machine – specifically a linear particle accelerator – in an only partially observable setting and without requiring training on the real machine....</p></div><footer class=entry-footer><span title='2022-07-22 00:00:00 +0000 UTC'>July 22, 2022</span>&nbsp;·&nbsp;174 words&nbsp;·&nbsp;RL4AA Collaboration</footer><a class=entry-link aria-label="post link to Learning-based Optimisation of Particle Accelerators Under Partial Observability Without Real-World Training" href=https://RL4aa.github.io/posts/publications/kaiser2022learningbased/></a></article><article class="post-entry tag-entry"><figure class=entry-cover><img loading=lazy src=https://RL4aa.github.io/imgs/madysa2022automated.png alt="Success rate of the various algorithms over initial beam intensity."></figure><header class=entry-header><h2>Automated Intensity Optimisation Using Reinforcement Learning at LEIR</h2></header><div class=entry-content><p>N. Madysa, V. Kain, R. Alemany Fernandez, N. Biancacci, B. Goddard, F. M. Velotti
CERN
13th Particle Accelerator Conference
Abstract High intensities in the Low Energy Ion Ring (LEIR) at CERN are achieved by stacking several multi-turn injec- tions from the pre-accelerator Linac3. Up to seven consec- utive 200 μs long, 200 ms spaced pulses are injected from Linac3 into LEIR. Two inclined septa, one magnetic and one electrostatic, combined with a collapsing horizontal or- bit bump allows a 6-D phase space painting via a linearly ramped mean momentum along the Linac3 pulse and in- jection at high dispersion....</p></div><footer class=entry-footer><span title='2022-06-12 00:00:00 +0000 UTC'>June 12, 2022</span>&nbsp;·&nbsp;264 words&nbsp;·&nbsp;RL4AA Collaboration</footer><a class=entry-link aria-label="post link to Automated Intensity Optimisation Using Reinforcement Learning at LEIR" href=https://RL4aa.github.io/posts/publications/madysa2022automated/></a></article><article class="post-entry tag-entry"><figure class=entry-cover><img loading=lazy src=https://RL4aa.github.io/imgs/kain2021test.png alt="Online training of NAF Agent of AWAKE electronline trajectory steering in the horizontal plane."></figure><header class=entry-header><h2>Test of Machine Learning at the CERN LINAC4</h2></header><div class=entry-content><p>V. Kain1, N. Bruchon1, S. Hirlander1, N. Madysa1, I. Vojskovic1, P. Skowronski1, G. Valentino2
1CERN, 2University of Malta
61st ICFA ABDW on High-Intensity and High-Brightness Hadron Beams
Abstract The CERN H−linear accelerator, LINAC4, served as atest bed for advanced algorithms during the CERN LongShutdown 2 in the years 2019/20. One of the main goals wasto show that reinforcement learning with all its benefits canbe used as a replacement for numerical optimization and asa complement to classical control in the accelerator controlcontext....</p></div><footer class=entry-footer><span title='2021-10-04 00:00:00 +0000 UTC'>October 4, 2021</span>&nbsp;·&nbsp;132 words&nbsp;·&nbsp;RL4AA Collaboration</footer><a class=entry-link aria-label="post link to Test of Machine Learning at the CERN LINAC4" href=https://RL4aa.github.io/posts/publications/kain2021test/></a></article><article class="post-entry tag-entry"><figure class=entry-cover><img loading=lazy src=https://RL4aa.github.io/imgs/eichler2021first.png alt="RL environment for beam optimisation in theARES EA."></figure><header class=entry-header><h2>First Steps Toward an Autonomous Accelerator, A Common Project Between DESY and KIT</h2></header><div class=entry-content><p>A. Eichler1, F. Burkart1, J. Kaiser1, W. Kuropka1, O. Stein1, E. Bründermann2, A. Santamaria Garcia2, C. Xu2
1Deutsches Elektronen-Synchrotron DESY, 2Karlsruhe Institute of Technology KIT
12th International Particle Accelerator Conference
Abstract Reinforcement learning algorithms have risen in pop-ularity in the accelerator physics community in recentyears, showing potential in beam control and in the opti-mization and automation of tasks in accelerator operation.The Helmholtz AI project “Machine Learning Toward Au-tonomous Accelerators” is a collaboration between DESYand KIT that works on investigating and developing rein-forcement learning applications for the automatic start-upof electron linear accelerators....</p></div><footer class=entry-footer><span title='2021-05-24 00:00:00 +0000 UTC'>May 24, 2021</span>&nbsp;·&nbsp;185 words&nbsp;·&nbsp;RL4AA Collaboration</footer><a class=entry-link aria-label="post link to First Steps Toward an Autonomous Accelerator, A Common Project Between DESY and KIT" href=https://RL4aa.github.io/posts/publications/eichler2021first/></a></article><article class="post-entry tag-entry"><figure class=entry-cover><img loading=lazy src=https://RL4aa.github.io/imgs/ivanov2021physicsenhanced.png alt="Reinforcement learning agent joint with the physics-based polynomial neural network."></figure><header class=entry-header><h2>Physics-Enhanced Reinforcement Learning for Optimal Control</h2></header><div class=entry-content><p>A. Ivanov, I. Agapov, A. Eichler, S. Tomin
Deutsches Elektronen Synchrotron DESY
12th International Particle Accelerator Conference
Abstract We propose an approach for incorporating acceleratorphysics models into reinforcement learning agents. The proposed approach is based on the Taylor mapping technique for the simulation of particle dynamics. The resulting computational graph is represented as a polynomial neural network and embedded into the traditional reinforcement learning agents. The application of the model is demonstrated in a nonlinear simulation model of beam transmission....</p></div><footer class=entry-footer><span title='2021-05-21 00:00:00 +0000 UTC'>May 21, 2021</span>&nbsp;·&nbsp;110 words&nbsp;·&nbsp;RL4AA Collaboration</footer><a class=entry-link aria-label="post link to Physics-Enhanced Reinforcement Learning for Optimal Control" href=https://RL4aa.github.io/posts/publications/ivanov2021physicsenhanced/></a></article><article class="post-entry tag-entry"><figure class=entry-cover><img loading=lazy src=https://RL4aa.github.io/imgs/bruchon2019toward.png alt="Simple scheme of the EOS laser alignment set up."></figure><header class=entry-header><h2>Toward the Application of Reinforcement Learning to the Intensity Control of a Seeded Free-Electron Laser</h2></header><div class=entry-content><p>N. Bruchon, G. Fenu, G. Gaio, M. Lonza, F. A. Pellegrino, E. Salvato
University of Trieste
23rd International Conference on Mechatronics Technology
Abstract The optimization of particle accelerators is a challenging task, and many different approaches have been proposed in years, to obtain an optimal tuning of the plant and to keep it optimally tuned despite drifts or disturbances. Indeed, the classical model-free approaches (such as Gradient Ascent or Extremum Seeking algorithms) have intrinsic limitations....</p></div><footer class=entry-footer><span title='2019-10-23 00:00:00 +0000 UTC'>October 23, 2019</span>&nbsp;·&nbsp;222 words&nbsp;·&nbsp;RL4AA Collaboration</footer><a class=entry-link aria-label="post link to Toward the Application of Reinforcement Learning to the Intensity Control of a Seeded Free-Electron Laser" href=https://RL4aa.github.io/posts/publications/bruchon2019toward/></a></article><article class="post-entry tag-entry"><figure class=entry-cover><img loading=lazy src=https://RL4aa.github.io/imgs/bruchon2019freeelectron.png alt="Scheme of the FERMI FEL seed laser alignmentset up"></figure><header class=entry-header><h2>Free-electron Laser Optimization with Reinforcement Learning</h2></header><div class=entry-content><p>N. Bruchon1, G. Gaio2, G. Fenu1, M. Lonza2, F. A. Pellegrino1, E. Salvato1
1University of Trieste, 2Elettra Sincrotrone Trieste
17th International Conference on Accelerator and Large Experimental Physics Control Systems
Abstract Reinforcement Learning (RL) is one of the most promis-ing techniques in Machine Learning because of its modestcomputational requirements with respect to other algorithms.RL uses an agent that takes actions within its environmentto maximize a reward related to the goal it is designed toachieve....</p></div><footer class=entry-footer><span title='2019-10-05 00:00:00 +0000 UTC'>October 5, 2019</span>&nbsp;·&nbsp;164 words&nbsp;·&nbsp;RL4AA Collaboration</footer><a class=entry-link aria-label="post link to Free-electron Laser Optimization with Reinforcement Learning" href=https://RL4aa.github.io/posts/publications/bruchon2019freeelectron/></a></article><article class="post-entry tag-entry"><figure class=entry-cover><img loading=lazy src=https://RL4aa.github.io/imgs/boltz2019feedback.png alt="General feedback scheme using the CSR powersignal to construct both, the state and reward signal of the Markov decision process (MDP)."></figure><header class=entry-header><h2>Feedback Design for Control of the Micro-Bunching Instability Based on Reinforcement Learning</h2></header><div class=entry-content><p>T. Boltz, M. Brosi, E. Bründermann, B. Haerer, P. Kaiser, C. Pohl, P. Schreiber, M. Yan,T. Asfour, A.-S. Müller
Karlsruhe Insitute of Technology KIT
10th International Particle Accelerator Conference
Abstract The operation of ring-based synchrotron light sourceswith short electron bunches increases the emission of co-herent synchrotron radiation (CSR) in the THz frequencyrange. However, the micro-bunching instability resultingfrom self-interaction of the bunch with its own radiationfield limits stable operation with constant intensity of CSRemission to a particular threshold current....</p></div><footer class=entry-footer><span title='2019-05-19 00:00:00 +0000 UTC'>May 19, 2019</span>&nbsp;·&nbsp;195 words&nbsp;·&nbsp;RL4AA Collaboration</footer><a class=entry-link aria-label="post link to Feedback Design for Control of the Micro-Bunching Instability Based on Reinforcement Learning" href=https://RL4aa.github.io/posts/publications/boltz2019feedback/></a></article><article class="post-entry tag-entry"><figure class=entry-cover><img loading=lazy src=https://RL4aa.github.io/imgs/edelen2017using.png alt="Layout of the accelerator."></figure><header class=entry-header><h2>Using a neural network control policy for rapid switching between beam parameters in an FEL</h2></header><div class=entry-content><p>A. L. Edelen1, S. G. Biedron2, J. P. Edelen3, S. V. Milton4, P. J. M. van der Slot5
1Colorado State University, 2Element Aero, 3Fermi National Accelerator Laboratory, 4Los Alamos National Laboratory, 5University of Twente
38th International Free Electron Laser Conference
Abstract FEL user facilities often must accommodate requests for a variety of beam parameters. This usually requires skilled operators to tune the machine, reducing the amount of available time for users....</p></div><footer class=entry-footer><span title='2017-08-25 00:00:00 +0000 UTC'>August 25, 2017</span>&nbsp;·&nbsp;138 words&nbsp;·&nbsp;RL4AA Collaboration</footer><a class=entry-link aria-label="post link to Using a neural network control policy for rapid switching between beam parameters in an FEL" href=https://RL4aa.github.io/posts/publications/edelen2017using/></a></article></main><footer class=footer><span>&copy; 2024 <a href=https://RL4aa.github.io/>RL4AA Collaboration | Homepage</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>