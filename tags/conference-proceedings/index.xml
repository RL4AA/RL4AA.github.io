<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/"><channel><title>Conference Proceedings on RL4AA Collaboration | Homepage</title><link>https://RL4aa.github.io/tags/conference-proceedings/</link><description>Recent content in Conference Proceedings on RL4AA Collaboration | Homepage</description><generator>Hugo -- 0.152.1</generator><language>en-us</language><lastBuildDate>Fri, 22 Jul 2022 00:00:00 +0000</lastBuildDate><atom:link href="https://RL4aa.github.io/tags/conference-proceedings/index.xml" rel="self" type="application/rss+xml"/><item><title>Learning-based Optimisation of Particle Accelerators Under Partial Observability Without Real-World Training</title><link>https://RL4aa.github.io/posts/publications/kaiser2022learningbased/</link><pubDate>Fri, 22 Jul 2022 00:00:00 +0000</pubDate><guid>https://RL4aa.github.io/posts/publications/kaiser2022learningbased/</guid><description>&lt;p&gt;&lt;em&gt;&lt;strong&gt;J. Kaiser, O. Stein, A. Eichler&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Deutsches Elektronen-Synchrotron DESY&lt;/p&gt;
&lt;p&gt;&lt;em&gt;39th International Conference on Machine Learning&lt;/em&gt;&lt;/p&gt;
&lt;h2 id="abstract"&gt;Abstract&lt;/h2&gt;
&lt;p&gt;In recent work, it has been shown that reinforcement learning (RL) is capable of solving a variety of problems at sometimes super-human performance levels. But despite continued advances in the field, applying RL to complex real-world control and optimisation problems has proven difficult. In this contribution, we demonstrate how to successfully apply RL to the optimisation of a highly complex real-world machine – specifically a linear particle accelerator – in an only partially observable setting and without requiring training on the real machine. Our method outperforms conventional optimisation algorithms in both the achieved result and time taken as well as already achieving close to human-level performance. We expect that such automation of machine optimisation will push the limits of operability, increase machine availability and lead to a paradigm shift in how such machines are operated, ultimately facilitating advances in a variety of fields, such as science and medicine among many others.&lt;/p&gt;</description></item><item><title>Automated Intensity Optimisation Using Reinforcement Learning at LEIR</title><link>https://RL4aa.github.io/posts/publications/madysa2022automated/</link><pubDate>Sun, 12 Jun 2022 00:00:00 +0000</pubDate><guid>https://RL4aa.github.io/posts/publications/madysa2022automated/</guid><description>&lt;p&gt;&lt;em&gt;&lt;strong&gt;N. Madysa, V. Kain, R. Alemany Fernandez, N. Biancacci, B. Goddard, F. M. Velotti&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;CERN&lt;/p&gt;
&lt;p&gt;&lt;em&gt;13th Particle Accelerator Conference&lt;/em&gt;&lt;/p&gt;
&lt;h2 id="abstract"&gt;Abstract&lt;/h2&gt;
&lt;p&gt;High intensities in the Low Energy Ion Ring (LEIR) at CERN are achieved by stacking several multi-turn injec- tions from the pre-accelerator Linac3. Up to seven consec- utive 200 μs long, 200 ms spaced pulses are injected from Linac3 into LEIR. Two inclined septa, one magnetic and one electrostatic, combined with a collapsing horizontal or- bit bump allows a 6-D phase space painting via a linearly ramped mean momentum along the Linac3 pulse and in- jection at high dispersion. The already circulating beam is cooled and dragged longitudinally via electron cooling (e- cooling) into a stacking momentum to free space for the fol- lowing injections. For optimal intensity accumulation, the electron energy and trajectory need to match the ion energy and orbit at the e-cooler section.&lt;/p&gt;</description></item><item><title>Test of Machine Learning at the CERN LINAC4</title><link>https://RL4aa.github.io/posts/publications/kain2021test/</link><pubDate>Mon, 04 Oct 2021 00:00:00 +0000</pubDate><guid>https://RL4aa.github.io/posts/publications/kain2021test/</guid><description>&lt;p&gt;&lt;em&gt;&lt;strong&gt;V. Kain&lt;sup&gt;1&lt;/sup&gt;, N. Bruchon&lt;sup&gt;1&lt;/sup&gt;, S. Hirlander&lt;sup&gt;1&lt;/sup&gt;, N. Madysa&lt;sup&gt;1&lt;/sup&gt;, I. Vojskovic&lt;sup&gt;1&lt;/sup&gt;, P. Skowronski&lt;sup&gt;1&lt;/sup&gt;, G. Valentino&lt;sup&gt;2&lt;/sup&gt;&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;sup&gt;1&lt;/sup&gt;CERN, &lt;sup&gt;2&lt;/sup&gt;University of Malta&lt;/p&gt;
&lt;p&gt;&lt;em&gt;61st ICFA ABDW on High-Intensity and High-Brightness Hadron Beams&lt;/em&gt;&lt;/p&gt;
&lt;h2 id="abstract"&gt;Abstract&lt;/h2&gt;
&lt;p&gt;The CERN H−linear accelerator, LINAC4, served as atest bed for advanced algorithms during the CERN LongShutdown 2 in the years 2019/20. One of the main goals wasto show that reinforcement learning with all its benefits canbe used as a replacement for numerical optimization and asa complement to classical control in the accelerator controlcontext. Many of the algorithms used were prepared before-hand at the electron line of the AWAKE facility to makethe best use of the limited time available at LINAC4. Anoverview of the algorithms and concepts tested at LINAC4and AWAKE will be given and the results discussed.&lt;/p&gt;</description></item><item><title>First Steps Toward an Autonomous Accelerator, A Common Project Between DESY and KIT</title><link>https://RL4aa.github.io/posts/publications/eichler2021first/</link><pubDate>Mon, 24 May 2021 00:00:00 +0000</pubDate><guid>https://RL4aa.github.io/posts/publications/eichler2021first/</guid><description>&lt;p&gt;&lt;em&gt;&lt;strong&gt;A. Eichler&lt;sup&gt;1&lt;/sup&gt;, F. Burkart&lt;sup&gt;1&lt;/sup&gt;, J. Kaiser&lt;sup&gt;1&lt;/sup&gt;, W. Kuropka&lt;sup&gt;1&lt;/sup&gt;, O. Stein&lt;sup&gt;1&lt;/sup&gt;, E. Bründermann&lt;sup&gt;2&lt;/sup&gt;, A. Santamaria Garcia&lt;sup&gt;2&lt;/sup&gt;, C. Xu&lt;sup&gt;2&lt;/sup&gt;&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;sup&gt;1&lt;/sup&gt;Deutsches Elektronen-Synchrotron DESY, &lt;sup&gt;2&lt;/sup&gt;Karlsruhe Institute of Technology KIT&lt;/p&gt;
&lt;p&gt;&lt;em&gt;12th International Particle Accelerator Conference&lt;/em&gt;&lt;/p&gt;
&lt;h2 id="abstract"&gt;Abstract&lt;/h2&gt;
&lt;p&gt;Reinforcement learning algorithms have risen in pop-ularity in the accelerator physics community in recentyears, showing potential in beam control and in the opti-mization and automation of tasks in accelerator operation.The Helmholtz AI project “Machine Learning Toward Au-tonomous Accelerators” is a collaboration between DESYand KIT that works on investigating and developing rein-forcement learning applications for the automatic start-upof electron linear accelerators. The work is carried out inparallel at two similar research accelerators: ARES at DESYand FLUTE at KIT, giving the unique opportunity of trans-fer learning between facilities. One of the first steps of thisproject is the establishment of a common interface betweenthe simulations and the machine, in order to test and applyvarious optimization approaches interchangeably betweenthe two accelerators. In this paper we present first results onthe common interface and its application to beam focusingin ARES as well as the idea of laser shaping with spatiallight modulators at FLUTE.&lt;/p&gt;</description></item><item><title>Physics-Enhanced Reinforcement Learning for Optimal Control</title><link>https://RL4aa.github.io/posts/publications/ivanov2021physicsenhanced/</link><pubDate>Fri, 21 May 2021 00:00:00 +0000</pubDate><guid>https://RL4aa.github.io/posts/publications/ivanov2021physicsenhanced/</guid><description>&lt;p&gt;&lt;em&gt;&lt;strong&gt;A. Ivanov, I. Agapov, A. Eichler, S. Tomin&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Deutsches Elektronen Synchrotron DESY&lt;/p&gt;
&lt;p&gt;&lt;em&gt;12th International Particle Accelerator Conference&lt;/em&gt;&lt;/p&gt;
&lt;h2 id="abstract"&gt;Abstract&lt;/h2&gt;
&lt;p&gt;We propose an approach for incorporating acceleratorphysics models into reinforcement learning agents. The proposed approach is based on the Taylor mapping technique for the simulation of particle dynamics. The resulting computational graph is represented as a polynomial neural network and embedded into the traditional reinforcement learning agents. The application of the model is demonstrated in a nonlinear simulation model of beam transmission. The comparison of the approach with the traditional numerical optimization as well as neural networks-based agents demonstrates better convergence of the proposed technique.&lt;/p&gt;</description></item><item><title>Toward the Application of Reinforcement Learning to the Intensity Control of a Seeded Free-Electron Laser</title><link>https://RL4aa.github.io/posts/publications/bruchon2019toward/</link><pubDate>Wed, 23 Oct 2019 00:00:00 +0000</pubDate><guid>https://RL4aa.github.io/posts/publications/bruchon2019toward/</guid><description>&lt;p&gt;&lt;em&gt;&lt;strong&gt;N. Bruchon, G. Fenu, G. Gaio, M. Lonza, F. A. Pellegrino, E. Salvato&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;University of Trieste&lt;/p&gt;
&lt;p&gt;&lt;em&gt;23rd International Conference on Mechatronics Technology&lt;/em&gt;&lt;/p&gt;
&lt;h2 id="abstract"&gt;Abstract&lt;/h2&gt;
&lt;p&gt;The optimization of particle accelerators is a challenging task, and many different approaches have been proposed in years, to obtain an optimal tuning of the plant and to keep it optimally tuned despite drifts or disturbances. Indeed, the classical model-free approaches (such as Gradient Ascent or Extremum Seeking algorithms) have intrinsic limitations. To overcome those limitations, Machine Learning techniques, in particular, the Reinforcement Learning, are attracting more and more attention in the particle accelerator community. The purpose of this paper is to apply a Reinforcement Learning model-free approach to the alignment of a seed laser, based on a rather general target function depending on the laser trajectory. The study focuses on the alignment of the lasers at FERMI, the free-electron laser facility at Elettra Sincrotrone Trieste. In particular, we employ Q-learning with linear function approximation and report experimental results obtained in two setups, which are the actual setups where the final application has to be deployed. Despite the simplicity of the approach, we report satisfactory preliminary results, that represent the first step toward a fully automatic procedure for seed laser to the electron beam. Such a superimposition is, at present, performed manually.&lt;/p&gt;</description></item><item><title>Free-electron Laser Optimization with Reinforcement Learning</title><link>https://RL4aa.github.io/posts/publications/bruchon2019freeelectron/</link><pubDate>Sat, 05 Oct 2019 00:00:00 +0000</pubDate><guid>https://RL4aa.github.io/posts/publications/bruchon2019freeelectron/</guid><description>&lt;p&gt;&lt;em&gt;&lt;strong&gt;N. Bruchon&lt;sup&gt;1&lt;/sup&gt;, G. Gaio&lt;sup&gt;2&lt;/sup&gt;, G. Fenu&lt;sup&gt;1&lt;/sup&gt;, M. Lonza&lt;sup&gt;2&lt;/sup&gt;, F. A. Pellegrino&lt;sup&gt;1&lt;/sup&gt;, E. Salvato&lt;sup&gt;1&lt;/sup&gt;&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;sup&gt;1&lt;/sup&gt;University of Trieste, &lt;sup&gt;2&lt;/sup&gt;Elettra Sincrotrone Trieste&lt;/p&gt;
&lt;p&gt;&lt;em&gt;17th International Conference on Accelerator and Large Experimental Physics Control Systems&lt;/em&gt;&lt;/p&gt;
&lt;h2 id="abstract"&gt;Abstract&lt;/h2&gt;
&lt;p&gt;Reinforcement Learning (RL) is one of the most promis-ing techniques in Machine Learning because of its modestcomputational requirements with respect to other algorithms.RL uses an agent that takes actions within its environmentto maximize a reward related to the goal it is designed toachieve. We have recently used RL as a model-free approachto improve the performance of the FERMI Free ElectronLaser. A number of machine parameters are adjusted tofind the optimum FEL output in terms of intensity and spec-tral quality. In particular we focus on the problem of thealignment of the seed laser with the electron beam, initiallyusing a simplified model and then applying the developedalgorithm on the real machine. This paper reports the resultsobtained and discusses pros and cons of this approach withplans for future applications.&lt;/p&gt;</description></item><item><title>Feedback Design for Control of the Micro-Bunching Instability Based on Reinforcement Learning</title><link>https://RL4aa.github.io/posts/publications/boltz2019feedback/</link><pubDate>Sun, 19 May 2019 00:00:00 +0000</pubDate><guid>https://RL4aa.github.io/posts/publications/boltz2019feedback/</guid><description>&lt;p&gt;&lt;em&gt;&lt;strong&gt;T. Boltz, M. Brosi, E. Bründermann, B. Haerer, P. Kaiser, C. Pohl, P. Schreiber, M. Yan,T. Asfour, A.-S. Müller&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Karlsruhe Insitute of Technology KIT&lt;/p&gt;
&lt;p&gt;&lt;em&gt;10th International Particle Accelerator Conference&lt;/em&gt;&lt;/p&gt;
&lt;h2 id="abstract"&gt;Abstract&lt;/h2&gt;
&lt;p&gt;The operation of ring-based synchrotron light sourceswith short electron bunches increases the emission of co-herent synchrotron radiation (CSR) in the THz frequencyrange. However, the micro-bunching instability resultingfrom self-interaction of the bunch with its own radiationfield limits stable operation with constant intensity of CSRemission to a particular threshold current. Above this thresh-old, the longitudinal charge distribution and thus the emittedradiation vary rapidly and continuously. Therefore, a fastand adaptive feedback system is the appropriate approach tostabilize the dynamics and to overcome the limitations givenby the instability. In this contribution, we discuss first effortstowards a longitudinal feedback design that acts on the RFsystem of the KIT storage ring KARA (Karlsruhe ResearchAccelerator) and aims for stabilization of the emitted THzradiation. Our approach is based on methods of adaptive con-trol that were developed in the field of reinforcement learningand have seen great success in other fields of research overthe past decade. We motivate this particular approach andcomment on different aspects of its implementation.&lt;/p&gt;</description></item><item><title>Using a neural network control policy for rapid switching between beam parameters in an FEL</title><link>https://RL4aa.github.io/posts/publications/edelen2017using/</link><pubDate>Fri, 25 Aug 2017 00:00:00 +0000</pubDate><guid>https://RL4aa.github.io/posts/publications/edelen2017using/</guid><description>&lt;p&gt;&lt;em&gt;&lt;strong&gt;A. L. Edelen&lt;sup&gt;1&lt;/sup&gt;, S. G. Biedron&lt;sup&gt;2&lt;/sup&gt;, J. P. Edelen&lt;sup&gt;3&lt;/sup&gt;, S. V. Milton&lt;sup&gt;4&lt;/sup&gt;, P. J. M. van der Slot&lt;sup&gt;5&lt;/sup&gt;&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;sup&gt;1&lt;/sup&gt;Colorado State University, &lt;sup&gt;2&lt;/sup&gt;Element Aero, &lt;sup&gt;3&lt;/sup&gt;Fermi National Accelerator Laboratory, &lt;sup&gt;4&lt;/sup&gt;Los Alamos National Laboratory, &lt;sup&gt;5&lt;/sup&gt;University of Twente&lt;/p&gt;
&lt;p&gt;&lt;em&gt;38th International Free Electron Laser Conference&lt;/em&gt;&lt;/p&gt;
&lt;h2 id="abstract"&gt;Abstract&lt;/h2&gt;
&lt;p&gt;FEL user facilities often must accommodate requests for a variety of beam parameters. This usually requires skilled operators to tune the machine, reducing the amount of available time for users. In principle, a neural network control policy that is trained on a broad range of operating states could be used to quickly switch between these requests without substantial need for human inter-vention. We present preliminary results from an ongoing study in which a neural network control policy is investi-gated for rapid switching between beam parameters in a compact THz FEL.&lt;/p&gt;</description></item></channel></rss>