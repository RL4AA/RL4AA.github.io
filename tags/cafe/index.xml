<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/"><channel><title>CAFe on RL4AA Collaboration | Homepage</title><link>https://RL4aa.github.io/tags/cafe/</link><description>Recent content in CAFe on RL4AA Collaboration | Homepage</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Tue, 23 May 2023 00:00:00 +0000</lastBuildDate><atom:link href="https://RL4aa.github.io/tags/cafe/index.xml" rel="self" type="application/rss+xml"/><item><title>Trend-Based SAC Beam Control Method with Zero-Shot in Superconducting Linear Accelerator</title><link>https://RL4aa.github.io/posts/publications/chen2023trendbased/</link><pubDate>Tue, 23 May 2023 00:00:00 +0000</pubDate><guid>https://RL4aa.github.io/posts/publications/chen2023trendbased/</guid><description>X. Chen, X. Qi, C. Su, Y. He, Z. Wang, K. Sun, C. Jin, W. Chen, S. Liu, X. Zhao, D. Jia, M. Yi
Chinese Academy of Sciences
arXiv
Abstract The superconducting linear accelerator is a highly flexiable facility for modern scientific discoveries, necessitating weekly reconfiguration and tuning. Accordingly, minimizing setup time proves essential in affording users with ample experimental time. We propose a trend-based soft actor-critic(TBSAC) beam control method with strong robustness, allowing the agents to be trained in a simulated environment and applied to the real accelerator directly with zero-shot.</description></item><item><title>Orbit Correction Based on Improved Reinforcement Learning Algorithm</title><link>https://RL4aa.github.io/posts/publications/chen2023orbit/</link><pubDate>Thu, 13 Apr 2023 00:00:00 +0000</pubDate><guid>https://RL4aa.github.io/posts/publications/chen2023orbit/</guid><description>X. Chen, Y. Jia, X. Qi, Z. Wang, Y. He
Chinese Academy of Sciences
Physical Review Accelerators and Beams
Abstract Recently, reinforcement learning (RL) algorithms have been applied to a wide range of control problems in accelerator commissioning. In order to achieve efficient and fast control, these algorithms need to be highly efficient, so as to minimize the online training time. In this paper, we incorporated the beam position monitor trend into the observation space of the twin delayed deep deterministic policy gradient (TD3) algorithm and trained two different structure agents, one based on physical prior knowledge and the other using the original TD3 network architecture.</description></item></channel></rss>