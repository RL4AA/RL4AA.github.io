<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/"><channel><title>CAFe on RL4AA Collaboration | Homepage</title><link>https://RL4aa.github.io/tags/cafe/</link><description>Recent content in CAFe on RL4AA Collaboration | Homepage</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Thu, 13 Apr 2023 00:00:00 +0000</lastBuildDate><atom:link href="https://RL4aa.github.io/tags/cafe/index.xml" rel="self" type="application/rss+xml"/><item><title>Orbit Correction Based on Improved Reinforcement Learning Algorithm</title><link>https://RL4aa.github.io/posts/publications/chen2023orbit/</link><pubDate>Thu, 13 Apr 2023 00:00:00 +0000</pubDate><guid>https://RL4aa.github.io/posts/publications/chen2023orbit/</guid><description>X. Chen, Y. Jia, X. Qi, Z. Wang, Y. He
Chinese Academy of Sciences
Physical Review Accelerators and Beams
Abstract Recently, reinforcement learning (RL) algorithms have been applied to a wide range of control problems in accelerator commissioning. In order to achieve efficient and fast control, these algorithms need to be highly efficient, so as to minimize the online training time. In this paper, we incorporated the beam position monitor trend into the observation space of the twin delayed deep deterministic policy gradient (TD3) algorithm and trained two different structure agents, one based on physical prior knowledge and the other using the original TD3 network architecture.</description></item></channel></rss>