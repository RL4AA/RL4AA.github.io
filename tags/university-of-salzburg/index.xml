<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/"><channel><title>University of Salzburg on RL4AA Collaboration | Homepage</title><link>https://RL4aa.github.io/tags/university-of-salzburg/</link><description>Recent content in University of Salzburg on RL4AA Collaboration | Homepage</description><generator>Hugo -- 0.152.1</generator><language>en-us</language><lastBuildDate>Fri, 16 Feb 2024 00:00:00 +0000</lastBuildDate><atom:link href="https://RL4aa.github.io/tags/university-of-salzburg/index.xml" rel="self" type="application/rss+xml"/><item><title>Successful RL4AA'24 workshop in Salzburg: Thanks everyone for joining!</title><link>https://RL4aa.github.io/posts/announcements/rl4aa24_after/</link><pubDate>Fri, 16 Feb 2024 00:00:00 +0000</pubDate><guid>https://RL4aa.github.io/posts/announcements/rl4aa24_after/</guid><description>&lt;p&gt;From 5 to 7 February 2024, &lt;a href="https://www.plus.ac.at/aihi/der-fachbereich/ida-lab/"&gt;IDA Lab&lt;/a&gt; at the &lt;a href="https://www.plus.ac.at/?lang=en"&gt;Paris Lodron University of Salzburg&lt;/a&gt; kindly hosted the RL4AA community for the 2nd workshop on Reinforcement Learning for Autonomous Accelerators (&lt;a href="https://rl4aa.github.io/RL4AA24/"&gt;RL4AA'24&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;With over 50 participants from more than 10 different countries, we are excited to see that our community is growing and that the interest in reinforcement learning (for particle accelerators) is increasing.
In a total of 19 talks, we got to hear about the latest developments and impressive results in the field.&lt;/p&gt;</description></item><item><title>Towards automatic setup of 18 MeV electron beamline using machine learning</title><link>https://RL4aa.github.io/posts/publications/velotti2023towards/</link><pubDate>Thu, 27 Apr 2023 00:00:00 +0000</pubDate><guid>https://RL4aa.github.io/posts/publications/velotti2023towards/</guid><description>&lt;p&gt;&lt;em&gt;&lt;strong&gt;F. M. Velotti&lt;sup&gt;1&lt;/sup&gt;, B. Goddard&lt;sup&gt;1&lt;/sup&gt;, V. Kain&lt;sup&gt;1&lt;/sup&gt;, R. Ramjiawan&lt;sup&gt;1&lt;/sup&gt;, G. Z. Della Porta&lt;sup&gt;1&lt;/sup&gt; and S. Hirlaender&lt;sup&gt;2&lt;/sup&gt;&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;sup&gt;1&lt;/sup&gt;CERN, &lt;sup&gt;2&lt;/sup&gt;University of Salzburg&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Machine Learning: Science and Technology&lt;/em&gt;&lt;/p&gt;
&lt;h2 id="abstract"&gt;Abstract&lt;/h2&gt;
&lt;p&gt;To improve the performance-critical stability and brightness of the electron bunch at injection into the proton-driven plasma wakefield at the AWAKE CERN experiment, automation approaches based on unsupervised machine learning (ML) were developed and deployed. Numerical optimisers were tested together with different model-free reinforcement learning (RL) agents. In order to avoid any bias, RL agents have been trained also using a completely unsupervised state encoding using auto-encoders. To aid hyper-parameter selection, a full synthetic model of the beamline was constructed using a variational auto-encoder trained to generate surrogate data from equipment settings. This paper describes the novel approaches based on deep learning and RL to aid the automatic setup of a low energy line, as the one used to deliver beam to the AWAKE facility. The results obtained with the different ML approaches, including automatic unsupervised feature extraction from images using computer vision are presented. The prospects for operational deployment and wider applicability are discussed.&lt;/p&gt;</description></item><item><title>Application of reinforcement learning in the LHC tune feedback</title><link>https://RL4aa.github.io/posts/publications/grech2022application/</link><pubDate>Wed, 07 Sep 2022 00:00:00 +0000</pubDate><guid>https://RL4aa.github.io/posts/publications/grech2022application/</guid><description>&lt;p&gt;&lt;em&gt;&lt;strong&gt;L. Grech&lt;sup&gt;1&lt;/sup&gt;, G. Valentino&lt;sup&gt;1&lt;/sup&gt;, D. Alves&lt;sup&gt;2&lt;/sup&gt; and Simon Hirlaender&lt;sup&gt;3&lt;/sup&gt;&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;sup&gt;1&lt;/sup&gt;University of Malta, &lt;sup&gt;2&lt;/sup&gt;CERN, &lt;sup&gt;3&lt;/sup&gt;University of Salzburg&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Frontiers in Physics&lt;/em&gt;&lt;/p&gt;
&lt;h2 id="abstract"&gt;Abstract&lt;/h2&gt;
&lt;p&gt;The Beam-Based Feedback System (BBFS) was primarily responsible for correcting the beam energy, orbit and tune in the CERN Large Hadron Collider (LHC). A major code renovation of the BBFS was planned and carried out during the LHC Long Shutdown 2 (LS2). This work consists of an explorative study to solve a beam-based control problem, the tune feedback (QFB), utilising state-of-the-art Reinforcement Learning (RL). A simulation environment was created to mimic the operation of the QFB. A series of RL agents were trained, and the best-performing agents were then subjected to a set of well-designed tests. The original feedback controller used in the QFB was reimplemented to compare the performance of the classical approach to the performance of selected RL agents in the test scenarios. Results from the simulated environment show that the RL agent performance can exceed the controller-based paradigm.&lt;/p&gt;</description></item><item><title>Model-free and Bayesian Ensembling Model-based Deep Reinforcement Learning for Particle Accelerator Control Demonstrated on the FERMI FEL</title><link>https://RL4aa.github.io/posts/publications/hirlaender2020modelfree/</link><pubDate>Thu, 17 Dec 2020 00:00:00 +0000</pubDate><guid>https://RL4aa.github.io/posts/publications/hirlaender2020modelfree/</guid><description>&lt;p&gt;&lt;em&gt;&lt;strong&gt;S. Hirlaender&lt;sup&gt;1&lt;/sup&gt;, N. Bruchon&lt;sup&gt;2&lt;/sup&gt;&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;sup&gt;1&lt;/sup&gt;University of Salzburg, &lt;sup&gt;2&lt;/sup&gt;University of Trieste&lt;/p&gt;
&lt;p&gt;&lt;em&gt;arXiv&lt;/em&gt;&lt;/p&gt;
&lt;h2 id="abstract"&gt;Abstract&lt;/h2&gt;
&lt;p&gt;Reinforcement learning holds tremendous promise in accelerator controls. The primary goal of this paper is to show how this approach can be utilised on an operational level on accelerator physics problems. Despite the success of model-free reinforcement learning in several domains, sample-efficiency still is a bottle-neck, which might be encompassed by model-based methods. We compare well-suited purely model-based to model-free reinforcement learning applied to the intensity optimisation on the FERMI FEL system. We find that the model-based approach demonstrates higher representational power and sample-efficiency, while the asymptotic performance of the model-free method is slightly superior. The model-based algorithm is implemented in a DYNA-style using an uncertainty aware model, and the model-free algorithm is based on tailored deep Q-learning. In both cases, the algorithms were implemented in a way, which presents increased noise robustness as omnipresent in accelerator control problems.&lt;/p&gt;</description></item></channel></rss>