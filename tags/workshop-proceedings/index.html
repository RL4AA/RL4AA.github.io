<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Workshop Proceedings | RL4AA Collaboration | Homepage</title><meta name=keywords content><meta name=description content="Reinforcement Learning for Autonomous Accelerator Collaboration - https://github.com/RL4AA"><meta name=author content="RL4AA Collaboration"><link rel=canonical href=https://RL4aa.github.io/tags/workshop-proceedings/><link crossorigin=anonymous href=/assets/css/stylesheet.a090830a421002426baafbd314e38f149d77b4c48a12ee9312700d770b27fb26.css integrity="sha256-oJCDCkIQAkJrqvvTFOOPFJ13tMSKEu6TEnANdwsn+yY=" rel="preload stylesheet" as=style><link rel=icon href=https://RL4aa.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://RL4aa.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://RL4aa.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://RL4aa.github.io/apple-touch-icon.png><link rel=mask-icon href=https://RL4aa.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate type=application/rss+xml href=https://RL4aa.github.io/tags/workshop-proceedings/index.xml title=rss><link rel=alternate hreflang=en href=https://RL4aa.github.io/tags/workshop-proceedings/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><meta property="og:url" content="https://RL4aa.github.io/tags/workshop-proceedings/"><meta property="og:site_name" content="RL4AA Collaboration | Homepage"><meta property="og:title" content="Workshop Proceedings"><meta property="og:description" content="Reinforcement Learning for Autonomous Accelerator Collaboration - https://github.com/RL4AA"><meta property="og:locale" content="en-us"><meta property="og:type" content="website"><meta name=twitter:card content="summary"><meta name=twitter:title content="Workshop Proceedings"><meta name=twitter:description content="Reinforcement Learning for Autonomous Accelerator Collaboration - https://github.com/RL4AA"></head><body class=list id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://RL4aa.github.io/ accesskey=h title="RL4AA Collaboration | Homepage (Alt + H)"><img src=https://RL4aa.github.io/imgs/rl4aa_logo.png alt aria-label=logo height=30>RL4AA Collaboration | Homepage</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://RL4aa.github.io/categories/announcements title=Announcements><span>Announcements</span></a></li><li><a href=https://RL4aa.github.io/categories/publications title=Publications><span>Publications</span></a></li><li><a href=https://RL4aa.github.io/tags/ title=Tags><span>Tags</span></a></li><li><a href=https://RL4aa.github.io/archives/ title=Archive><span>Archive</span></a></li><li><a href=https://RL4aa.github.io/aboutus/ title="About Us"><span>About Us</span></a></li></ul></nav></header><main class=main><header class=page-header><div class=breadcrumbs><a href=https://RL4aa.github.io/>Home</a>&nbsp;»&nbsp;<a href=https://RL4aa.github.io/tags/>Tags</a></div><h1>Workshop Proceedings</h1></header><article class="post-entry tag-entry"><figure class=entry-cover><img loading=lazy src=https://RL4aa.github.io/imgs/pang2020autonomous.png alt="Policy network maps states to actions."></figure><header class=entry-header><h2 class=entry-hint-parent>Autonomous Control of a Particle Accelerator using Deep Reinforcement Learning</h2></header><div class=entry-content><p>X. Pang1, S. Thulasidasan2, L. Rybarcyk2
1Apple, 2Los Alamos National Laboratory
Machine Learning for Engineering Modeling, Simulation, and Design Workshop at Neural Information Processing Systems 2020
Abstract We describe an approach to learning optimal control policies for a large, linear particle accelerator using deep reinforcement learning coupled with a high-fidelity physics engine. The framework consists of an AI controller that uses deep neural networks for state and action-space representation and learns optimal policies using reward signals that are provided by the physics simulator. For this work, we only focus on controlling a small section of the entire accelerator. Nevertheless, initial results indicate that we can achieve better-than-human level performance in terms of particle beam current and distribution. The ultimate goal of this line of work is to substantially reduce the tuning time for such facilities by orders of magnitude, and achieve near-autonomous control.
...</p></div><footer class=entry-footer><span title='2020-12-12 00:00:00 +0000 UTC'>December 12, 2020</span>&nbsp;·&nbsp;<span>150 words</span>&nbsp;·&nbsp;<span>RL4AA Collaboration</span></footer><a class=entry-link aria-label="post link to Autonomous Control of a Particle Accelerator using Deep Reinforcement Learning" href=https://RL4aa.github.io/posts/publications/pang2020autonomous/></a></article><article class="post-entry tag-entry"><figure class=entry-cover><img loading=lazy src=https://RL4aa.github.io/imgs/edelen2017using2.png alt="General setup for the neural network model."></figure><header class=entry-header><h2 class=entry-hint-parent>Using Neural Network Control Policies For Rapid Switching Between Beam Parameters in a Free Electron Laser</h2></header><div class=entry-content><p>A. L. Edelen1, S. G. Biedron2, J. P. Edelen3, S. V. Milton4, P. J. M. van der Slot5
1Colorado State University, 2Element Aero, 3Fermi National Accelerator Laboratory, 4Los Alamos National Laboratory, 5University of Twente
Workshop on Deep Learning for Physical Sciences at the Conference on Neural Information Processing Systems 2017
Abstract Free Electron Laser (FEL) facilities often must accommodate requests for a varietyof electron beam parameters in order to supply scientific users with appropriatephoton beam characteristics. This usually requires skilled human operators to tunethe machine. In principle, a neural network control policy that is trained on a broadrange of machine operating states could be used to quickly switch between theserequests without substantial need for human intervention. We present preliminaryresults from an ongoing simulation study in which a neural network control policyis investigated for rapid switching between beam parameters in a compact THzFEL that exhibits nonlinear electron beam dynamics. To accomplish this, we firsttrain a feed-forward neural network to mimic a physics-based simulation of theFEL. We then train a neural network control policy by first pre-training it as aninverse model (using supervised learning with a subset of the simulation data) andthen training it more extensively with reinforcement learning. In this case, thereinforcement learning component consists of letting the policy network interactwith the learned system model and backpropagating the cost through the modelnetwork to the controller network.
...</p></div><footer class=entry-footer><span title='2017-08-25 00:00:00 +0000 UTC'>August 25, 2017</span>&nbsp;·&nbsp;<span>232 words</span>&nbsp;·&nbsp;<span>RL4AA Collaboration</span></footer><a class=entry-link aria-label="post link to Using Neural Network Control Policies For Rapid Switching Between Beam Parameters in a Free Electron Laser" href=https://RL4aa.github.io/posts/publications/edelen2017using2/></a></article></main><footer class=footer><span>&copy; 2025 <a href=https://RL4aa.github.io/>RL4AA Collaboration | Homepage</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>