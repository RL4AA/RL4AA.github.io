<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/"><channel><title>University of Malta on RL4AA Collaboration | Homepage</title><link>https://RL4aa.github.io/tags/university-of-malta/</link><description>Recent content in University of Malta on RL4AA Collaboration | Homepage</description><generator>Hugo -- 0.152.1</generator><language>en-us</language><lastBuildDate>Wed, 07 Sep 2022 00:00:00 +0000</lastBuildDate><atom:link href="https://RL4aa.github.io/tags/university-of-malta/index.xml" rel="self" type="application/rss+xml"/><item><title>Application of reinforcement learning in the LHC tune feedback</title><link>https://RL4aa.github.io/posts/publications/grech2022application/</link><pubDate>Wed, 07 Sep 2022 00:00:00 +0000</pubDate><guid>https://RL4aa.github.io/posts/publications/grech2022application/</guid><description>&lt;p&gt;&lt;em&gt;&lt;strong&gt;L. Grech&lt;sup&gt;1&lt;/sup&gt;, G. Valentino&lt;sup&gt;1&lt;/sup&gt;, D. Alves&lt;sup&gt;2&lt;/sup&gt; and Simon Hirlaender&lt;sup&gt;3&lt;/sup&gt;&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;sup&gt;1&lt;/sup&gt;University of Malta, &lt;sup&gt;2&lt;/sup&gt;CERN, &lt;sup&gt;3&lt;/sup&gt;University of Salzburg&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Frontiers in Physics&lt;/em&gt;&lt;/p&gt;
&lt;h2 id="abstract"&gt;Abstract&lt;/h2&gt;
&lt;p&gt;The Beam-Based Feedback System (BBFS) was primarily responsible for correcting the beam energy, orbit and tune in the CERN Large Hadron Collider (LHC). A major code renovation of the BBFS was planned and carried out during the LHC Long Shutdown 2 (LS2). This work consists of an explorative study to solve a beam-based control problem, the tune feedback (QFB), utilising state-of-the-art Reinforcement Learning (RL). A simulation environment was created to mimic the operation of the QFB. A series of RL agents were trained, and the best-performing agents were then subjected to a set of well-designed tests. The original feedback controller used in the QFB was reimplemented to compare the performance of the classical approach to the performance of selected RL agents in the test scenarios. Results from the simulated environment show that the RL agent performance can exceed the controller-based paradigm.&lt;/p&gt;</description></item><item><title>Test of Machine Learning at the CERN LINAC4</title><link>https://RL4aa.github.io/posts/publications/kain2021test/</link><pubDate>Mon, 04 Oct 2021 00:00:00 +0000</pubDate><guid>https://RL4aa.github.io/posts/publications/kain2021test/</guid><description>&lt;p&gt;&lt;em&gt;&lt;strong&gt;V. Kain&lt;sup&gt;1&lt;/sup&gt;, N. Bruchon&lt;sup&gt;1&lt;/sup&gt;, S. Hirlander&lt;sup&gt;1&lt;/sup&gt;, N. Madysa&lt;sup&gt;1&lt;/sup&gt;, I. Vojskovic&lt;sup&gt;1&lt;/sup&gt;, P. Skowronski&lt;sup&gt;1&lt;/sup&gt;, G. Valentino&lt;sup&gt;2&lt;/sup&gt;&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;sup&gt;1&lt;/sup&gt;CERN, &lt;sup&gt;2&lt;/sup&gt;University of Malta&lt;/p&gt;
&lt;p&gt;&lt;em&gt;61st ICFA ABDW on High-Intensity and High-Brightness Hadron Beams&lt;/em&gt;&lt;/p&gt;
&lt;h2 id="abstract"&gt;Abstract&lt;/h2&gt;
&lt;p&gt;The CERN Hâˆ’linear accelerator, LINAC4, served as atest bed for advanced algorithms during the CERN LongShutdown 2 in the years 2019/20. One of the main goals wasto show that reinforcement learning with all its benefits canbe used as a replacement for numerical optimization and asa complement to classical control in the accelerator controlcontext. Many of the algorithms used were prepared before-hand at the electron line of the AWAKE facility to makethe best use of the limited time available at LINAC4. Anoverview of the algorithms and concepts tested at LINAC4and AWAKE will be given and the results discussed.&lt;/p&gt;</description></item><item><title>Renovation of the beam-based feedback systems in the LHC</title><link>https://RL4aa.github.io/posts/publications/grech2021renovation/</link><pubDate>Wed, 01 Sep 2021 00:00:00 +0000</pubDate><guid>https://RL4aa.github.io/posts/publications/grech2021renovation/</guid><description>&lt;p&gt;&lt;em&gt;&lt;strong&gt;L. Grech&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;University of Malta&lt;/p&gt;
&lt;p&gt;&lt;em&gt;PhD thesis&lt;/em&gt;&lt;/p&gt;
&lt;h2 id="abstract"&gt;Abstract&lt;/h2&gt;
&lt;p&gt;The Large Hadron Collider (LHC) at the European Organization for Nuclear Research (CERN) is the largest synchrotron built to date, having a circumference of approx- imately 27km. The LHC is able to accelerate two counter-rotating proton and/or heavy-ion beams up to 7 TeV per charge. These highly energetic beams are contained inside a vacuum chamber with an inner diameter of 80 mm by means of strong mag- netic fields produced by superconducting magnets. A beam cleaning and machine protection system is in place to prevent high-energy halo particles from impacting and heating the superconducting magnets.&lt;/p&gt;</description></item><item><title>Sample-efficient reinforcement learning for CERN accelerator control</title><link>https://RL4aa.github.io/posts/publications/kain2020sampleefficient/</link><pubDate>Tue, 01 Dec 2020 00:00:00 +0000</pubDate><guid>https://RL4aa.github.io/posts/publications/kain2020sampleefficient/</guid><description>&lt;p&gt;&lt;em&gt;&lt;strong&gt;V. Kain&lt;sup&gt;1&lt;/sup&gt;, S. Hirlander&lt;sup&gt;1&lt;/sup&gt;, B. Goddard&lt;sup&gt;1&lt;/sup&gt;, F. M. Velotti&lt;sup&gt;1&lt;/sup&gt;, G. Z. Della Porta&lt;sup&gt;1&lt;/sup&gt;, N. Bruchon&lt;sup&gt;2&lt;/sup&gt;, G. Valentino&lt;sup&gt;3&lt;/sup&gt;&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;sup&gt;1&lt;/sup&gt;CERN, &lt;sup&gt;2&lt;/sup&gt;University of Trieste, &lt;sup&gt;3&lt;/sup&gt;University of Malta&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Physical Review Accelerators and Beams&lt;/em&gt;&lt;/p&gt;
&lt;h2 id="abstract"&gt;Abstract&lt;/h2&gt;
&lt;p&gt;Numerical optimization algorithms are already established tools to increase and stabilize the performance of particle accelerators. These algorithms have many advantages, are available out of the box, and can be adapted to a wide range of optimization problems in accelerator operation. The next boost in efficiency is expected to come from reinforcement learning algorithms that learn the optimal policy for a certain control problem and hence, once trained, can do without the time-consuming exploration phase needed for numerical optimizers. To investigate this approach, continuous model-free reinforcement learning with up to 16 degrees of freedom was developed and successfully tested at various facilities at CERN. The approach and algorithms used are discussed and the results obtained for trajectory steering at the AWAKE electron line and LINAC4 are presented. The necessary next steps, such as uncertainty aware model-based approaches, and the potential for future applications at particle accelerators are addressed.&lt;/p&gt;</description></item></channel></rss>