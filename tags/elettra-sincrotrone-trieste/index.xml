<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/"><channel><title>Elettra-Sincrotrone Trieste on RL4AA Collaboration | Homepage</title><link>https://RL4aa.github.io/tags/elettra-sincrotrone-trieste/</link><description>Recent content in Elettra-Sincrotrone Trieste on RL4AA Collaboration | Homepage</description><generator>Hugo -- 0.152.1</generator><language>en-us</language><lastBuildDate>Mon, 21 Dec 2020 00:00:00 +0000</lastBuildDate><atom:link href="https://RL4aa.github.io/tags/elettra-sincrotrone-trieste/index.xml" rel="self" type="application/rss+xml"/><item><title>Policy gradient methods for free-electron laser and terahertz source optimization and stabilization at the FERMI free-electron laser at Elettra</title><link>https://RL4aa.github.io/posts/publications/oshea202policy/</link><pubDate>Mon, 21 Dec 2020 00:00:00 +0000</pubDate><guid>https://RL4aa.github.io/posts/publications/oshea202policy/</guid><description>&lt;p&gt;&lt;em&gt;&lt;strong&gt;F. H. O&amp;rsquo;Shea&lt;sup&gt;1&lt;/sup&gt;, N. Bruchon&lt;sup&gt;2&lt;/sup&gt;, G. Gaio&lt;sup&gt;1&lt;/sup&gt;&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;sup&gt;1&lt;/sup&gt;Elettra Sincrotrone Trieste, &lt;sup&gt;2&lt;/sup&gt;University of Trieste&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Physical Review Accelerators and Beams&lt;/em&gt;&lt;/p&gt;
&lt;h2 id="abstract"&gt;Abstract&lt;/h2&gt;
&lt;p&gt;In this article we report on the application of a model-free reinforcement learning method to the optimization of accelerator systems. We simplify a policy gradient algorithm to accelerator control from sophisticated algorithms that have recently been demonstrated to solve complex dynamic problems. After outlining a theoretical basis for the functioning of the algorithm, we explore the small hyperparameter space to develop intuition about said parameters using a simple number-guess environment. Finally, we demonstrate the algorithm optimizing both a free-electron laser and an accelerator-based terahertz source in-situ. The algorithm is applied to different accelerator control systems and optimizes the desired signals in a few hundred steps without any domain knowledge using up to five control parameters. In addition, the algorithm shows modest tolerance to accelerator fault conditions without any special preparation for such conditions.&lt;/p&gt;</description></item><item><title>Model-free and Bayesian Ensembling Model-based Deep Reinforcement Learning for Particle Accelerator Control Demonstrated on the FERMI FEL</title><link>https://RL4aa.github.io/posts/publications/hirlaender2020modelfree/</link><pubDate>Thu, 17 Dec 2020 00:00:00 +0000</pubDate><guid>https://RL4aa.github.io/posts/publications/hirlaender2020modelfree/</guid><description>&lt;p&gt;&lt;em&gt;&lt;strong&gt;S. Hirlaender&lt;sup&gt;1&lt;/sup&gt;, N. Bruchon&lt;sup&gt;2&lt;/sup&gt;&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;sup&gt;1&lt;/sup&gt;University of Salzburg, &lt;sup&gt;2&lt;/sup&gt;University of Trieste&lt;/p&gt;
&lt;p&gt;&lt;em&gt;arXiv&lt;/em&gt;&lt;/p&gt;
&lt;h2 id="abstract"&gt;Abstract&lt;/h2&gt;
&lt;p&gt;Reinforcement learning holds tremendous promise in accelerator controls. The primary goal of this paper is to show how this approach can be utilised on an operational level on accelerator physics problems. Despite the success of model-free reinforcement learning in several domains, sample-efficiency still is a bottle-neck, which might be encompassed by model-based methods. We compare well-suited purely model-based to model-free reinforcement learning applied to the intensity optimisation on the FERMI FEL system. We find that the model-based approach demonstrates higher representational power and sample-efficiency, while the asymptotic performance of the model-free method is slightly superior. The model-based algorithm is implemented in a DYNA-style using an uncertainty aware model, and the model-free algorithm is based on tailored deep Q-learning. In both cases, the algorithms were implemented in a way, which presents increased noise robustness as omnipresent in accelerator control problems.&lt;/p&gt;</description></item><item><title>Basic Reinforcement Learning Techniques to Control the Intensity of a Seeded Free-Electron Laser</title><link>https://RL4aa.github.io/posts/publications/bruchon2020basic/</link><pubDate>Sat, 09 May 2020 00:00:00 +0000</pubDate><guid>https://RL4aa.github.io/posts/publications/bruchon2020basic/</guid><description>&lt;p&gt;&lt;em&gt;&lt;strong&gt;N. Bruchon&lt;sup&gt;1&lt;/sup&gt;, G. Fenu&lt;sup&gt;1&lt;/sup&gt;, G. Gaio&lt;sup&gt;2&lt;/sup&gt;, M. Lonza&lt;sup&gt;2&lt;/sup&gt;, F. H. O’Shea&lt;sup&gt;2&lt;/sup&gt;, F. A. Pellegrino&lt;sup&gt;1&lt;/sup&gt;, E. Salvato&lt;sup&gt;1&lt;/sup&gt;&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;sup&gt;1&lt;/sup&gt;University of Trieste, &lt;sup&gt;2&lt;/sup&gt;Elettra Sincrotrone Trieste&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Electronics&lt;/em&gt;&lt;/p&gt;
&lt;h2 id="abstract"&gt;Abstract&lt;/h2&gt;
&lt;p&gt;Optimal tuning of particle accelerators is a challenging task. Many different approaches have been proposed in the past to solve two main problems—attainment of an optimal working point and performance recovery after machine drifts. The most classical model-free techniques (e.g., Gradient Ascent or Extremum Seeking algorithms) have some intrinsic limitations. To overcome those limitations, Machine Learning tools, in particular Reinforcement Learning (RL), are attracting more and more attention in the particle accelerator community. We investigate the feasibility of RL model-free approaches to align the seed laser, as well as other service lasers, at FERMI, the free-electron laser facility at Elettra Sincrotrone Trieste. We apply two different techniques—the first, based on the episodic Q-learning with linear function approximation, for performance optimization; the second, based on the continuous Natural Policy Gradient REINFORCE algorithm, for performance recovery. Despite the simplicity of these approaches, we report satisfactory preliminary results, that represent the first step toward a new fully automatic procedure for the alignment of the seed laser to the electron beam. Such an alignment is, at present, performed manually.&lt;/p&gt;</description></item><item><title>Toward the Application of Reinforcement Learning to the Intensity Control of a Seeded Free-Electron Laser</title><link>https://RL4aa.github.io/posts/publications/bruchon2019toward/</link><pubDate>Wed, 23 Oct 2019 00:00:00 +0000</pubDate><guid>https://RL4aa.github.io/posts/publications/bruchon2019toward/</guid><description>&lt;p&gt;&lt;em&gt;&lt;strong&gt;N. Bruchon, G. Fenu, G. Gaio, M. Lonza, F. A. Pellegrino, E. Salvato&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;University of Trieste&lt;/p&gt;
&lt;p&gt;&lt;em&gt;23rd International Conference on Mechatronics Technology&lt;/em&gt;&lt;/p&gt;
&lt;h2 id="abstract"&gt;Abstract&lt;/h2&gt;
&lt;p&gt;The optimization of particle accelerators is a challenging task, and many different approaches have been proposed in years, to obtain an optimal tuning of the plant and to keep it optimally tuned despite drifts or disturbances. Indeed, the classical model-free approaches (such as Gradient Ascent or Extremum Seeking algorithms) have intrinsic limitations. To overcome those limitations, Machine Learning techniques, in particular, the Reinforcement Learning, are attracting more and more attention in the particle accelerator community. The purpose of this paper is to apply a Reinforcement Learning model-free approach to the alignment of a seed laser, based on a rather general target function depending on the laser trajectory. The study focuses on the alignment of the lasers at FERMI, the free-electron laser facility at Elettra Sincrotrone Trieste. In particular, we employ Q-learning with linear function approximation and report experimental results obtained in two setups, which are the actual setups where the final application has to be deployed. Despite the simplicity of the approach, we report satisfactory preliminary results, that represent the first step toward a fully automatic procedure for seed laser to the electron beam. Such a superimposition is, at present, performed manually.&lt;/p&gt;</description></item><item><title>Free-electron Laser Optimization with Reinforcement Learning</title><link>https://RL4aa.github.io/posts/publications/bruchon2019freeelectron/</link><pubDate>Sat, 05 Oct 2019 00:00:00 +0000</pubDate><guid>https://RL4aa.github.io/posts/publications/bruchon2019freeelectron/</guid><description>&lt;p&gt;&lt;em&gt;&lt;strong&gt;N. Bruchon&lt;sup&gt;1&lt;/sup&gt;, G. Gaio&lt;sup&gt;2&lt;/sup&gt;, G. Fenu&lt;sup&gt;1&lt;/sup&gt;, M. Lonza&lt;sup&gt;2&lt;/sup&gt;, F. A. Pellegrino&lt;sup&gt;1&lt;/sup&gt;, E. Salvato&lt;sup&gt;1&lt;/sup&gt;&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;sup&gt;1&lt;/sup&gt;University of Trieste, &lt;sup&gt;2&lt;/sup&gt;Elettra Sincrotrone Trieste&lt;/p&gt;
&lt;p&gt;&lt;em&gt;17th International Conference on Accelerator and Large Experimental Physics Control Systems&lt;/em&gt;&lt;/p&gt;
&lt;h2 id="abstract"&gt;Abstract&lt;/h2&gt;
&lt;p&gt;Reinforcement Learning (RL) is one of the most promis-ing techniques in Machine Learning because of its modestcomputational requirements with respect to other algorithms.RL uses an agent that takes actions within its environmentto maximize a reward related to the goal it is designed toachieve. We have recently used RL as a model-free approachto improve the performance of the FERMI Free ElectronLaser. A number of machine parameters are adjusted tofind the optimum FEL output in terms of intensity and spec-tral quality. In particular we focus on the problem of thealignment of the seed laser with the electron beam, initiallyusing a simplified model and then applying the developedalgorithm on the real machine. This paper reports the resultsobtained and discusses pros and cons of this approach withplans for future applications.&lt;/p&gt;</description></item></channel></rss>