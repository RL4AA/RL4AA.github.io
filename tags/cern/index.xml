<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/"><channel><title>Cern on RL4AA Collaboration | Homepage</title><link>https://RL4aa.github.io/tags/cern/</link><description>Recent content in Cern on RL4AA Collaboration | Homepage</description><generator>Hugo -- 0.152.1</generator><language>en-us</language><lastBuildDate>Thu, 27 Apr 2023 00:00:00 +0000</lastBuildDate><atom:link href="https://RL4aa.github.io/tags/cern/index.xml" rel="self" type="application/rss+xml"/><item><title>Towards automatic setup of 18 MeV electron beamline using machine learning</title><link>https://RL4aa.github.io/posts/publications/velotti2023towards/</link><pubDate>Thu, 27 Apr 2023 00:00:00 +0000</pubDate><guid>https://RL4aa.github.io/posts/publications/velotti2023towards/</guid><description>&lt;p&gt;&lt;em&gt;&lt;strong&gt;F. M. Velotti&lt;sup&gt;1&lt;/sup&gt;, B. Goddard&lt;sup&gt;1&lt;/sup&gt;, V. Kain&lt;sup&gt;1&lt;/sup&gt;, R. Ramjiawan&lt;sup&gt;1&lt;/sup&gt;, G. Z. Della Porta&lt;sup&gt;1&lt;/sup&gt; and S. Hirlaender&lt;sup&gt;2&lt;/sup&gt;&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;sup&gt;1&lt;/sup&gt;CERN, &lt;sup&gt;2&lt;/sup&gt;University of Salzburg&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Machine Learning: Science and Technology&lt;/em&gt;&lt;/p&gt;
&lt;h2 id="abstract"&gt;Abstract&lt;/h2&gt;
&lt;p&gt;To improve the performance-critical stability and brightness of the electron bunch at injection into the proton-driven plasma wakefield at the AWAKE CERN experiment, automation approaches based on unsupervised machine learning (ML) were developed and deployed. Numerical optimisers were tested together with different model-free reinforcement learning (RL) agents. In order to avoid any bias, RL agents have been trained also using a completely unsupervised state encoding using auto-encoders. To aid hyper-parameter selection, a full synthetic model of the beamline was constructed using a variational auto-encoder trained to generate surrogate data from equipment settings. This paper describes the novel approaches based on deep learning and RL to aid the automatic setup of a low energy line, as the one used to deliver beam to the AWAKE facility. The results obtained with the different ML approaches, including automatic unsupervised feature extraction from images using computer vision are presented. The prospects for operational deployment and wider applicability are discussed.&lt;/p&gt;</description></item><item><title>Application of reinforcement learning in the LHC tune feedback</title><link>https://RL4aa.github.io/posts/publications/grech2022application/</link><pubDate>Wed, 07 Sep 2022 00:00:00 +0000</pubDate><guid>https://RL4aa.github.io/posts/publications/grech2022application/</guid><description>&lt;p&gt;&lt;em&gt;&lt;strong&gt;L. Grech&lt;sup&gt;1&lt;/sup&gt;, G. Valentino&lt;sup&gt;1&lt;/sup&gt;, D. Alves&lt;sup&gt;2&lt;/sup&gt; and Simon Hirlaender&lt;sup&gt;3&lt;/sup&gt;&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;sup&gt;1&lt;/sup&gt;University of Malta, &lt;sup&gt;2&lt;/sup&gt;CERN, &lt;sup&gt;3&lt;/sup&gt;University of Salzburg&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Frontiers in Physics&lt;/em&gt;&lt;/p&gt;
&lt;h2 id="abstract"&gt;Abstract&lt;/h2&gt;
&lt;p&gt;The Beam-Based Feedback System (BBFS) was primarily responsible for correcting the beam energy, orbit and tune in the CERN Large Hadron Collider (LHC). A major code renovation of the BBFS was planned and carried out during the LHC Long Shutdown 2 (LS2). This work consists of an explorative study to solve a beam-based control problem, the tune feedback (QFB), utilising state-of-the-art Reinforcement Learning (RL). A simulation environment was created to mimic the operation of the QFB. A series of RL agents were trained, and the best-performing agents were then subjected to a set of well-designed tests. The original feedback controller used in the QFB was reimplemented to compare the performance of the classical approach to the performance of selected RL agents in the test scenarios. Results from the simulated environment show that the RL agent performance can exceed the controller-based paradigm.&lt;/p&gt;</description></item><item><title>Automated Intensity Optimisation Using Reinforcement Learning at LEIR</title><link>https://RL4aa.github.io/posts/publications/madysa2022automated/</link><pubDate>Sun, 12 Jun 2022 00:00:00 +0000</pubDate><guid>https://RL4aa.github.io/posts/publications/madysa2022automated/</guid><description>&lt;p&gt;&lt;em&gt;&lt;strong&gt;N. Madysa, V. Kain, R. Alemany Fernandez, N. Biancacci, B. Goddard, F. M. Velotti&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;CERN&lt;/p&gt;
&lt;p&gt;&lt;em&gt;13th Particle Accelerator Conference&lt;/em&gt;&lt;/p&gt;
&lt;h2 id="abstract"&gt;Abstract&lt;/h2&gt;
&lt;p&gt;High intensities in the Low Energy Ion Ring (LEIR) at CERN are achieved by stacking several multi-turn injec- tions from the pre-accelerator Linac3. Up to seven consec- utive 200 μs long, 200 ms spaced pulses are injected from Linac3 into LEIR. Two inclined septa, one magnetic and one electrostatic, combined with a collapsing horizontal or- bit bump allows a 6-D phase space painting via a linearly ramped mean momentum along the Linac3 pulse and in- jection at high dispersion. The already circulating beam is cooled and dragged longitudinally via electron cooling (e- cooling) into a stacking momentum to free space for the fol- lowing injections. For optimal intensity accumulation, the electron energy and trajectory need to match the ion energy and orbit at the e-cooler section.&lt;/p&gt;</description></item><item><title>Test of Machine Learning at the CERN LINAC4</title><link>https://RL4aa.github.io/posts/publications/kain2021test/</link><pubDate>Mon, 04 Oct 2021 00:00:00 +0000</pubDate><guid>https://RL4aa.github.io/posts/publications/kain2021test/</guid><description>&lt;p&gt;&lt;em&gt;&lt;strong&gt;V. Kain&lt;sup&gt;1&lt;/sup&gt;, N. Bruchon&lt;sup&gt;1&lt;/sup&gt;, S. Hirlander&lt;sup&gt;1&lt;/sup&gt;, N. Madysa&lt;sup&gt;1&lt;/sup&gt;, I. Vojskovic&lt;sup&gt;1&lt;/sup&gt;, P. Skowronski&lt;sup&gt;1&lt;/sup&gt;, G. Valentino&lt;sup&gt;2&lt;/sup&gt;&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;sup&gt;1&lt;/sup&gt;CERN, &lt;sup&gt;2&lt;/sup&gt;University of Malta&lt;/p&gt;
&lt;p&gt;&lt;em&gt;61st ICFA ABDW on High-Intensity and High-Brightness Hadron Beams&lt;/em&gt;&lt;/p&gt;
&lt;h2 id="abstract"&gt;Abstract&lt;/h2&gt;
&lt;p&gt;The CERN H−linear accelerator, LINAC4, served as atest bed for advanced algorithms during the CERN LongShutdown 2 in the years 2019/20. One of the main goals wasto show that reinforcement learning with all its benefits canbe used as a replacement for numerical optimization and asa complement to classical control in the accelerator controlcontext. Many of the algorithms used were prepared before-hand at the electron line of the AWAKE facility to makethe best use of the limited time available at LINAC4. Anoverview of the algorithms and concepts tested at LINAC4and AWAKE will be given and the results discussed.&lt;/p&gt;</description></item><item><title>Sample-efficient reinforcement learning for CERN accelerator control</title><link>https://RL4aa.github.io/posts/publications/kain2020sampleefficient/</link><pubDate>Tue, 01 Dec 2020 00:00:00 +0000</pubDate><guid>https://RL4aa.github.io/posts/publications/kain2020sampleefficient/</guid><description>&lt;p&gt;&lt;em&gt;&lt;strong&gt;V. Kain&lt;sup&gt;1&lt;/sup&gt;, S. Hirlander&lt;sup&gt;1&lt;/sup&gt;, B. Goddard&lt;sup&gt;1&lt;/sup&gt;, F. M. Velotti&lt;sup&gt;1&lt;/sup&gt;, G. Z. Della Porta&lt;sup&gt;1&lt;/sup&gt;, N. Bruchon&lt;sup&gt;2&lt;/sup&gt;, G. Valentino&lt;sup&gt;3&lt;/sup&gt;&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;sup&gt;1&lt;/sup&gt;CERN, &lt;sup&gt;2&lt;/sup&gt;University of Trieste, &lt;sup&gt;3&lt;/sup&gt;University of Malta&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Physical Review Accelerators and Beams&lt;/em&gt;&lt;/p&gt;
&lt;h2 id="abstract"&gt;Abstract&lt;/h2&gt;
&lt;p&gt;Numerical optimization algorithms are already established tools to increase and stabilize the performance of particle accelerators. These algorithms have many advantages, are available out of the box, and can be adapted to a wide range of optimization problems in accelerator operation. The next boost in efficiency is expected to come from reinforcement learning algorithms that learn the optimal policy for a certain control problem and hence, once trained, can do without the time-consuming exploration phase needed for numerical optimizers. To investigate this approach, continuous model-free reinforcement learning with up to 16 degrees of freedom was developed and successfully tested at various facilities at CERN. The approach and algorithms used are discussed and the results obtained for trajectory steering at the AWAKE electron line and LINAC4 are presented. The necessary next steps, such as uncertainty aware model-based approaches, and the potential for future applications at particle accelerators are addressed.&lt;/p&gt;</description></item></channel></rss>