<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/"><channel><title>preprint on RL4AA Collaboration | Homepage</title><link>https://RL4aa.github.io/tags/preprint/</link><description>Recent content in preprint on RL4AA Collaboration | Homepage</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Thu, 17 Dec 2020 00:00:00 +0000</lastBuildDate><atom:link href="https://RL4aa.github.io/tags/preprint/index.xml" rel="self" type="application/rss+xml"/><item><title>Model-free and Bayesian Ensembling Model-based Deep Reinforcement Learning for Particle Accelerator Control Demonstrated on the FERMI FEL</title><link>https://RL4aa.github.io/posts/publications/hirlaender2020modelfree/</link><pubDate>Thu, 17 Dec 2020 00:00:00 +0000</pubDate><guid>https://RL4aa.github.io/posts/publications/hirlaender2020modelfree/</guid><description>S. Hirlaender, N. Bruchon
University of Salzburg, University of Trieste
arXiv
Abstract Reinforcement learning holds tremendous promise in accelerator controls. The primary goal of this paper is to show how this approach can be utilised on an operational level on accelerator physics problems. Despite the success of model-free reinforcement learning in several domains, sample-efficiency still is a bottle-neck, which might be encompassed by model-based methods. We compare well-suited purely model-based to model-free reinforcement learning applied to the intensity optimisation on the FERMI FEL system.</description></item></channel></rss>