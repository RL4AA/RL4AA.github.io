<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/"><channel><title>LNAL on RL4AA Collaboration | Homepage</title><link>https://RL4aa.github.io/tags/lnal/</link><description>Recent content in LNAL on RL4AA Collaboration | Homepage</description><generator>Hugo -- 0.152.1</generator><language>en-us</language><lastBuildDate>Sat, 12 Dec 2020 00:00:00 +0000</lastBuildDate><atom:link href="https://RL4aa.github.io/tags/lnal/index.xml" rel="self" type="application/rss+xml"/><item><title>Autonomous Control of a Particle Accelerator using Deep Reinforcement Learning</title><link>https://RL4aa.github.io/posts/publications/pang2020autonomous/</link><pubDate>Sat, 12 Dec 2020 00:00:00 +0000</pubDate><guid>https://RL4aa.github.io/posts/publications/pang2020autonomous/</guid><description>&lt;p&gt;&lt;em&gt;&lt;strong&gt;X. Pang&lt;sup&gt;1&lt;/sup&gt;, S. Thulasidasan&lt;sup&gt;2&lt;/sup&gt;, L. Rybarcyk&lt;sup&gt;2&lt;/sup&gt;&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;sup&gt;1&lt;/sup&gt;Apple, &lt;sup&gt;2&lt;/sup&gt;Los Alamos National Laboratory&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Machine Learning for Engineering Modeling, Simulation, and Design Workshop at Neural Information Processing Systems 2020&lt;/em&gt;&lt;/p&gt;
&lt;h2 id="abstract"&gt;Abstract&lt;/h2&gt;
&lt;p&gt;We describe an approach to learning optimal control policies for a large, linear particle accelerator using deep reinforcement learning coupled with a high-fidelity physics engine. The framework consists of an AI controller that uses deep neural networks for state and action-space representation and learns optimal policies using reward signals that are provided by the physics simulator. For this work, we only focus on controlling a small section of the entire accelerator. Nevertheless, initial results indicate that we can achieve better-than-human level performance in terms of particle beam current and distribution. The ultimate goal of this line of work is to substantially reduce the tuning time for such facilities by orders of magnitude, and achieve near-autonomous control.&lt;/p&gt;</description></item></channel></rss>