<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/"><channel><title>Neurips Workshop on RL4AA Collaboration | Homepage</title><link>https://RL4aa.github.io/tags/neurips-workshop/</link><description>Recent content in Neurips Workshop on RL4AA Collaboration | Homepage</description><generator>Hugo -- 0.152.1</generator><language>en-us</language><lastBuildDate>Sat, 12 Dec 2020 00:00:00 +0000</lastBuildDate><atom:link href="https://RL4aa.github.io/tags/neurips-workshop/index.xml" rel="self" type="application/rss+xml"/><item><title>Autonomous Control of a Particle Accelerator using Deep Reinforcement Learning</title><link>https://RL4aa.github.io/posts/publications/pang2020autonomous/</link><pubDate>Sat, 12 Dec 2020 00:00:00 +0000</pubDate><guid>https://RL4aa.github.io/posts/publications/pang2020autonomous/</guid><description>&lt;p&gt;&lt;em&gt;&lt;strong&gt;X. Pang&lt;sup&gt;1&lt;/sup&gt;, S. Thulasidasan&lt;sup&gt;2&lt;/sup&gt;, L. Rybarcyk&lt;sup&gt;2&lt;/sup&gt;&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;sup&gt;1&lt;/sup&gt;Apple, &lt;sup&gt;2&lt;/sup&gt;Los Alamos National Laboratory&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Machine Learning for Engineering Modeling, Simulation, and Design Workshop at Neural Information Processing Systems 2020&lt;/em&gt;&lt;/p&gt;
&lt;h2 id="abstract"&gt;Abstract&lt;/h2&gt;
&lt;p&gt;We describe an approach to learning optimal control policies for a large, linear particle accelerator using deep reinforcement learning coupled with a high-fidelity physics engine. The framework consists of an AI controller that uses deep neural networks for state and action-space representation and learns optimal policies using reward signals that are provided by the physics simulator. For this work, we only focus on controlling a small section of the entire accelerator. Nevertheless, initial results indicate that we can achieve better-than-human level performance in terms of particle beam current and distribution. The ultimate goal of this line of work is to substantially reduce the tuning time for such facilities by orders of magnitude, and achieve near-autonomous control.&lt;/p&gt;</description></item><item><title>Using Neural Network Control Policies For Rapid Switching Between Beam Parameters in a Free Electron Laser</title><link>https://RL4aa.github.io/posts/publications/edelen2017using2/</link><pubDate>Fri, 25 Aug 2017 00:00:00 +0000</pubDate><guid>https://RL4aa.github.io/posts/publications/edelen2017using2/</guid><description>&lt;p&gt;&lt;em&gt;&lt;strong&gt;A. L. Edelen&lt;sup&gt;1&lt;/sup&gt;, S. G. Biedron&lt;sup&gt;2&lt;/sup&gt;, J. P. Edelen&lt;sup&gt;3&lt;/sup&gt;, S. V. Milton&lt;sup&gt;4&lt;/sup&gt;, P. J. M. van der Slot&lt;sup&gt;5&lt;/sup&gt;&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;sup&gt;1&lt;/sup&gt;Colorado State University, &lt;sup&gt;2&lt;/sup&gt;Element Aero, &lt;sup&gt;3&lt;/sup&gt;Fermi National Accelerator Laboratory, &lt;sup&gt;4&lt;/sup&gt;Los Alamos National Laboratory, &lt;sup&gt;5&lt;/sup&gt;University of Twente&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Workshop on Deep Learning for Physical Sciences at the Conference on Neural Information Processing Systems 2017&lt;/em&gt;&lt;/p&gt;
&lt;h2 id="abstract"&gt;Abstract&lt;/h2&gt;
&lt;p&gt;Free Electron Laser (FEL) facilities often must accommodate requests for a varietyof electron beam parameters in order to supply scientific users with appropriatephoton beam characteristics. This usually requires skilled human operators to tunethe machine. In principle, a neural network control policy that is trained on a broadrange of machine operating states could be used to quickly switch between theserequests without substantial need for human intervention. We present preliminaryresults from an ongoing simulation study in which a neural network control policyis investigated for rapid switching between beam parameters in a compact THzFEL that exhibits nonlinear electron beam dynamics. To accomplish this, we firsttrain a feed-forward neural network to mimic a physics-based simulation of theFEL. We then train a neural network control policy by first pre-training it as aninverse model (using supervised learning with a subset of the simulation data) andthen training it more extensively with reinforcement learning. In this case, thereinforcement learning component consists of letting the policy network interactwith the learned system model and backpropagating the cost through the modelnetwork to the controller network.&lt;/p&gt;</description></item></channel></rss>