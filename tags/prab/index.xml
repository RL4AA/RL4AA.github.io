<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/"><channel><title>Prab on RL4AA Collaboration | Homepage</title><link>https://RL4aa.github.io/tags/prab/</link><description>Recent content in Prab on RL4AA Collaboration | Homepage</description><generator>Hugo -- 0.152.1</generator><language>en-us</language><lastBuildDate>Thu, 13 Apr 2023 00:00:00 +0000</lastBuildDate><atom:link href="https://RL4aa.github.io/tags/prab/index.xml" rel="self" type="application/rss+xml"/><item><title>Orbit Correction Based on Improved Reinforcement Learning Algorithm</title><link>https://RL4aa.github.io/posts/publications/chen2023orbit/</link><pubDate>Thu, 13 Apr 2023 00:00:00 +0000</pubDate><guid>https://RL4aa.github.io/posts/publications/chen2023orbit/</guid><description>&lt;p&gt;&lt;em&gt;&lt;strong&gt;X. Chen, Y. Jia, X. Qi, Z. Wang, Y. He&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Chinese Academy of Sciences&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Physical Review Accelerators and Beams&lt;/em&gt;&lt;/p&gt;
&lt;h2 id="abstract"&gt;Abstract&lt;/h2&gt;
&lt;p&gt;Recently, reinforcement learning (RL) algorithms have been applied to a wide range of control problems in accelerator commissioning. In order to achieve efficient and fast control, these algorithms need to be highly efficient, so as to minimize the online training time. In this paper, we incorporated the beam position monitor trend into the observation space of the twin delayed deep deterministic policy gradient (TD3) algorithm and trained two different structure agents, one based on physical prior knowledge and the other using the original TD3 network architecture. Both of the agents exhibit strong robustness in the simulated environment. The effectiveness of the agent based on physical prior knowledge has been validated in a real accelerator. Results show that the agent can overcome the difference between simulated and real accelerator environments. Once the training is completed in the simulated environment, the agent can be directly applied to the real accelerator without any online training process. The RL agent is deployed to the medium energy beam transport section of China Accelerator Facility for Superheavy Elements. Fast and automatic orbit correction is being tested with up to ten degrees of freedom. The experimental results show that the agents can correct the orbit to within 1 mm. Moreover, due to the strong robustness of the agent, when a trained agent is applied to different lattices of different particles, the orbit correction can still be completed. Since there are no online data collection and training processes, all online corrections are done within 30 s. This paper shows that, as long as the robustness of the RL algorithm is sufficient, the offline learning agents can be directly applied to online correction, which will greatly improve the efficiency of orbit correction. Such an approach to RL may find promising applications in other areas of accelerator commissioning.&lt;/p&gt;</description></item><item><title>Optimizing a superconducting radio-frequency gun using deep reinforcement learning</title><link>https://RL4aa.github.io/posts/publications/meier2022optimizing/</link><pubDate>Fri, 28 Oct 2022 00:00:00 +0000</pubDate><guid>https://RL4aa.github.io/posts/publications/meier2022optimizing/</guid><description>&lt;p&gt;&lt;em&gt;&lt;strong&gt;D. Meier&lt;sup&gt;1&lt;/sup&gt;, L. V. Ramirez&lt;sup&gt;1&lt;/sup&gt;, J. VÃ¶lker&lt;sup&gt;1&lt;/sup&gt;, J. Viefhaus&lt;sup&gt;1&lt;/sup&gt;, B. Sick&lt;sup&gt;2&lt;/sup&gt;, G. Hartmann&lt;sup&gt;1&lt;/sup&gt;&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;sup&gt;1&lt;/sup&gt;Helmholtz-Zentrum Berlin, &lt;sup&gt;2&lt;/sup&gt;University of Kassel&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Physical Review Accelerators and Beams&lt;/em&gt;&lt;/p&gt;
&lt;h2 id="abstract"&gt;Abstract&lt;/h2&gt;
&lt;p&gt;Superconducting photoelectron injectors are promising for generating highly brilliant pulsed electron beams with high repetition rates and low emittances. Experiments such as ultrafast electron diffraction, experiments at the Terahertz scale, and energy recovery linac applications require such properties. However, optimizing the beam properties is challenging due to the high number of possible machine parameter combinations. This article shows the successful automated optimization of beam properties utilizing an already existing simulation model. To reduce the required computation time, we replace the costly simulation with a faster approximation with a neural network. For optimization, we propose a reinforcement learning approach leveraging the simple computation of the derivative of the approximation. We prove that our approach outperforms standard optimization methods for the required function evaluations given a defined minimum accuracy.&lt;/p&gt;</description></item><item><title>Real-time artificial intelligence for accelerator control: A study at the Fermilab Booster</title><link>https://RL4aa.github.io/posts/publications/stjohn2021realtime/</link><pubDate>Mon, 18 Oct 2021 00:00:00 +0000</pubDate><guid>https://RL4aa.github.io/posts/publications/stjohn2021realtime/</guid><description>&lt;p&gt;&lt;em&gt;&lt;strong&gt;J. St. John&lt;sup&gt;1&lt;/sup&gt;, C. Herwig&lt;sup&gt;1&lt;/sup&gt;, D. Kafkes&lt;sup&gt;1&lt;/sup&gt;, J. Mitrevski&lt;sup&gt;1&lt;/sup&gt;, W. A. Pellico&lt;sup&gt;1&lt;/sup&gt;, G. N. Perdue&lt;sup&gt;1&lt;/sup&gt;, A. Quintero-Parra&lt;sup&gt;1&lt;/sup&gt;, B. A. Schupbach&lt;sup&gt;1&lt;/sup&gt;, K. Seiya&lt;sup&gt;1&lt;/sup&gt;, N. Tran&lt;sup&gt;1&lt;/sup&gt;, M. Schram&lt;sup&gt;2&lt;/sup&gt;, J. M. Duarte&lt;sup&gt;3&lt;/sup&gt;, Y. Huang&lt;sup&gt;4&lt;/sup&gt;, R. Keller&lt;sup&gt;5&lt;/sup&gt;&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;sup&gt;1&lt;/sup&gt;Fermi National Accelerator Laboratory, &lt;sup&gt;2&lt;/sup&gt;Thomas Jefferson National Accelerator Laboratory, &lt;sup&gt;3&lt;/sup&gt;University of California San Diego, &lt;sup&gt;4&lt;/sup&gt;Pacific Northwest National Laboratory, &lt;sup&gt;5&lt;/sup&gt;Columbia University&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Physical Review Accelerators and Beams&lt;/em&gt;&lt;/p&gt;
&lt;h2 id="abstract"&gt;Abstract&lt;/h2&gt;
&lt;p&gt;We describe a method for precisely regulating the gradient magnet power supply (GMPS) at the Fermilab Booster accelerator complex using a neural network trained via reinforcement learning. We demonstrate preliminary results by training a surrogate machine-learning model on real accelerator data to emulate the GMPS, and using this surrogate model in turn to train the neural network for its regulation task. We additionally show how the neural networks to be deployed for control purposes may be compiled to execute on field-programmable gate arrays (FPGAs), and show the first machine-learning based control algorithm implemented on an FPGA for controls at the Fermilab accelerator complex. As there are no surprise latencies on an FPGA, this capability is important for operational stability in complicated environments such as an accelerator facility.&lt;/p&gt;</description></item><item><title>Policy gradient methods for free-electron laser and terahertz source optimization and stabilization at the FERMI free-electron laser at Elettra</title><link>https://RL4aa.github.io/posts/publications/oshea202policy/</link><pubDate>Mon, 21 Dec 2020 00:00:00 +0000</pubDate><guid>https://RL4aa.github.io/posts/publications/oshea202policy/</guid><description>&lt;p&gt;&lt;em&gt;&lt;strong&gt;F. H. O&amp;rsquo;Shea&lt;sup&gt;1&lt;/sup&gt;, N. Bruchon&lt;sup&gt;2&lt;/sup&gt;, G. Gaio&lt;sup&gt;1&lt;/sup&gt;&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;sup&gt;1&lt;/sup&gt;Elettra Sincrotrone Trieste, &lt;sup&gt;2&lt;/sup&gt;University of Trieste&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Physical Review Accelerators and Beams&lt;/em&gt;&lt;/p&gt;
&lt;h2 id="abstract"&gt;Abstract&lt;/h2&gt;
&lt;p&gt;In this article we report on the application of a model-free reinforcement learning method to the optimization of accelerator systems. We simplify a policy gradient algorithm to accelerator control from sophisticated algorithms that have recently been demonstrated to solve complex dynamic problems. After outlining a theoretical basis for the functioning of the algorithm, we explore the small hyperparameter space to develop intuition about said parameters using a simple number-guess environment. Finally, we demonstrate the algorithm optimizing both a free-electron laser and an accelerator-based terahertz source in-situ. The algorithm is applied to different accelerator control systems and optimizes the desired signals in a few hundred steps without any domain knowledge using up to five control parameters. In addition, the algorithm shows modest tolerance to accelerator fault conditions without any special preparation for such conditions.&lt;/p&gt;</description></item><item><title>Sample-efficient reinforcement learning for CERN accelerator control</title><link>https://RL4aa.github.io/posts/publications/kain2020sampleefficient/</link><pubDate>Tue, 01 Dec 2020 00:00:00 +0000</pubDate><guid>https://RL4aa.github.io/posts/publications/kain2020sampleefficient/</guid><description>&lt;p&gt;&lt;em&gt;&lt;strong&gt;V. Kain&lt;sup&gt;1&lt;/sup&gt;, S. Hirlander&lt;sup&gt;1&lt;/sup&gt;, B. Goddard&lt;sup&gt;1&lt;/sup&gt;, F. M. Velotti&lt;sup&gt;1&lt;/sup&gt;, G. Z. Della Porta&lt;sup&gt;1&lt;/sup&gt;, N. Bruchon&lt;sup&gt;2&lt;/sup&gt;, G. Valentino&lt;sup&gt;3&lt;/sup&gt;&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;sup&gt;1&lt;/sup&gt;CERN, &lt;sup&gt;2&lt;/sup&gt;University of Trieste, &lt;sup&gt;3&lt;/sup&gt;University of Malta&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Physical Review Accelerators and Beams&lt;/em&gt;&lt;/p&gt;
&lt;h2 id="abstract"&gt;Abstract&lt;/h2&gt;
&lt;p&gt;Numerical optimization algorithms are already established tools to increase and stabilize the performance of particle accelerators. These algorithms have many advantages, are available out of the box, and can be adapted to a wide range of optimization problems in accelerator operation. The next boost in efficiency is expected to come from reinforcement learning algorithms that learn the optimal policy for a certain control problem and hence, once trained, can do without the time-consuming exploration phase needed for numerical optimizers. To investigate this approach, continuous model-free reinforcement learning with up to 16 degrees of freedom was developed and successfully tested at various facilities at CERN. The approach and algorithms used are discussed and the results obtained for trajectory steering at the AWAKE electron line and LINAC4 are presented. The necessary next steps, such as uncertainty aware model-based approaches, and the potential for future applications at particle accelerators are addressed.&lt;/p&gt;</description></item></channel></rss>