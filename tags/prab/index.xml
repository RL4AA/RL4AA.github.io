<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/"><channel><title>prab on &lt;img src="imgs/rl4aa_logo.png"/> RL4AA Collaboration | Homepage</title><link>https://RL4aa.github.io/tags/prab/</link><description>Recent content in prab on &lt;img src="imgs/rl4aa_logo.png"/> RL4AA Collaboration | Homepage</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Thu, 13 Apr 2023 00:00:00 +0000</lastBuildDate><atom:link href="https://RL4aa.github.io/tags/prab/index.xml" rel="self" type="application/rss+xml"/><item><title>Orbit Correction Based on Improved Reinforcement Learning Algorithm</title><link>https://RL4aa.github.io/posts/publications/chen2023orbit/</link><pubDate>Thu, 13 Apr 2023 00:00:00 +0000</pubDate><guid>https://RL4aa.github.io/posts/publications/chen2023orbit/</guid><description>X. Chen, Y. Jia, X. Qi, Z. Wang, Y. He
Chinese Academy of Sciences
Physical Review Accelerators and Beams
Abstract Recently, reinforcement learning (RL) algorithms have been applied to a wide range of control problems in accelerator commissioning. In order to achieve efficient and fast control, these algorithms need to be highly efficient, so as to minimize the online training time. In this paper, we incorporated the beam position monitor trend into the observation space of the twin delayed deep deterministic policy gradient (TD3) algorithm and trained two different structure agents, one based on physical prior knowledge and the other using the original TD3 network architecture.</description></item><item><title>Optimizing a superconducting radio-frequency gun using deep reinforcement learning</title><link>https://RL4aa.github.io/posts/publications/meier2022optimizing/</link><pubDate>Fri, 28 Oct 2022 00:00:00 +0000</pubDate><guid>https://RL4aa.github.io/posts/publications/meier2022optimizing/</guid><description>D. Meier1, L. V. Ramirez1, J. VÃ¶lker1, J. Viefhaus1, B. Sick2, G. Hartmann1
1Helmholtz-Zentrum Berlin, 2University of Kassel
Physical Review Accelerators and Beams
Abstract Superconducting photoelectron injectors are promising for generating highly brilliant pulsed electron beams with high repetition rates and low emittances. Experiments such as ultrafast electron diffraction, experiments at the Terahertz scale, and energy recovery linac applications require such properties. However, optimizing the beam properties is challenging due to the high number of possible machine parameter combinations.</description></item><item><title>Real-time artificial intelligence for accelerator control: A study at the Fermilab Booster</title><link>https://RL4aa.github.io/posts/publications/stjohn2021realtime/</link><pubDate>Mon, 18 Oct 2021 00:00:00 +0000</pubDate><guid>https://RL4aa.github.io/posts/publications/stjohn2021realtime/</guid><description>J. St. John1, C. Herwig1, D. Kafkes1, J. Mitrevski1, W. A. Pellico1, G. N. Perdue1, A. Quintero-Parra1, B. A. Schupbach1, K. Seiya1, N. Tran1, M. Schram2, J. M. Duarte3, Y. Huang4, R. Keller5
1Fermi National Accelerator Laboratory, 2Thomas Jefferson National Accelerator Laboratory, 3University of California San Diego, 4Pacific Northwest National Laboratory, 5Columbia University
Physical Review Accelerators and Beams
Abstract We describe a method for precisely regulating the gradient magnet power supply (GMPS) at the Fermilab Booster accelerator complex using a neural network trained via reinforcement learning.</description></item><item><title>Policy gradient methods for free-electron laser and terahertz source optimization and stabilization at the FERMI free-electron laser at Elettra</title><link>https://RL4aa.github.io/posts/publications/oshea202policy/</link><pubDate>Mon, 21 Dec 2020 00:00:00 +0000</pubDate><guid>https://RL4aa.github.io/posts/publications/oshea202policy/</guid><description>F. H. O&amp;rsquo;Shea1, N. Bruchon2, G. Gaio1
1Elettra Sincrotrone Trieste, 2University of Trieste
Physical Review Accelerators and Beams
Abstract In this article we report on the application of a model-free reinforcement learning method to the optimization of accelerator systems. We simplify a policy gradient algorithm to accelerator control from sophisticated algorithms that have recently been demonstrated to solve complex dynamic problems. After outlining a theoretical basis for the functioning of the algorithm, we explore the small hyperparameter space to develop intuition about said parameters using a simple number-guess environment.</description></item><item><title>Sample-efficient reinforcement learning for CERN accelerator control</title><link>https://RL4aa.github.io/posts/publications/kain2020sampleefficient/</link><pubDate>Tue, 01 Dec 2020 00:00:00 +0000</pubDate><guid>https://RL4aa.github.io/posts/publications/kain2020sampleefficient/</guid><description>V. Kain1, S. Hirlander1, B. Goddard1, F. M. Velotti1, G. Z. Della Porta1, N. Bruchon2, G. Valentino3
1CERN, 2University of Trieste, 3University of Malta
Physical Review Accelerators and Beams
Abstract Numerical optimization algorithms are already established tools to increase and stabilize the performance of particle accelerators. These algorithms have many advantages, are available out of the box, and can be adapted to a wide range of optimization problems in accelerator operation.</description></item></channel></rss>