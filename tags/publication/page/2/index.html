<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Publication | RL4AA Collaboration | Homepage</title><meta name=keywords content><meta name=description content="Reinforcement Learning for Autonomous Accelerator Collaboration - https://github.com/RL4AA"><meta name=author content="RL4AA Collaboration"><link rel=canonical href=https://RL4aa.github.io/tags/publication/><link crossorigin=anonymous href=/assets/css/stylesheet.a090830a421002426baafbd314e38f149d77b4c48a12ee9312700d770b27fb26.css integrity="sha256-oJCDCkIQAkJrqvvTFOOPFJ13tMSKEu6TEnANdwsn+yY=" rel="preload stylesheet" as=style><link rel=icon href=https://RL4aa.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://RL4aa.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://RL4aa.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://RL4aa.github.io/apple-touch-icon.png><link rel=mask-icon href=https://RL4aa.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate type=application/rss+xml href=https://RL4aa.github.io/tags/publication/index.xml title=rss><link rel=alternate hreflang=en href=https://RL4aa.github.io/tags/publication/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><meta property="og:url" content="https://RL4aa.github.io/tags/publication/"><meta property="og:site_name" content="RL4AA Collaboration | Homepage"><meta property="og:title" content="Publication"><meta property="og:description" content="Reinforcement Learning for Autonomous Accelerator Collaboration - https://github.com/RL4AA"><meta property="og:locale" content="en-us"><meta property="og:type" content="website"><meta name=twitter:card content="summary"><meta name=twitter:title content="Publication"><meta name=twitter:description content="Reinforcement Learning for Autonomous Accelerator Collaboration - https://github.com/RL4AA"></head><body class=list id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://RL4aa.github.io/ accesskey=h title="RL4AA Collaboration | Homepage (Alt + H)"><img src=https://RL4aa.github.io/imgs/rl4aa_logo.png alt aria-label=logo height=30>RL4AA Collaboration | Homepage</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://RL4aa.github.io/categories/announcements title=Announcements><span>Announcements</span></a></li><li><a href=https://RL4aa.github.io/categories/publications title=Publications><span>Publications</span></a></li><li><a href=https://RL4aa.github.io/tags/ title=Tags><span>Tags</span></a></li><li><a href=https://RL4aa.github.io/archives/ title=Archive><span>Archive</span></a></li><li><a href=https://RL4aa.github.io/contact/ title=Contact><span>Contact</span></a></li></ul></nav></header><main class=main><header class=page-header><div class=breadcrumbs><a href=https://RL4aa.github.io/>Home</a>&nbsp;»&nbsp;<a href=https://RL4aa.github.io/tags/>Tags</a></div><h1>Publication</h1></header><article class="post-entry tag-entry"><figure class=entry-cover><img loading=lazy src=https://RL4aa.github.io/imgs/stjohn2021realtime.png alt="Schematic view of the GMPS control environment."></figure><header class=entry-header><h2 class=entry-hint-parent>Real-time artificial intelligence for accelerator control: A study at the Fermilab Booster</h2></header><div class=entry-content><p>J. St. John1, C. Herwig1, D. Kafkes1, J. Mitrevski1, W. A. Pellico1, G. N. Perdue1, A. Quintero-Parra1, B. A. Schupbach1, K. Seiya1, N. Tran1, M. Schram2, J. M. Duarte3, Y. Huang4, R. Keller5
1Fermi National Accelerator Laboratory, 2Thomas Jefferson National Accelerator Laboratory, 3University of California San Diego, 4Pacific Northwest National Laboratory, 5Columbia University
Physical Review Accelerators and Beams
Abstract We describe a method for precisely regulating the gradient magnet power supply (GMPS) at the Fermilab Booster accelerator complex using a neural network trained via reinforcement learning. We demonstrate preliminary results by training a surrogate machine-learning model on real accelerator data to emulate the GMPS, and using this surrogate model in turn to train the neural network for its regulation task. We additionally show how the neural networks to be deployed for control purposes may be compiled to execute on field-programmable gate arrays (FPGAs), and show the first machine-learning based control algorithm implemented on an FPGA for controls at the Fermilab accelerator complex. As there are no surprise latencies on an FPGA, this capability is important for operational stability in complicated environments such as an accelerator facility.
...</p></div><footer class=entry-footer><span title='2021-10-18 00:00:00 +0000 UTC'>October 18, 2021</span>&nbsp;·&nbsp;<span>194 words</span>&nbsp;·&nbsp;<span>RL4AA Collaboration</span></footer><a class=entry-link aria-label="post link to Real-time artificial intelligence for accelerator control: A study at the Fermilab Booster" href=https://RL4aa.github.io/posts/publications/stjohn2021realtime/></a></article><article class="post-entry tag-entry"><figure class=entry-cover><img loading=lazy src=https://RL4aa.github.io/imgs/kain2021test.png alt="Online training of NAF Agent of AWAKE electronline trajectory steering in the horizontal plane."></figure><header class=entry-header><h2 class=entry-hint-parent>Test of Machine Learning at the CERN LINAC4</h2></header><div class=entry-content><p>V. Kain1, N. Bruchon1, S. Hirlander1, N. Madysa1, I. Vojskovic1, P. Skowronski1, G. Valentino2
1CERN, 2University of Malta
61st ICFA ABDW on High-Intensity and High-Brightness Hadron Beams
Abstract The CERN H−linear accelerator, LINAC4, served as atest bed for advanced algorithms during the CERN LongShutdown 2 in the years 2019/20. One of the main goals wasto show that reinforcement learning with all its benefits canbe used as a replacement for numerical optimization and asa complement to classical control in the accelerator controlcontext. Many of the algorithms used were prepared before-hand at the electron line of the AWAKE facility to makethe best use of the limited time available at LINAC4. Anoverview of the algorithms and concepts tested at LINAC4and AWAKE will be given and the results discussed.
...</p></div><footer class=entry-footer><span title='2021-10-04 00:00:00 +0000 UTC'>October 4, 2021</span>&nbsp;·&nbsp;<span>132 words</span>&nbsp;·&nbsp;<span>RL4AA Collaboration</span></footer><a class=entry-link aria-label="post link to Test of Machine Learning at the CERN LINAC4" href=https://RL4aa.github.io/posts/publications/kain2021test/></a></article><article class="post-entry tag-entry"><figure class=entry-cover><img loading=lazy src=https://RL4aa.github.io/imgs/grech2021renovation.png alt="Best PPO agent. Action is deterministic."></figure><header class=entry-header><h2 class=entry-hint-parent>Renovation of the beam-based feedback systems in the LHC</h2></header><div class=entry-content><p>L. Grech
University of Malta
PhD thesis
Abstract The Large Hadron Collider (LHC) at the European Organization for Nuclear Research (CERN) is the largest synchrotron built to date, having a circumference of approx- imately 27km. The LHC is able to accelerate two counter-rotating proton and/or heavy-ion beams up to 7 TeV per charge. These highly energetic beams are contained inside a vacuum chamber with an inner diameter of 80 mm by means of strong mag- netic fields produced by superconducting magnets. A beam cleaning and machine protection system is in place to prevent high-energy halo particles from impacting and heating the superconducting magnets.
...</p></div><footer class=entry-footer><span title='2021-09-01 00:00:00 +0000 UTC'>September 1, 2021</span>&nbsp;·&nbsp;<span>655 words</span>&nbsp;·&nbsp;<span>RL4AA Collaboration</span></footer><a class=entry-link aria-label="post link to Renovation of the beam-based feedback systems in the LHC" href=https://RL4aa.github.io/posts/publications/grech2021renovation/></a></article><article class="post-entry tag-entry"><figure class=entry-cover><img loading=lazy src=https://RL4aa.github.io/imgs/wang2021accelerated.png alt="Hardware solution  for RL control."></figure><header class=entry-header><h2 class=entry-hint-parent>Accelerated Deep Reinforcement Learning for Fast Feedback of Beam Dynamics at KARA</h2></header><div class=entry-content><p>W. Wang1, M. Caselle1, T. Boltz1, E. Blomley1, M. Brosi1, T. Dritschler1, A. Ebersoldt1, A. Kopmann1, A. Santamaria Garcia1, P. Schreiber1, E. Bründermann1, M. Weber1, A.-S. Müller1, Y. Fang2
1Karlsruhe Insitute of Technology KIT, 2Northwestern Polytechnical University
IEEE Transactions on Nuclear Science
Abstract Coherent synchrotron radiation (CSR) is generated when the electron bunch length is in the order of the magnitude of the wavelength of the emitted radiation. The self-interaction of short electron bunches with their own electromagnetic fields changes the longitudinal beam dynamics significantly. Above a certain current threshold, the micro-bunching instability develops, characterized by the appearance of distinguishable substructures in the longitudinal phase space of the bunch. To stabilize the CSR emission, a real-time feedback control loop based on reinforcement learning (RL) is proposed. Informed by the available THz diagnostics, the feedback is designed to act on the radio frequency (RF) system of the storage ring to mitigate the micro-bunching dynamics. To satisfy low-latency requirements given by the longitudinal beam dynamics, the RL controller has been implemented on hardware (FPGA). In this article, a real-time feedback loop architecture and its performance is presented and compared with a software implementation using Keras-RL on CPU/GPU. The results obtained with the CSR simulation Inovesa demonstrate that the functionality of both platforms is equivalent. The training performance of the hardware implementation is similar to software solution, while it outperforms the Keras-RL implementation by an order of magnitude. The presented RL hardware controller is considered as an essential platform for the development of intelligent CSR control systems.
...</p></div><footer class=entry-footer><span title='2021-05-27 00:00:00 +0000 UTC'>May 27, 2021</span>&nbsp;·&nbsp;<span>260 words</span>&nbsp;·&nbsp;<span>RL4AA Collaboration</span></footer><a class=entry-link aria-label="post link to Accelerated Deep Reinforcement Learning for Fast Feedback of Beam Dynamics at KARA" href=https://RL4aa.github.io/posts/publications/wang2021accelerated/></a></article><article class="post-entry tag-entry"><figure class=entry-cover><img loading=lazy src=https://RL4aa.github.io/imgs/eichler2021first.png alt="RL environment for beam optimisation in theARES EA."></figure><header class=entry-header><h2 class=entry-hint-parent>First Steps Toward an Autonomous Accelerator, A Common Project Between DESY and KIT</h2></header><div class=entry-content><p>A. Eichler1, F. Burkart1, J. Kaiser1, W. Kuropka1, O. Stein1, E. Bründermann2, A. Santamaria Garcia2, C. Xu2
1Deutsches Elektronen-Synchrotron DESY, 2Karlsruhe Institute of Technology KIT
12th International Particle Accelerator Conference
Abstract Reinforcement learning algorithms have risen in pop-ularity in the accelerator physics community in recentyears, showing potential in beam control and in the opti-mization and automation of tasks in accelerator operation.The Helmholtz AI project “Machine Learning Toward Au-tonomous Accelerators” is a collaboration between DESYand KIT that works on investigating and developing rein-forcement learning applications for the automatic start-upof electron linear accelerators. The work is carried out inparallel at two similar research accelerators: ARES at DESYand FLUTE at KIT, giving the unique opportunity of trans-fer learning between facilities. One of the first steps of thisproject is the establishment of a common interface betweenthe simulations and the machine, in order to test and applyvarious optimization approaches interchangeably betweenthe two accelerators. In this paper we present first results onthe common interface and its application to beam focusingin ARES as well as the idea of laser shaping with spatiallight modulators at FLUTE.
...</p></div><footer class=entry-footer><span title='2021-05-24 00:00:00 +0000 UTC'>May 24, 2021</span>&nbsp;·&nbsp;<span>185 words</span>&nbsp;·&nbsp;<span>RL4AA Collaboration</span></footer><a class=entry-link aria-label="post link to First Steps Toward an Autonomous Accelerator, A Common Project Between DESY and KIT" href=https://RL4aa.github.io/posts/publications/eichler2021first/></a></article><article class="post-entry tag-entry"><figure class=entry-cover><img loading=lazy src=https://RL4aa.github.io/imgs/ivanov2021physicsenhanced.png alt="Reinforcement learning agent joint with the physics-based polynomial neural network."></figure><header class=entry-header><h2 class=entry-hint-parent>Physics-Enhanced Reinforcement Learning for Optimal Control</h2></header><div class=entry-content><p>A. Ivanov, I. Agapov, A. Eichler, S. Tomin
Deutsches Elektronen Synchrotron DESY
12th International Particle Accelerator Conference
Abstract We propose an approach for incorporating acceleratorphysics models into reinforcement learning agents. The proposed approach is based on the Taylor mapping technique for the simulation of particle dynamics. The resulting computational graph is represented as a polynomial neural network and embedded into the traditional reinforcement learning agents. The application of the model is demonstrated in a nonlinear simulation model of beam transmission. The comparison of the approach with the traditional numerical optimization as well as neural networks-based agents demonstrates better convergence of the proposed technique.
...</p></div><footer class=entry-footer><span title='2021-05-21 00:00:00 +0000 UTC'>May 21, 2021</span>&nbsp;·&nbsp;<span>110 words</span>&nbsp;·&nbsp;<span>RL4AA Collaboration</span></footer><a class=entry-link aria-label="post link to Physics-Enhanced Reinforcement Learning for Optimal Control" href=https://RL4aa.github.io/posts/publications/ivanov2021physicsenhanced/></a></article><article class="post-entry tag-entry"><figure class=entry-cover><img loading=lazy src=https://RL4aa.github.io/imgs/bruchon2021feasibility.png alt="Simple scheme of the FERMI FEL seed laser alignment set up."></figure><header class=entry-header><h2 class=entry-hint-parent>Feasibility Investigation on Several Reinforcement Learning Techniques to Improve the Performance of the FERMI Free-Electron Laser</h2></header><div class=entry-content><p>N. Bruchon
University of Trieste
PhD thesis
Abstract The research carried out in particle accelerator facilities does not concern only particle and condensed matter physics, although these are the main topics covered in the field. Indeed, since a particle accelerator is composed of many different sub-systems, its proper functioning depends both on each of these parts and their interconnection. It follows that the study, implementation, and improvement of the various sub-systems are fundamental points of investigation too. In particular, an interesting aspect for the automation engineering community is the control of such systems that usually are complex, large, noise-affected, and non-linear.
...</p></div><footer class=entry-footer><span title='2021-03-18 00:00:00 +0000 UTC'>March 18, 2021</span>&nbsp;·&nbsp;<span>322 words</span>&nbsp;·&nbsp;<span>RL4AA Collaboration</span></footer><a class=entry-link aria-label="post link to Feasibility Investigation on Several Reinforcement Learning Techniques to Improve the Performance of the FERMI Free-Electron Laser" href=https://RL4aa.github.io/posts/publications/bruchon2021feasiblity/></a></article><article class="post-entry tag-entry"><figure class=entry-cover><img loading=lazy src=https://RL4aa.github.io/imgs/oshea2020policy.png alt="Plot of the reward received by the agent versus step number."></figure><header class=entry-header><h2 class=entry-hint-parent>Policy gradient methods for free-electron laser and terahertz source optimization and stabilization at the FERMI free-electron laser at Elettra</h2></header><div class=entry-content><p>F. H. O’Shea1, N. Bruchon2, G. Gaio1
1Elettra Sincrotrone Trieste, 2University of Trieste
Physical Review Accelerators and Beams
Abstract In this article we report on the application of a model-free reinforcement learning method to the optimization of accelerator systems. We simplify a policy gradient algorithm to accelerator control from sophisticated algorithms that have recently been demonstrated to solve complex dynamic problems. After outlining a theoretical basis for the functioning of the algorithm, we explore the small hyperparameter space to develop intuition about said parameters using a simple number-guess environment. Finally, we demonstrate the algorithm optimizing both a free-electron laser and an accelerator-based terahertz source in-situ. The algorithm is applied to different accelerator control systems and optimizes the desired signals in a few hundred steps without any domain knowledge using up to five control parameters. In addition, the algorithm shows modest tolerance to accelerator fault conditions without any special preparation for such conditions.
...</p></div><footer class=entry-footer><span title='2020-12-21 00:00:00 +0000 UTC'>December 21, 2020</span>&nbsp;·&nbsp;<span>160 words</span>&nbsp;·&nbsp;<span>RL4AA Collaboration</span></footer><a class=entry-link aria-label="post link to Policy gradient methods for free-electron laser and terahertz source optimization and stabilization at the FERMI free-electron laser at Elettra" href=https://RL4aa.github.io/posts/publications/oshea202policy/></a></article><article class="post-entry tag-entry"><figure class=entry-cover><img loading=lazy src=https://RL4aa.github.io/imgs/hirlaender2020modelfree.png alt="A schematic overview of theAE-DYNAapproach used in this paper."></figure><header class=entry-header><h2 class=entry-hint-parent>Model-free and Bayesian Ensembling Model-based Deep Reinforcement Learning for Particle Accelerator Control Demonstrated on the FERMI FEL</h2></header><div class=entry-content><p>S. Hirlaender1, N. Bruchon2
1University of Salzburg, 2University of Trieste
arXiv
Abstract Reinforcement learning holds tremendous promise in accelerator controls. The primary goal of this paper is to show how this approach can be utilised on an operational level on accelerator physics problems. Despite the success of model-free reinforcement learning in several domains, sample-efficiency still is a bottle-neck, which might be encompassed by model-based methods. We compare well-suited purely model-based to model-free reinforcement learning applied to the intensity optimisation on the FERMI FEL system. We find that the model-based approach demonstrates higher representational power and sample-efficiency, while the asymptotic performance of the model-free method is slightly superior. The model-based algorithm is implemented in a DYNA-style using an uncertainty aware model, and the model-free algorithm is based on tailored deep Q-learning. In both cases, the algorithms were implemented in a way, which presents increased noise robustness as omnipresent in accelerator control problems.
...</p></div><footer class=entry-footer><span title='2020-12-17 00:00:00 +0000 UTC'>December 17, 2020</span>&nbsp;·&nbsp;<span>158 words</span>&nbsp;·&nbsp;<span>RL4AA Collaboration</span></footer><a class=entry-link aria-label="post link to Model-free and Bayesian Ensembling Model-based Deep Reinforcement Learning for Particle Accelerator Control Demonstrated on the FERMI FEL" href=https://RL4aa.github.io/posts/publications/hirlaender2020modelfree/></a></article><article class="post-entry tag-entry"><figure class=entry-cover><img loading=lazy src=https://RL4aa.github.io/imgs/pang2020autonomous.png alt="Policy network maps states to actions."></figure><header class=entry-header><h2 class=entry-hint-parent>Autonomous Control of a Particle Accelerator using Deep Reinforcement Learning</h2></header><div class=entry-content><p>X. Pang1, S. Thulasidasan2, L. Rybarcyk2
1Apple, 2Los Alamos National Laboratory
Machine Learning for Engineering Modeling, Simulation, and Design Workshop at Neural Information Processing Systems 2020
Abstract We describe an approach to learning optimal control policies for a large, linear particle accelerator using deep reinforcement learning coupled with a high-fidelity physics engine. The framework consists of an AI controller that uses deep neural networks for state and action-space representation and learns optimal policies using reward signals that are provided by the physics simulator. For this work, we only focus on controlling a small section of the entire accelerator. Nevertheless, initial results indicate that we can achieve better-than-human level performance in terms of particle beam current and distribution. The ultimate goal of this line of work is to substantially reduce the tuning time for such facilities by orders of magnitude, and achieve near-autonomous control.
...</p></div><footer class=entry-footer><span title='2020-12-12 00:00:00 +0000 UTC'>December 12, 2020</span>&nbsp;·&nbsp;<span>150 words</span>&nbsp;·&nbsp;<span>RL4AA Collaboration</span></footer><a class=entry-link aria-label="post link to Autonomous Control of a Particle Accelerator using Deep Reinforcement Learning" href=https://RL4aa.github.io/posts/publications/pang2020autonomous/></a></article><footer class=page-footer><nav class=pagination><a class=prev href=https://RL4aa.github.io/tags/publication/>«&nbsp;Prev&nbsp;
</a><a class=next href=https://RL4aa.github.io/tags/publication/page/3/>Next&nbsp;&nbsp;»</a></nav></footer></main><footer class=footer><span>&copy; 2025 <a href=https://RL4aa.github.io/>RL4AA Collaboration | Homepage</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>