<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/"><channel><title>RL4AA Collaboration | Homepage</title><link>https://RL4aa.github.io/</link><description>Recent content on RL4AA Collaboration | Homepage</description><generator>Hugo -- 0.152.1</generator><language>en-us</language><lastBuildDate>Mon, 07 Apr 2025 00:00:00 +0000</lastBuildDate><atom:link href="https://RL4aa.github.io/index.xml" rel="self" type="application/rss+xml"/><item><title>RL4AA'25 at DESY in Hamburg was a blast! Thank you everyone!</title><link>https://RL4aa.github.io/posts/announcements/rl4aa25_after/</link><pubDate>Mon, 07 Apr 2025 00:00:00 +0000</pubDate><guid>https://RL4aa.github.io/posts/announcements/rl4aa25_after/</guid><description>&lt;p&gt;Wow! The RL4AA'25 workshop at &lt;a href="https://desy.de"&gt;DESY&lt;/a&gt; in Hamburg was a blast, and we couldn&amp;rsquo;t be prouder of the community we have brought together! üòä&lt;/p&gt;
&lt;p&gt;It&amp;rsquo;s unbelievable that we started just over 2 years ago with 30-ish participants, and this year we hosted our third workshop with almost 80 attendees from all over the world, two brilliant keynotes by &lt;a href="https://www.linkedin.com/in/janrpeters/?lipi=urn%3Ali%3Apage%3Ad_flagship3_detail_base%3BKQY%2BWPKNQ5eDSSubgz3gWA%3D%3D"&gt;Jan Peters&lt;/a&gt; and &lt;a href="https://www.linkedin.com/in/alessandro-pau-a46916ba/?lipi=urn%3Ali%3Apage%3Ad_flagship3_detail_base%3BKQY%2BWPKNQ5eDSSubgz3gWA%3D%3D"&gt;Alessandro Pau&lt;/a&gt;, and a very well-received hands-on RL challenge, where the competing teams far exceeded our expectations. Various interesting talks, intriguing posters, lab and city tours, as well as social events were also on our schedule.&lt;/p&gt;</description></item><item><title>Announcing RL4AA'25 taking place 2 - 4 April 2025 in Hamburg, Germany</title><link>https://RL4aa.github.io/posts/announcements/rl4aa25_announcement/</link><pubDate>Thu, 17 Oct 2024 00:00:00 +0000</pubDate><guid>https://RL4aa.github.io/posts/announcements/rl4aa25_announcement/</guid><description>&lt;p&gt;We are delighted to announce the &lt;a href="https://rl4aa.github.io/RL4AA25/"&gt;&amp;ldquo;3rd International Workshop on Reinforcement Learning for Autonomous Accelerators&amp;rdquo; RL4AA‚Äô25&lt;/a&gt;! To be held &lt;strong&gt;2 - 4 April 2025&lt;/strong&gt; at &lt;strong&gt;DESY&lt;/strong&gt; in &lt;strong&gt;Hamburg, Germany&lt;/strong&gt;. Following the very successful RL4AA'23 and RL4AA'24 workshops, the goal of this workshop is to exchange experiences and ideas about RL in the context of particle accelerators amongst both experts and beginners.&lt;/p&gt;
&lt;h3 id="we-have-an-exciting-workshop-program-lined-up"&gt;We have an exciting workshop program lined up!&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Two keynotes by RL experts
&lt;ul&gt;
&lt;li&gt;Jan Peters (University of Darmstadt)&lt;/li&gt;
&lt;li&gt;The second speaker will be announced soon.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Hands-on RL challenge&lt;/li&gt;
&lt;li&gt;Contributed talks&lt;/li&gt;
&lt;li&gt;Posters&lt;/li&gt;
&lt;li&gt;Introduction to RL&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="more-details"&gt;More details:&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Workshop website: &lt;a href="https://rl4aa.github.io/RL4AA25/"&gt;https://rl4aa.github.io/RL4AA25/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Indico link: &lt;a href="https://indico.scc.kit.edu/event/4216/"&gt;https://indico.scc.kit.edu/event/4216/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Call for abstracts: open until 24 January 2025&lt;/li&gt;
&lt;li&gt;&lt;a href="https://indico.scc.kit.edu/event/4216/registrations/764/"&gt;Registration&lt;/a&gt;:
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Open right now!&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;Registration deadline: 7 March 2025&lt;/li&gt;
&lt;li&gt;Thanks to generous sponsorships, there are &lt;strong&gt;no registration fees!&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Workshop: 2 - 4 April 2025, Hamburg, Germany&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Please join us if you have worked in reinforcement learning or you are simply interested and would like to start! This workshop is intended to foster discussions and to start interesting projects together.&lt;/p&gt;</description></item><item><title>Successful RL4AA'24 workshop in Salzburg: Thanks everyone for joining!</title><link>https://RL4aa.github.io/posts/announcements/rl4aa24_after/</link><pubDate>Fri, 16 Feb 2024 00:00:00 +0000</pubDate><guid>https://RL4aa.github.io/posts/announcements/rl4aa24_after/</guid><description>&lt;p&gt;From 5 to 7 February 2024, &lt;a href="https://www.plus.ac.at/aihi/der-fachbereich/ida-lab/"&gt;IDA Lab&lt;/a&gt; at the &lt;a href="https://www.plus.ac.at/?lang=en"&gt;Paris Lodron University of Salzburg&lt;/a&gt; kindly hosted the RL4AA community for the 2nd workshop on Reinforcement Learning for Autonomous Accelerators (&lt;a href="https://rl4aa.github.io/RL4AA24/"&gt;RL4AA'24&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;With over 50 participants from more than 10 different countries, we are excited to see that our community is growing and that the interest in reinforcement learning (for particle accelerators) is increasing.
In a total of 19 talks, we got to hear about the latest developments and impressive results in the field.&lt;/p&gt;</description></item><item><title>RL4AA'24 call for abstracts extended to 5 January. Exciting keynote speakers announced. Register now!</title><link>https://RL4aa.github.io/posts/announcements/rl4aa24_last_chance/</link><pubDate>Fri, 22 Dec 2023 00:00:00 +0000</pubDate><guid>https://RL4aa.github.io/posts/announcements/rl4aa24_last_chance/</guid><description>&lt;p&gt;The call for abstracts for the RL4AA'24 workshop, taking place 05 - 07 February 2024 in Salzburg, Austria, has been extended. &lt;strong&gt;Register for the workshop and submit your abstract until 5 January 2024!&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;We are also excited to announce our keynote speakers:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://araffin.github.io/"&gt;Antonin Raffin&lt;/a&gt; (German Aerospace Center)&lt;/li&gt;
&lt;li&gt;&lt;a href="https://berkenkamp.me/"&gt;Felix Berkenkamp&lt;/a&gt; (Bosch Center for AI)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;For more information on the workshop, please see the official workshop website: &lt;a href="https://rl4aa.github.io/RL4AA24/"&gt;https://rl4aa.github.io/RL4AA24/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;To get directly to registration, please visit: &lt;a href="https://indico.scc.kit.edu/event/3746/"&gt;https://indico.scc.kit.edu/event/3746/&lt;/a&gt;&lt;/p&gt;</description></item><item><title>Registration is now open for RL4AA'24 taking place 05 - 07 February 2024 in Salzburg, Austria</title><link>https://RL4aa.github.io/posts/announcements/rl4aa24_announcement/</link><pubDate>Fri, 18 Aug 2023 00:00:00 +0000</pubDate><guid>https://RL4aa.github.io/posts/announcements/rl4aa24_announcement/</guid><description>&lt;h2 id="announcing-rl4aa24---registration-is-open"&gt;Announcing RL4AA'24 - Registration is open!&lt;/h2&gt;
&lt;p&gt;Following up on the very successful RL4AA'23 workshop in Karlsruhe earlier this year, we are excited to announce the 2nd RL4AA workshop RL4AA'24, which will be held in Salzburg, Austria, from 05 - 07 February 2024. The workshop will be hosted at the &lt;a href="https://www.plus.ac.at/?lang=en"&gt;Paris Lodron University of Salzburg&lt;/a&gt;. We are looking forward to an exciting workshop with many interesting talks and discussions on reinforcement learning for autonomous particle accelerators and hope to see you all in Salzburg!&lt;/p&gt;</description></item><item><title>Learning to Do or Learning While Doing: Reinforcement Learning and Bayesian Optimisation for Online Continuous Tuning</title><link>https://RL4aa.github.io/posts/publications/kaiser2023learning/</link><pubDate>Tue, 06 Jun 2023 00:00:00 +0000</pubDate><guid>https://RL4aa.github.io/posts/publications/kaiser2023learning/</guid><description>&lt;p&gt;&lt;em&gt;&lt;strong&gt;J. Kaiser&lt;sup&gt;1&lt;/sup&gt;, C. Xu&lt;sup&gt;2&lt;/sup&gt;, A. Eichler&lt;sup&gt;1&lt;/sup&gt;, A. Santamaria Garcia&lt;sup&gt;2&lt;/sup&gt;, O. Stein&lt;sup&gt;1&lt;/sup&gt;, E. Br√ºndermann&lt;sup&gt;2&lt;/sup&gt;, W. Kuropka&lt;sup&gt;1&lt;/sup&gt;, H. Dinter&lt;sup&gt;1&lt;/sup&gt;, F. Mayet&lt;sup&gt;1&lt;/sup&gt;, T. Vinatier&lt;sup&gt;1&lt;/sup&gt;, F. Burkart&lt;sup&gt;1&lt;/sup&gt;, H. Schlarb&lt;sup&gt;1&lt;/sup&gt;&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;sup&gt;1&lt;/sup&gt;Deutsches Elektronen-Synchrotron DESY, &lt;sup&gt;2&lt;/sup&gt; Karlsruhe Institute of Technology KIT&lt;/p&gt;
&lt;p&gt;&lt;em&gt;arXiv&lt;/em&gt;&lt;/p&gt;
&lt;h2 id="abstract"&gt;Abstract&lt;/h2&gt;
&lt;p&gt;Online tuning of real-world plants is a complex optimisation problem that continues to require manual intervention by experienced human operators. Autonomous tuning is a rapidly expanding field of research, where learning-based methods, such as Reinforcement Learning-trained Optimisation (RLO) and Bayesian optimisation (BO), hold great promise for achieving outstanding plant performance and reducing tuning times. Which algorithm to choose in different scenarios, however, remains an open question. Here we present a comparative study using a routine task in a real particle accelerator as an example, showing that RLO generally outperforms BO, but is not always the best choice. Based on the study&amp;rsquo;s results, we provide a clear set of criteria to guide the choice of algorithm for a given tuning task. These can ease the adoption of learning-based autonomous tuning solutions to the operation of complex real-world plants, ultimately improving the availability and pushing the limits of operability of these facilities, thereby enabling scientific and engineering advancements.&lt;/p&gt;</description></item><item><title>The RL4AA Discord server is up!</title><link>https://RL4aa.github.io/posts/announcements/discord_server/</link><pubDate>Fri, 02 Jun 2023 00:00:00 +0000</pubDate><guid>https://RL4aa.github.io/posts/announcements/discord_server/</guid><description>&lt;h2 id="good-news"&gt;Good News!&lt;/h2&gt;
&lt;p&gt;The RL4AA community is happy to announce its &lt;a href="https://discord.gg/QtBMqsjWH2"&gt;Discord server&lt;/a&gt;! If you are interested in discussing reinforcement learning applied to accelerators, please join for announcements (e.g. new publications), forum discussions, an open chat, and meeting rooms.&lt;/p&gt;
&lt;p&gt;Hope to see you there!&lt;/p&gt;
&lt;p&gt;&lt;a href="https://discord.gg/QtBMqsjWH2"&gt;https://discord.gg/QtBMqsjWH2&lt;/a&gt;&lt;/p&gt;</description></item><item><title>Trend-Based SAC Beam Control Method with Zero-Shot in Superconducting Linear Accelerator</title><link>https://RL4aa.github.io/posts/publications/chen2023trendbased/</link><pubDate>Tue, 23 May 2023 00:00:00 +0000</pubDate><guid>https://RL4aa.github.io/posts/publications/chen2023trendbased/</guid><description>&lt;p&gt;&lt;em&gt;&lt;strong&gt;X. Chen, X. Qi, C. Su, Y. He, Z. Wang, K. Sun, C. Jin, W. Chen, S. Liu, X. Zhao, D. Jia, M. Yi&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Chinese Academy of Sciences&lt;/p&gt;
&lt;p&gt;&lt;em&gt;arXiv&lt;/em&gt;&lt;/p&gt;
&lt;h2 id="abstract"&gt;Abstract&lt;/h2&gt;
&lt;p&gt;The superconducting linear accelerator is a highly flexiable facility for modern scientific discoveries, necessitating weekly reconfiguration and tuning. Accordingly, minimizing setup time proves essential in affording users with ample experimental time. We propose a trend-based soft actor-critic(TBSAC) beam control method with strong robustness, allowing the agents to be trained in a simulated environment and applied to the real accelerator directly with zero-shot. To validate the effectiveness of our method, two different typical beam control tasks were performed on China Accelerator Facility for Superheavy Elements (CAFe II) and a light particle injector(LPI) respectively. The orbit correction tasks were performed in three cryomodules in CAFe II seperately, the time required for tuning has been reduced to one-tenth of that needed by human experts, and the RMS values of the corrected orbit were all less than 1mm. The other transmission efficiency optimization task was conducted in the LPI, our agent successfully optimized the transmission efficiency of radio-frequency quadrupole(RFQ) to over 85% within 2 minutes. The outcomes of these two experiments offer substantiation that our proposed TBSAC approach can efficiently and effectively accomplish beam commissioning tasks while upholding the same standard as skilled human experts. As such, our method exhibits potential for future applications in other accelerator commissioning fields.&lt;/p&gt;</description></item><item><title>Ultra fast reinforcement learning demonstrated at CERN AWAKE</title><link>https://RL4aa.github.io/posts/publications/ultra_fast_rl_awake/</link><pubDate>Mon, 01 May 2023 00:00:00 +0000</pubDate><guid>https://RL4aa.github.io/posts/publications/ultra_fast_rl_awake/</guid><description>&lt;p&gt;&lt;em&gt;** Simon Hirlaender, Lukas Lamminger, Giovanni Zevi Della Porta, Verena Kain**&lt;/em&gt;&lt;/p&gt;
&lt;h2 id="abstract"&gt;Abstract&lt;/h2&gt;
&lt;p&gt;Reinforcement learning (RL) is a promising direction in machine learning for the control and optimisation of particle accelerators since it learns directly from experience without needing a model a-priori. However, RL generally suffers from low sample efficiency and thus training from scracth on the machine is often not an option. RL agents are usually trained or pre-tuned on simulators and then transferred to the real environment. In this work we propose a model-based RL approach based on Gaussian processes (GPs) to overcome the sample efficiency limitation. Our RL agent was able to learn to control the trajectory at the CERN AWAKE (Advanced Wakefield Experiment) facility, a problem of 10 degrees of freedom, within a few interactions only.
To date, numerical optimises are used to restore or increase and stabilise the performance of accelerators. A major drawback is that they must explore the optimisation space each time they are applied. Our RL approach learns as quickly as numerical optimisers for one optimisation run, but can be used afterwards as single-shot or few-shot controllers. Furthermore, it can also handle safety and time-varying systems and can be used for the online stabilisation of accelerator operation.This approach opens a new avenue for the application of RL in accelerator control and brings it into the realm of everyday applications.&lt;/p&gt;</description></item><item><title>The 2nd RL4AA workshop 2024 will be held 05 - 07 February 2024 in Salzburg</title><link>https://RL4aa.github.io/posts/announcements/rl4aa24_stay_tuned/</link><pubDate>Thu, 27 Apr 2023 00:00:00 +0000</pubDate><guid>https://RL4aa.github.io/posts/announcements/rl4aa24_stay_tuned/</guid><description>&lt;h2 id="good-news---save-the-date"&gt;Good News - save the date!&lt;/h2&gt;
&lt;p&gt;Our first workshop, RL4AA 2023, was very successful. Because of this, we are hoping to hold the 2nd RL4AA workshop in spring 2024 in Salzburg, Austria in 05 - 07 February 2024. Further details will follow soon.&lt;/p&gt;</description></item><item><title>Towards automatic setup of 18 MeV electron beamline using machine learning</title><link>https://RL4aa.github.io/posts/publications/velotti2023towards/</link><pubDate>Thu, 27 Apr 2023 00:00:00 +0000</pubDate><guid>https://RL4aa.github.io/posts/publications/velotti2023towards/</guid><description>&lt;p&gt;&lt;em&gt;&lt;strong&gt;F. M. Velotti&lt;sup&gt;1&lt;/sup&gt;, B. Goddard&lt;sup&gt;1&lt;/sup&gt;, V. Kain&lt;sup&gt;1&lt;/sup&gt;, R. Ramjiawan&lt;sup&gt;1&lt;/sup&gt;, G. Z. Della Porta&lt;sup&gt;1&lt;/sup&gt; and S. Hirlaender&lt;sup&gt;2&lt;/sup&gt;&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;sup&gt;1&lt;/sup&gt;CERN, &lt;sup&gt;2&lt;/sup&gt;University of Salzburg&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Machine Learning: Science and Technology&lt;/em&gt;&lt;/p&gt;
&lt;h2 id="abstract"&gt;Abstract&lt;/h2&gt;
&lt;p&gt;To improve the performance-critical stability and brightness of the electron bunch at injection into the proton-driven plasma wakefield at the AWAKE CERN experiment, automation approaches based on unsupervised machine learning (ML) were developed and deployed. Numerical optimisers were tested together with different model-free reinforcement learning (RL) agents. In order to avoid any bias, RL agents have been trained also using a completely unsupervised state encoding using auto-encoders. To aid hyper-parameter selection, a full synthetic model of the beamline was constructed using a variational auto-encoder trained to generate surrogate data from equipment settings. This paper describes the novel approaches based on deep learning and RL to aid the automatic setup of a low energy line, as the one used to deliver beam to the AWAKE facility. The results obtained with the different ML approaches, including automatic unsupervised feature extraction from images using computer vision are presented. The prospects for operational deployment and wider applicability are discussed.&lt;/p&gt;</description></item><item><title>Orbit Correction Based on Improved Reinforcement Learning Algorithm</title><link>https://RL4aa.github.io/posts/publications/chen2023orbit/</link><pubDate>Thu, 13 Apr 2023 00:00:00 +0000</pubDate><guid>https://RL4aa.github.io/posts/publications/chen2023orbit/</guid><description>&lt;p&gt;&lt;em&gt;&lt;strong&gt;X. Chen, Y. Jia, X. Qi, Z. Wang, Y. He&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Chinese Academy of Sciences&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Physical Review Accelerators and Beams&lt;/em&gt;&lt;/p&gt;
&lt;h2 id="abstract"&gt;Abstract&lt;/h2&gt;
&lt;p&gt;Recently, reinforcement learning (RL) algorithms have been applied to a wide range of control problems in accelerator commissioning. In order to achieve efficient and fast control, these algorithms need to be highly efficient, so as to minimize the online training time. In this paper, we incorporated the beam position monitor trend into the observation space of the twin delayed deep deterministic policy gradient (TD3) algorithm and trained two different structure agents, one based on physical prior knowledge and the other using the original TD3 network architecture. Both of the agents exhibit strong robustness in the simulated environment. The effectiveness of the agent based on physical prior knowledge has been validated in a real accelerator. Results show that the agent can overcome the difference between simulated and real accelerator environments. Once the training is completed in the simulated environment, the agent can be directly applied to the real accelerator without any online training process. The RL agent is deployed to the medium energy beam transport section of China Accelerator Facility for Superheavy Elements. Fast and automatic orbit correction is being tested with up to ten degrees of freedom. The experimental results show that the agents can correct the orbit to within 1 mm. Moreover, due to the strong robustness of the agent, when a trained agent is applied to different lattices of different particles, the orbit correction can still be completed. Since there are no online data collection and training processes, all online corrections are done within 30 s. This paper shows that, as long as the robustness of the RL algorithm is sufficient, the offline learning agents can be directly applied to online correction, which will greatly improve the efficiency of orbit correction. Such an approach to RL may find promising applications in other areas of accelerator commissioning.&lt;/p&gt;</description></item><item><title>RL4AA'23: 1st Collaboration Workshop on Reinforcement Learning for Autonomous Accelerators</title><link>https://RL4aa.github.io/posts/announcements/rl4aa23/</link><pubDate>Tue, 21 Feb 2023 00:00:00 +0000</pubDate><guid>https://RL4aa.github.io/posts/announcements/rl4aa23/</guid><description>&lt;p&gt;Reinforcement learning is the most difficult learning paradigms to understand and to efficiently use, but it holds a lot of promise in the field of accelerator physics.
The applications of reinforcement learning to accelerators today are not very numerous yet, but the interest of the community is growing considerably.
This is how the 1st collaboration workshop on Reinforcement Learning for Autonomous Accelerators (RL4AA'23) came to be! The &lt;a href="https://www.ibpt.kit.edu/AI4Accelerators.php"&gt;AI4Accelerators&lt;/a&gt; team organized and hosted the workshop at KIT, gathering colleagues involved in reinforcement learning. The workshop offered introductory lectures to reinforcement learning, a Python tutorial that studied the real deployment of such an algorithm in a real accelerator, and guided discussion sessions on the most pressing topics. The contents of the discussion will be published in the form of proceedings later.&lt;/p&gt;</description></item><item><title>Optimizing a superconducting radio-frequency gun using deep reinforcement learning</title><link>https://RL4aa.github.io/posts/publications/meier2022optimizing/</link><pubDate>Fri, 28 Oct 2022 00:00:00 +0000</pubDate><guid>https://RL4aa.github.io/posts/publications/meier2022optimizing/</guid><description>&lt;p&gt;&lt;em&gt;&lt;strong&gt;D. Meier&lt;sup&gt;1&lt;/sup&gt;, L. V. Ramirez&lt;sup&gt;1&lt;/sup&gt;, J. V√∂lker&lt;sup&gt;1&lt;/sup&gt;, J. Viefhaus&lt;sup&gt;1&lt;/sup&gt;, B. Sick&lt;sup&gt;2&lt;/sup&gt;, G. Hartmann&lt;sup&gt;1&lt;/sup&gt;&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;sup&gt;1&lt;/sup&gt;Helmholtz-Zentrum Berlin, &lt;sup&gt;2&lt;/sup&gt;University of Kassel&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Physical Review Accelerators and Beams&lt;/em&gt;&lt;/p&gt;
&lt;h2 id="abstract"&gt;Abstract&lt;/h2&gt;
&lt;p&gt;Superconducting photoelectron injectors are promising for generating highly brilliant pulsed electron beams with high repetition rates and low emittances. Experiments such as ultrafast electron diffraction, experiments at the Terahertz scale, and energy recovery linac applications require such properties. However, optimizing the beam properties is challenging due to the high number of possible machine parameter combinations. This article shows the successful automated optimization of beam properties utilizing an already existing simulation model. To reduce the required computation time, we replace the costly simulation with a faster approximation with a neural network. For optimization, we propose a reinforcement learning approach leveraging the simple computation of the derivative of the approximation. We prove that our approach outperforms standard optimization methods for the required function evaluations given a defined minimum accuracy.&lt;/p&gt;</description></item><item><title>Application of reinforcement learning in the LHC tune feedback</title><link>https://RL4aa.github.io/posts/publications/grech2022application/</link><pubDate>Wed, 07 Sep 2022 00:00:00 +0000</pubDate><guid>https://RL4aa.github.io/posts/publications/grech2022application/</guid><description>&lt;p&gt;&lt;em&gt;&lt;strong&gt;L. Grech&lt;sup&gt;1&lt;/sup&gt;, G. Valentino&lt;sup&gt;1&lt;/sup&gt;, D. Alves&lt;sup&gt;2&lt;/sup&gt; and Simon Hirlaender&lt;sup&gt;3&lt;/sup&gt;&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;sup&gt;1&lt;/sup&gt;University of Malta, &lt;sup&gt;2&lt;/sup&gt;CERN, &lt;sup&gt;3&lt;/sup&gt;University of Salzburg&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Frontiers in Physics&lt;/em&gt;&lt;/p&gt;
&lt;h2 id="abstract"&gt;Abstract&lt;/h2&gt;
&lt;p&gt;The Beam-Based Feedback System (BBFS) was primarily responsible for correcting the beam energy, orbit and tune in the CERN Large Hadron Collider (LHC). A major code renovation of the BBFS was planned and carried out during the LHC Long Shutdown 2 (LS2). This work consists of an explorative study to solve a beam-based control problem, the tune feedback (QFB), utilising state-of-the-art Reinforcement Learning (RL). A simulation environment was created to mimic the operation of the QFB. A series of RL agents were trained, and the best-performing agents were then subjected to a set of well-designed tests. The original feedback controller used in the QFB was reimplemented to compare the performance of the classical approach to the performance of selected RL agents in the test scenarios. Results from the simulated environment show that the RL agent performance can exceed the controller-based paradigm.&lt;/p&gt;</description></item><item><title>Learning-based Optimisation of Particle Accelerators Under Partial Observability Without Real-World Training</title><link>https://RL4aa.github.io/posts/publications/kaiser2022learningbased/</link><pubDate>Fri, 22 Jul 2022 00:00:00 +0000</pubDate><guid>https://RL4aa.github.io/posts/publications/kaiser2022learningbased/</guid><description>&lt;p&gt;&lt;em&gt;&lt;strong&gt;J. Kaiser, O. Stein, A. Eichler&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Deutsches Elektronen-Synchrotron DESY&lt;/p&gt;
&lt;p&gt;&lt;em&gt;39th International Conference on Machine Learning&lt;/em&gt;&lt;/p&gt;
&lt;h2 id="abstract"&gt;Abstract&lt;/h2&gt;
&lt;p&gt;In recent work, it has been shown that reinforcement learning (RL) is capable of solving a variety of problems at sometimes super-human performance levels. But despite continued advances in the field, applying RL to complex real-world control and optimisation problems has proven difficult. In this contribution, we demonstrate how to successfully apply RL to the optimisation of a highly complex real-world machine ‚Äì specifically a linear particle accelerator ‚Äì in an only partially observable setting and without requiring training on the real machine. Our method outperforms conventional optimisation algorithms in both the achieved result and time taken as well as already achieving close to human-level performance. We expect that such automation of machine optimisation will push the limits of operability, increase machine availability and lead to a paradigm shift in how such machines are operated, ultimately facilitating advances in a variety of fields, such as science and medicine among many others.&lt;/p&gt;</description></item><item><title>Automated Intensity Optimisation Using Reinforcement Learning at LEIR</title><link>https://RL4aa.github.io/posts/publications/madysa2022automated/</link><pubDate>Sun, 12 Jun 2022 00:00:00 +0000</pubDate><guid>https://RL4aa.github.io/posts/publications/madysa2022automated/</guid><description>&lt;p&gt;&lt;em&gt;&lt;strong&gt;N. Madysa, V. Kain, R. Alemany Fernandez, N. Biancacci, B. Goddard, F. M. Velotti&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;CERN&lt;/p&gt;
&lt;p&gt;&lt;em&gt;13th Particle Accelerator Conference&lt;/em&gt;&lt;/p&gt;
&lt;h2 id="abstract"&gt;Abstract&lt;/h2&gt;
&lt;p&gt;High intensities in the Low Energy Ion Ring (LEIR) at CERN are achieved by stacking several multi-turn injec- tions from the pre-accelerator Linac3. Up to seven consec- utive 200 Œºs long, 200 ms spaced pulses are injected from Linac3 into LEIR. Two inclined septa, one magnetic and one electrostatic, combined with a collapsing horizontal or- bit bump allows a 6-D phase space painting via a linearly ramped mean momentum along the Linac3 pulse and in- jection at high dispersion. The already circulating beam is cooled and dragged longitudinally via electron cooling (e- cooling) into a stacking momentum to free space for the fol- lowing injections. For optimal intensity accumulation, the electron energy and trajectory need to match the ion energy and orbit at the e-cooler section.&lt;/p&gt;</description></item><item><title>Micro-Bunching Control at Electron Storage Rings with Reinforcement Learning</title><link>https://RL4aa.github.io/posts/publications/boltz2019microbunching/</link><pubDate>Fri, 12 Nov 2021 00:00:00 +0000</pubDate><guid>https://RL4aa.github.io/posts/publications/boltz2019microbunching/</guid><description>&lt;p&gt;&lt;em&gt;&lt;strong&gt;T. Boltz&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Karlsruhe Insitute of Technology KIT&lt;/p&gt;
&lt;p&gt;&lt;em&gt;PhD thesis&lt;/em&gt;&lt;/p&gt;
&lt;h2 id="abstract"&gt;Abstract&lt;/h2&gt;
&lt;p&gt;At the time this thesis is written, the world finds itself amidst and partly in the process of recovering from the COVID-19 pandemic caused by the SARS-Cov-2 virus. One major contribution to the worldwide efforts of bringing this pandemic to an end are the vaccines developed by different research teams all around the globe. Produced in a remarkably short time frame, a crucial first step for the discovery of these vaccines was mapping out the atomic structure of the proteins making up the virus and their interactions. Due to the bright X-rays required in the process, synchrotron light sources play an active role in the ongoing efforts of accomplishing that goal. Synchrotron light sources are particle accelerators that are capable of providing intense electromagnetic radiation by accelerating packages of electrons, called bunches, and forcing them on curved trajectories. Besides the support of research on the SARS-Cov-2 virus, the remarkable properties of synchrotron radiation lead to a multitude of applications in a variety of scientific fields such as materials science, geology, biology and medicine. As a special form of synchrotron radiation, this thesis is concerned with the coherent synchrotron radiation (CSR) generated by short electron bunches in a storage ring. At wavelengths larger than the size of the emitting electron structure, the particles within a bunch radiate coherently. This coherent emission of synchrotron radiation scales with the number of involved particles and can thus enhance the intensity of the emitted radiation by several orders of magnitude. As a consequence, modern synchrotron light sources, such as the Karlsruhe Research Accelerator (KARA) at the Karlsruhe Institute of Technology (KIT), are deliberately operating with short bunch lengths to extend the radiated CSR spectrum to higher frequencies and to increase the intensity of the emitted radiation. Yet, the continuous reduction of the bunch length at high beam intensities eventually leads to complex longitudinal dynamics caused by the self-interaction of the electron bunches with their own emitted CSR. This phenomenon, generally referred to as micro-bunching or micro-wave instability, can lead to the formation of dynamically changing micro-structures within the charge distribution of the electron bunches and thus to a uctuating emission of CSR. Moreover, it can cause oscillations of the bunch length and the energy spread, which can be detrimental to the operation of a synchrotron light source. On the other hand, as electron structures smaller than the full electron bunch, the micro-structures created by the instability lead to an increased emission of CSR at frequencies up to the THz frequency range. The instability can thus also be beneficial for a variety of applications that rely on intense radiation in that particular frequency range.&lt;/p&gt;</description></item><item><title>Real-time artificial intelligence for accelerator control: A study at the Fermilab Booster</title><link>https://RL4aa.github.io/posts/publications/stjohn2021realtime/</link><pubDate>Mon, 18 Oct 2021 00:00:00 +0000</pubDate><guid>https://RL4aa.github.io/posts/publications/stjohn2021realtime/</guid><description>&lt;p&gt;&lt;em&gt;&lt;strong&gt;J. St. John&lt;sup&gt;1&lt;/sup&gt;, C. Herwig&lt;sup&gt;1&lt;/sup&gt;, D. Kafkes&lt;sup&gt;1&lt;/sup&gt;, J. Mitrevski&lt;sup&gt;1&lt;/sup&gt;, W. A. Pellico&lt;sup&gt;1&lt;/sup&gt;, G. N. Perdue&lt;sup&gt;1&lt;/sup&gt;, A. Quintero-Parra&lt;sup&gt;1&lt;/sup&gt;, B. A. Schupbach&lt;sup&gt;1&lt;/sup&gt;, K. Seiya&lt;sup&gt;1&lt;/sup&gt;, N. Tran&lt;sup&gt;1&lt;/sup&gt;, M. Schram&lt;sup&gt;2&lt;/sup&gt;, J. M. Duarte&lt;sup&gt;3&lt;/sup&gt;, Y. Huang&lt;sup&gt;4&lt;/sup&gt;, R. Keller&lt;sup&gt;5&lt;/sup&gt;&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;sup&gt;1&lt;/sup&gt;Fermi National Accelerator Laboratory, &lt;sup&gt;2&lt;/sup&gt;Thomas Jefferson National Accelerator Laboratory, &lt;sup&gt;3&lt;/sup&gt;University of California San Diego, &lt;sup&gt;4&lt;/sup&gt;Pacific Northwest National Laboratory, &lt;sup&gt;5&lt;/sup&gt;Columbia University&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Physical Review Accelerators and Beams&lt;/em&gt;&lt;/p&gt;
&lt;h2 id="abstract"&gt;Abstract&lt;/h2&gt;
&lt;p&gt;We describe a method for precisely regulating the gradient magnet power supply (GMPS) at the Fermilab Booster accelerator complex using a neural network trained via reinforcement learning. We demonstrate preliminary results by training a surrogate machine-learning model on real accelerator data to emulate the GMPS, and using this surrogate model in turn to train the neural network for its regulation task. We additionally show how the neural networks to be deployed for control purposes may be compiled to execute on field-programmable gate arrays (FPGAs), and show the first machine-learning based control algorithm implemented on an FPGA for controls at the Fermilab accelerator complex. As there are no surprise latencies on an FPGA, this capability is important for operational stability in complicated environments such as an accelerator facility.&lt;/p&gt;</description></item><item><title>Test of Machine Learning at the CERN LINAC4</title><link>https://RL4aa.github.io/posts/publications/kain2021test/</link><pubDate>Mon, 04 Oct 2021 00:00:00 +0000</pubDate><guid>https://RL4aa.github.io/posts/publications/kain2021test/</guid><description>&lt;p&gt;&lt;em&gt;&lt;strong&gt;V. Kain&lt;sup&gt;1&lt;/sup&gt;, N. Bruchon&lt;sup&gt;1&lt;/sup&gt;, S. Hirlander&lt;sup&gt;1&lt;/sup&gt;, N. Madysa&lt;sup&gt;1&lt;/sup&gt;, I. Vojskovic&lt;sup&gt;1&lt;/sup&gt;, P. Skowronski&lt;sup&gt;1&lt;/sup&gt;, G. Valentino&lt;sup&gt;2&lt;/sup&gt;&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;sup&gt;1&lt;/sup&gt;CERN, &lt;sup&gt;2&lt;/sup&gt;University of Malta&lt;/p&gt;
&lt;p&gt;&lt;em&gt;61st ICFA ABDW on High-Intensity and High-Brightness Hadron Beams&lt;/em&gt;&lt;/p&gt;
&lt;h2 id="abstract"&gt;Abstract&lt;/h2&gt;
&lt;p&gt;The CERN H‚àílinear accelerator, LINAC4, served as atest bed for advanced algorithms during the CERN LongShutdown 2 in the years 2019/20. One of the main goals wasto show that reinforcement learning with all its benefits canbe used as a replacement for numerical optimization and asa complement to classical control in the accelerator controlcontext. Many of the algorithms used were prepared before-hand at the electron line of the AWAKE facility to makethe best use of the limited time available at LINAC4. Anoverview of the algorithms and concepts tested at LINAC4and AWAKE will be given and the results discussed.&lt;/p&gt;</description></item><item><title>Renovation of the beam-based feedback systems in the LHC</title><link>https://RL4aa.github.io/posts/publications/grech2021renovation/</link><pubDate>Wed, 01 Sep 2021 00:00:00 +0000</pubDate><guid>https://RL4aa.github.io/posts/publications/grech2021renovation/</guid><description>&lt;p&gt;&lt;em&gt;&lt;strong&gt;L. Grech&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;University of Malta&lt;/p&gt;
&lt;p&gt;&lt;em&gt;PhD thesis&lt;/em&gt;&lt;/p&gt;
&lt;h2 id="abstract"&gt;Abstract&lt;/h2&gt;
&lt;p&gt;The Large Hadron Collider (LHC) at the European Organization for Nuclear Research (CERN) is the largest synchrotron built to date, having a circumference of approx- imately 27km. The LHC is able to accelerate two counter-rotating proton and/or heavy-ion beams up to 7 TeV per charge. These highly energetic beams are contained inside a vacuum chamber with an inner diameter of 80 mm by means of strong mag- netic fields produced by superconducting magnets. A beam cleaning and machine protection system is in place to prevent high-energy halo particles from impacting and heating the superconducting magnets.&lt;/p&gt;</description></item><item><title>Accelerated Deep Reinforcement Learning for Fast Feedback of Beam Dynamics at KARA</title><link>https://RL4aa.github.io/posts/publications/wang2021accelerated/</link><pubDate>Thu, 27 May 2021 00:00:00 +0000</pubDate><guid>https://RL4aa.github.io/posts/publications/wang2021accelerated/</guid><description>&lt;p&gt;&lt;em&gt;&lt;strong&gt;W. Wang&lt;sup&gt;1&lt;/sup&gt;, M. Caselle&lt;sup&gt;1&lt;/sup&gt;, T. Boltz&lt;sup&gt;1&lt;/sup&gt;, E. Blomley&lt;sup&gt;1&lt;/sup&gt;, M. Brosi&lt;sup&gt;1&lt;/sup&gt;, T. Dritschler&lt;sup&gt;1&lt;/sup&gt;, A. Ebersoldt&lt;sup&gt;1&lt;/sup&gt;, A. Kopmann&lt;sup&gt;1&lt;/sup&gt;, A. Santamaria Garcia&lt;sup&gt;1&lt;/sup&gt;, P. Schreiber&lt;sup&gt;1&lt;/sup&gt;, E. Br√ºndermann&lt;sup&gt;1&lt;/sup&gt;, M. Weber&lt;sup&gt;1&lt;/sup&gt;, A.-S. M√ºller&lt;sup&gt;1&lt;/sup&gt;, Y. Fang&lt;sup&gt;2&lt;/sup&gt;&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;sup&gt;1&lt;/sup&gt;Karlsruhe Insitute of Technology KIT, &lt;sup&gt;2&lt;/sup&gt;Northwestern Polytechnical University&lt;/p&gt;
&lt;p&gt;&lt;em&gt;IEEE Transactions on Nuclear Science&lt;/em&gt;&lt;/p&gt;
&lt;h2 id="abstract"&gt;Abstract&lt;/h2&gt;
&lt;p&gt;Coherent synchrotron radiation (CSR) is generated when the electron bunch length is in the order of the magnitude of the wavelength of the emitted radiation. The self-interaction of short electron bunches with their own electromagnetic fields changes the longitudinal beam dynamics significantly. Above a certain current threshold, the micro-bunching instability develops, characterized by the appearance of distinguishable substructures in the longitudinal phase space of the bunch. To stabilize the CSR emission, a real-time feedback control loop based on reinforcement learning (RL) is proposed. Informed by the available THz diagnostics, the feedback is designed to act on the radio frequency (RF) system of the storage ring to mitigate the micro-bunching dynamics. To satisfy low-latency requirements given by the longitudinal beam dynamics, the RL controller has been implemented on hardware (FPGA). In this article, a real-time feedback loop architecture and its performance is presented and compared with a software implementation using Keras-RL on CPU/GPU. The results obtained with the CSR simulation Inovesa demonstrate that the functionality of both platforms is equivalent. The training performance of the hardware implementation is similar to software solution, while it outperforms the Keras-RL implementation by an order of magnitude. The presented RL hardware controller is considered as an essential platform for the development of intelligent CSR control systems.&lt;/p&gt;</description></item><item><title>First Steps Toward an Autonomous Accelerator, A Common Project Between DESY and KIT</title><link>https://RL4aa.github.io/posts/publications/eichler2021first/</link><pubDate>Mon, 24 May 2021 00:00:00 +0000</pubDate><guid>https://RL4aa.github.io/posts/publications/eichler2021first/</guid><description>&lt;p&gt;&lt;em&gt;&lt;strong&gt;A. Eichler&lt;sup&gt;1&lt;/sup&gt;, F. Burkart&lt;sup&gt;1&lt;/sup&gt;, J. Kaiser&lt;sup&gt;1&lt;/sup&gt;, W. Kuropka&lt;sup&gt;1&lt;/sup&gt;, O. Stein&lt;sup&gt;1&lt;/sup&gt;, E. Br√ºndermann&lt;sup&gt;2&lt;/sup&gt;, A. Santamaria Garcia&lt;sup&gt;2&lt;/sup&gt;, C. Xu&lt;sup&gt;2&lt;/sup&gt;&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;sup&gt;1&lt;/sup&gt;Deutsches Elektronen-Synchrotron DESY, &lt;sup&gt;2&lt;/sup&gt;Karlsruhe Institute of Technology KIT&lt;/p&gt;
&lt;p&gt;&lt;em&gt;12th International Particle Accelerator Conference&lt;/em&gt;&lt;/p&gt;
&lt;h2 id="abstract"&gt;Abstract&lt;/h2&gt;
&lt;p&gt;Reinforcement learning algorithms have risen in pop-ularity in the accelerator physics community in recentyears, showing potential in beam control and in the opti-mization and automation of tasks in accelerator operation.The Helmholtz AI project ‚ÄúMachine Learning Toward Au-tonomous Accelerators‚Äù is a collaboration between DESYand KIT that works on investigating and developing rein-forcement learning applications for the automatic start-upof electron linear accelerators. The work is carried out inparallel at two similar research accelerators: ARES at DESYand FLUTE at KIT, giving the unique opportunity of trans-fer learning between facilities. One of the first steps of thisproject is the establishment of a common interface betweenthe simulations and the machine, in order to test and applyvarious optimization approaches interchangeably betweenthe two accelerators. In this paper we present first results onthe common interface and its application to beam focusingin ARES as well as the idea of laser shaping with spatiallight modulators at FLUTE.&lt;/p&gt;</description></item><item><title>Physics-Enhanced Reinforcement Learning for Optimal Control</title><link>https://RL4aa.github.io/posts/publications/ivanov2021physicsenhanced/</link><pubDate>Fri, 21 May 2021 00:00:00 +0000</pubDate><guid>https://RL4aa.github.io/posts/publications/ivanov2021physicsenhanced/</guid><description>&lt;p&gt;&lt;em&gt;&lt;strong&gt;A. Ivanov, I. Agapov, A. Eichler, S. Tomin&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Deutsches Elektronen Synchrotron DESY&lt;/p&gt;
&lt;p&gt;&lt;em&gt;12th International Particle Accelerator Conference&lt;/em&gt;&lt;/p&gt;
&lt;h2 id="abstract"&gt;Abstract&lt;/h2&gt;
&lt;p&gt;We propose an approach for incorporating acceleratorphysics models into reinforcement learning agents. The proposed approach is based on the Taylor mapping technique for the simulation of particle dynamics. The resulting computational graph is represented as a polynomial neural network and embedded into the traditional reinforcement learning agents. The application of the model is demonstrated in a nonlinear simulation model of beam transmission. The comparison of the approach with the traditional numerical optimization as well as neural networks-based agents demonstrates better convergence of the proposed technique.&lt;/p&gt;</description></item><item><title>Feasibility Investigation on Several Reinforcement Learning Techniques to Improve the Performance of the FERMI Free-Electron Laser</title><link>https://RL4aa.github.io/posts/publications/bruchon2021feasiblity/</link><pubDate>Thu, 18 Mar 2021 00:00:00 +0000</pubDate><guid>https://RL4aa.github.io/posts/publications/bruchon2021feasiblity/</guid><description>&lt;p&gt;&lt;em&gt;&lt;strong&gt;N. Bruchon&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;University of Trieste&lt;/p&gt;
&lt;p&gt;&lt;em&gt;PhD thesis&lt;/em&gt;&lt;/p&gt;
&lt;h2 id="abstract"&gt;Abstract&lt;/h2&gt;
&lt;p&gt;The research carried out in particle accelerator facilities does not concern only particle and condensed matter physics, although these are the main topics covered in the field. Indeed, since a particle accelerator is composed of many different sub-systems, its proper functioning depends both on each of these parts and their interconnection. It follows that the study, implementation, and improvement of the various sub-systems are fundamental points of investigation too. In particular, an interesting aspect for the automation engineering community is the control of such systems that usually are complex, large, noise-affected, and non-linear.&lt;/p&gt;</description></item><item><title>Policy gradient methods for free-electron laser and terahertz source optimization and stabilization at the FERMI free-electron laser at Elettra</title><link>https://RL4aa.github.io/posts/publications/oshea202policy/</link><pubDate>Mon, 21 Dec 2020 00:00:00 +0000</pubDate><guid>https://RL4aa.github.io/posts/publications/oshea202policy/</guid><description>&lt;p&gt;&lt;em&gt;&lt;strong&gt;F. H. O&amp;rsquo;Shea&lt;sup&gt;1&lt;/sup&gt;, N. Bruchon&lt;sup&gt;2&lt;/sup&gt;, G. Gaio&lt;sup&gt;1&lt;/sup&gt;&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;sup&gt;1&lt;/sup&gt;Elettra Sincrotrone Trieste, &lt;sup&gt;2&lt;/sup&gt;University of Trieste&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Physical Review Accelerators and Beams&lt;/em&gt;&lt;/p&gt;
&lt;h2 id="abstract"&gt;Abstract&lt;/h2&gt;
&lt;p&gt;In this article we report on the application of a model-free reinforcement learning method to the optimization of accelerator systems. We simplify a policy gradient algorithm to accelerator control from sophisticated algorithms that have recently been demonstrated to solve complex dynamic problems. After outlining a theoretical basis for the functioning of the algorithm, we explore the small hyperparameter space to develop intuition about said parameters using a simple number-guess environment. Finally, we demonstrate the algorithm optimizing both a free-electron laser and an accelerator-based terahertz source in-situ. The algorithm is applied to different accelerator control systems and optimizes the desired signals in a few hundred steps without any domain knowledge using up to five control parameters. In addition, the algorithm shows modest tolerance to accelerator fault conditions without any special preparation for such conditions.&lt;/p&gt;</description></item><item><title>Model-free and Bayesian Ensembling Model-based Deep Reinforcement Learning for Particle Accelerator Control Demonstrated on the FERMI FEL</title><link>https://RL4aa.github.io/posts/publications/hirlaender2020modelfree/</link><pubDate>Thu, 17 Dec 2020 00:00:00 +0000</pubDate><guid>https://RL4aa.github.io/posts/publications/hirlaender2020modelfree/</guid><description>&lt;p&gt;&lt;em&gt;&lt;strong&gt;S. Hirlaender&lt;sup&gt;1&lt;/sup&gt;, N. Bruchon&lt;sup&gt;2&lt;/sup&gt;&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;sup&gt;1&lt;/sup&gt;University of Salzburg, &lt;sup&gt;2&lt;/sup&gt;University of Trieste&lt;/p&gt;
&lt;p&gt;&lt;em&gt;arXiv&lt;/em&gt;&lt;/p&gt;
&lt;h2 id="abstract"&gt;Abstract&lt;/h2&gt;
&lt;p&gt;Reinforcement learning holds tremendous promise in accelerator controls. The primary goal of this paper is to show how this approach can be utilised on an operational level on accelerator physics problems. Despite the success of model-free reinforcement learning in several domains, sample-efficiency still is a bottle-neck, which might be encompassed by model-based methods. We compare well-suited purely model-based to model-free reinforcement learning applied to the intensity optimisation on the FERMI FEL system. We find that the model-based approach demonstrates higher representational power and sample-efficiency, while the asymptotic performance of the model-free method is slightly superior. The model-based algorithm is implemented in a DYNA-style using an uncertainty aware model, and the model-free algorithm is based on tailored deep Q-learning. In both cases, the algorithms were implemented in a way, which presents increased noise robustness as omnipresent in accelerator control problems.&lt;/p&gt;</description></item><item><title>Autonomous Control of a Particle Accelerator using Deep Reinforcement Learning</title><link>https://RL4aa.github.io/posts/publications/pang2020autonomous/</link><pubDate>Sat, 12 Dec 2020 00:00:00 +0000</pubDate><guid>https://RL4aa.github.io/posts/publications/pang2020autonomous/</guid><description>&lt;p&gt;&lt;em&gt;&lt;strong&gt;X. Pang&lt;sup&gt;1&lt;/sup&gt;, S. Thulasidasan&lt;sup&gt;2&lt;/sup&gt;, L. Rybarcyk&lt;sup&gt;2&lt;/sup&gt;&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;sup&gt;1&lt;/sup&gt;Apple, &lt;sup&gt;2&lt;/sup&gt;Los Alamos National Laboratory&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Machine Learning for Engineering Modeling, Simulation, and Design Workshop at Neural Information Processing Systems 2020&lt;/em&gt;&lt;/p&gt;
&lt;h2 id="abstract"&gt;Abstract&lt;/h2&gt;
&lt;p&gt;We describe an approach to learning optimal control policies for a large, linear particle accelerator using deep reinforcement learning coupled with a high-fidelity physics engine. The framework consists of an AI controller that uses deep neural networks for state and action-space representation and learns optimal policies using reward signals that are provided by the physics simulator. For this work, we only focus on controlling a small section of the entire accelerator. Nevertheless, initial results indicate that we can achieve better-than-human level performance in terms of particle beam current and distribution. The ultimate goal of this line of work is to substantially reduce the tuning time for such facilities by orders of magnitude, and achieve near-autonomous control.&lt;/p&gt;</description></item><item><title>Sample-efficient reinforcement learning for CERN accelerator control</title><link>https://RL4aa.github.io/posts/publications/kain2020sampleefficient/</link><pubDate>Tue, 01 Dec 2020 00:00:00 +0000</pubDate><guid>https://RL4aa.github.io/posts/publications/kain2020sampleefficient/</guid><description>&lt;p&gt;&lt;em&gt;&lt;strong&gt;V. Kain&lt;sup&gt;1&lt;/sup&gt;, S. Hirlander&lt;sup&gt;1&lt;/sup&gt;, B. Goddard&lt;sup&gt;1&lt;/sup&gt;, F. M. Velotti&lt;sup&gt;1&lt;/sup&gt;, G. Z. Della Porta&lt;sup&gt;1&lt;/sup&gt;, N. Bruchon&lt;sup&gt;2&lt;/sup&gt;, G. Valentino&lt;sup&gt;3&lt;/sup&gt;&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;sup&gt;1&lt;/sup&gt;CERN, &lt;sup&gt;2&lt;/sup&gt;University of Trieste, &lt;sup&gt;3&lt;/sup&gt;University of Malta&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Physical Review Accelerators and Beams&lt;/em&gt;&lt;/p&gt;
&lt;h2 id="abstract"&gt;Abstract&lt;/h2&gt;
&lt;p&gt;Numerical optimization algorithms are already established tools to increase and stabilize the performance of particle accelerators. These algorithms have many advantages, are available out of the box, and can be adapted to a wide range of optimization problems in accelerator operation. The next boost in efficiency is expected to come from reinforcement learning algorithms that learn the optimal policy for a certain control problem and hence, once trained, can do without the time-consuming exploration phase needed for numerical optimizers. To investigate this approach, continuous model-free reinforcement learning with up to 16 degrees of freedom was developed and successfully tested at various facilities at CERN. The approach and algorithms used are discussed and the results obtained for trajectory steering at the AWAKE electron line and LINAC4 are presented. The necessary next steps, such as uncertainty aware model-based approaches, and the potential for future applications at particle accelerators are addressed.&lt;/p&gt;</description></item><item><title>Neural Networks for Modeling and Control of Particle Accelerators</title><link>https://RL4aa.github.io/posts/publications/edelen2020neural/</link><pubDate>Wed, 01 Jul 2020 00:00:00 +0000</pubDate><guid>https://RL4aa.github.io/posts/publications/edelen2020neural/</guid><description>&lt;p&gt;&lt;em&gt;&lt;strong&gt;A. L. Edelen&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Colorado State University&lt;/p&gt;
&lt;p&gt;&lt;em&gt;PhD thesis&lt;/em&gt;&lt;/p&gt;
&lt;h2 id="abstract"&gt;Abstract&lt;/h2&gt;
&lt;p&gt;Charged particle accelerators support a wide variety of scientific, industrial, and medical applications. They range in scale and complexity from systems with just a few components for beam acceleration and manipulation, to large scientific user facilities that span many kilometers and have hundreds-to-thousands of individually-controllable components. Specific operational requirements must be met by adjusting the many controllable variables of the accelerator. Meeting these requirements can be challenging, both in terms of the ability to achieve specific beam quality metrics in a reliable fashion and in terms of the time needed to set up and maintain the optimal operating conditions. One avenue toward addressing this challenge is to incorporate techniques from the fields of machine learning (ML) and artificial intelligence (AI) into the way particle accelerators are modeled and controlled. While many promising approaches within AI/ML could beused for particle accelerators, this dissertation focuses on approaches based on neural networks. Neural networks are particularly well-suited to modeling, control, and diagnostic analysis of non-linear systems, as well as systems with large parameter spaces. They are also very appealing for their ability to process high-dimensional data types, such as images and time series (both of which are ubiquitous in particle accelerators). In this work, key studies that demonstrated the potential utility of modern neural network-based approaches to modeling and control of particle accelerators are presented. The context for this work is important: at the start of this work in 2012, there was little interest in AI/ML in the particle accelerator community, and many of the advances in neural networks and deep learning that enabled its present success had not yet been made at that time. As such, this work was both an exploration of possible application areas and a generator of initial demonstrations in these areas, including some of the first applications of modern deep neural networks in particle accelerators.&lt;/p&gt;</description></item><item><title>Basic Reinforcement Learning Techniques to Control the Intensity of a Seeded Free-Electron Laser</title><link>https://RL4aa.github.io/posts/publications/bruchon2020basic/</link><pubDate>Sat, 09 May 2020 00:00:00 +0000</pubDate><guid>https://RL4aa.github.io/posts/publications/bruchon2020basic/</guid><description>&lt;p&gt;&lt;em&gt;&lt;strong&gt;N. Bruchon&lt;sup&gt;1&lt;/sup&gt;, G. Fenu&lt;sup&gt;1&lt;/sup&gt;, G. Gaio&lt;sup&gt;2&lt;/sup&gt;, M. Lonza&lt;sup&gt;2&lt;/sup&gt;, F. H. O‚ÄôShea&lt;sup&gt;2&lt;/sup&gt;, F. A. Pellegrino&lt;sup&gt;1&lt;/sup&gt;, E. Salvato&lt;sup&gt;1&lt;/sup&gt;&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;sup&gt;1&lt;/sup&gt;University of Trieste, &lt;sup&gt;2&lt;/sup&gt;Elettra Sincrotrone Trieste&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Electronics&lt;/em&gt;&lt;/p&gt;
&lt;h2 id="abstract"&gt;Abstract&lt;/h2&gt;
&lt;p&gt;Optimal tuning of particle accelerators is a challenging task. Many different approaches have been proposed in the past to solve two main problems‚Äîattainment of an optimal working point and performance recovery after machine drifts. The most classical model-free techniques (e.g., Gradient Ascent or Extremum Seeking algorithms) have some intrinsic limitations. To overcome those limitations, Machine Learning tools, in particular Reinforcement Learning (RL), are attracting more and more attention in the particle accelerator community. We investigate the feasibility of RL model-free approaches to align the seed laser, as well as other service lasers, at FERMI, the free-electron laser facility at Elettra Sincrotrone Trieste. We apply two different techniques‚Äîthe first, based on the episodic Q-learning with linear function approximation, for performance optimization; the second, based on the continuous Natural Policy Gradient REINFORCE algorithm, for performance recovery. Despite the simplicity of these approaches, we report satisfactory preliminary results, that represent the first step toward a new fully automatic procedure for the alignment of the seed laser to the electron beam. Such an alignment is, at present, performed manually.&lt;/p&gt;</description></item><item><title>Toward the Application of Reinforcement Learning to the Intensity Control of a Seeded Free-Electron Laser</title><link>https://RL4aa.github.io/posts/publications/bruchon2019toward/</link><pubDate>Wed, 23 Oct 2019 00:00:00 +0000</pubDate><guid>https://RL4aa.github.io/posts/publications/bruchon2019toward/</guid><description>&lt;p&gt;&lt;em&gt;&lt;strong&gt;N. Bruchon, G. Fenu, G. Gaio, M. Lonza, F. A. Pellegrino, E. Salvato&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;University of Trieste&lt;/p&gt;
&lt;p&gt;&lt;em&gt;23rd International Conference on Mechatronics Technology&lt;/em&gt;&lt;/p&gt;
&lt;h2 id="abstract"&gt;Abstract&lt;/h2&gt;
&lt;p&gt;The optimization of particle accelerators is a challenging task, and many different approaches have been proposed in years, to obtain an optimal tuning of the plant and to keep it optimally tuned despite drifts or disturbances. Indeed, the classical model-free approaches (such as Gradient Ascent or Extremum Seeking algorithms) have intrinsic limitations. To overcome those limitations, Machine Learning techniques, in particular, the Reinforcement Learning, are attracting more and more attention in the particle accelerator community. The purpose of this paper is to apply a Reinforcement Learning model-free approach to the alignment of a seed laser, based on a rather general target function depending on the laser trajectory. The study focuses on the alignment of the lasers at FERMI, the free-electron laser facility at Elettra Sincrotrone Trieste. In particular, we employ Q-learning with linear function approximation and report experimental results obtained in two setups, which are the actual setups where the final application has to be deployed. Despite the simplicity of the approach, we report satisfactory preliminary results, that represent the first step toward a fully automatic procedure for seed laser to the electron beam. Such a superimposition is, at present, performed manually.&lt;/p&gt;</description></item><item><title>Free-electron Laser Optimization with Reinforcement Learning</title><link>https://RL4aa.github.io/posts/publications/bruchon2019freeelectron/</link><pubDate>Sat, 05 Oct 2019 00:00:00 +0000</pubDate><guid>https://RL4aa.github.io/posts/publications/bruchon2019freeelectron/</guid><description>&lt;p&gt;&lt;em&gt;&lt;strong&gt;N. Bruchon&lt;sup&gt;1&lt;/sup&gt;, G. Gaio&lt;sup&gt;2&lt;/sup&gt;, G. Fenu&lt;sup&gt;1&lt;/sup&gt;, M. Lonza&lt;sup&gt;2&lt;/sup&gt;, F. A. Pellegrino&lt;sup&gt;1&lt;/sup&gt;, E. Salvato&lt;sup&gt;1&lt;/sup&gt;&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;sup&gt;1&lt;/sup&gt;University of Trieste, &lt;sup&gt;2&lt;/sup&gt;Elettra Sincrotrone Trieste&lt;/p&gt;
&lt;p&gt;&lt;em&gt;17th International Conference on Accelerator and Large Experimental Physics Control Systems&lt;/em&gt;&lt;/p&gt;
&lt;h2 id="abstract"&gt;Abstract&lt;/h2&gt;
&lt;p&gt;Reinforcement Learning (RL) is one of the most promis-ing techniques in Machine Learning because of its modestcomputational requirements with respect to other algorithms.RL uses an agent that takes actions within its environmentto maximize a reward related to the goal it is designed toachieve. We have recently used RL as a model-free approachto improve the performance of the FERMI Free ElectronLaser. A number of machine parameters are adjusted tofind the optimum FEL output in terms of intensity and spec-tral quality. In particular we focus on the problem of thealignment of the seed laser with the electron beam, initiallyusing a simplified model and then applying the developedalgorithm on the real machine. This paper reports the resultsobtained and discusses pros and cons of this approach withplans for future applications.&lt;/p&gt;</description></item><item><title>Feedback Design for Control of the Micro-Bunching Instability Based on Reinforcement Learning</title><link>https://RL4aa.github.io/posts/publications/boltz2019feedback/</link><pubDate>Sun, 19 May 2019 00:00:00 +0000</pubDate><guid>https://RL4aa.github.io/posts/publications/boltz2019feedback/</guid><description>&lt;p&gt;&lt;em&gt;&lt;strong&gt;T. Boltz, M. Brosi, E. Br√ºndermann, B. Haerer, P. Kaiser, C. Pohl, P. Schreiber, M. Yan,T. Asfour, A.-S. M√ºller&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Karlsruhe Insitute of Technology KIT&lt;/p&gt;
&lt;p&gt;&lt;em&gt;10th International Particle Accelerator Conference&lt;/em&gt;&lt;/p&gt;
&lt;h2 id="abstract"&gt;Abstract&lt;/h2&gt;
&lt;p&gt;The operation of ring-based synchrotron light sourceswith short electron bunches increases the emission of co-herent synchrotron radiation (CSR) in the THz frequencyrange. However, the micro-bunching instability resultingfrom self-interaction of the bunch with its own radiationfield limits stable operation with constant intensity of CSRemission to a particular threshold current. Above this thresh-old, the longitudinal charge distribution and thus the emittedradiation vary rapidly and continuously. Therefore, a fastand adaptive feedback system is the appropriate approach tostabilize the dynamics and to overcome the limitations givenby the instability. In this contribution, we discuss first effortstowards a longitudinal feedback design that acts on the RFsystem of the KIT storage ring KARA (Karlsruhe ResearchAccelerator) and aims for stabilization of the emitted THzradiation. Our approach is based on methods of adaptive con-trol that were developed in the field of reinforcement learningand have seen great success in other fields of research overthe past decade. We motivate this particular approach andcomment on different aspects of its implementation.&lt;/p&gt;</description></item><item><title>Using a neural network control policy for rapid switching between beam parameters in an FEL</title><link>https://RL4aa.github.io/posts/publications/edelen2017using/</link><pubDate>Fri, 25 Aug 2017 00:00:00 +0000</pubDate><guid>https://RL4aa.github.io/posts/publications/edelen2017using/</guid><description>&lt;p&gt;&lt;em&gt;&lt;strong&gt;A. L. Edelen&lt;sup&gt;1&lt;/sup&gt;, S. G. Biedron&lt;sup&gt;2&lt;/sup&gt;, J. P. Edelen&lt;sup&gt;3&lt;/sup&gt;, S. V. Milton&lt;sup&gt;4&lt;/sup&gt;, P. J. M. van der Slot&lt;sup&gt;5&lt;/sup&gt;&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;sup&gt;1&lt;/sup&gt;Colorado State University, &lt;sup&gt;2&lt;/sup&gt;Element Aero, &lt;sup&gt;3&lt;/sup&gt;Fermi National Accelerator Laboratory, &lt;sup&gt;4&lt;/sup&gt;Los Alamos National Laboratory, &lt;sup&gt;5&lt;/sup&gt;University of Twente&lt;/p&gt;
&lt;p&gt;&lt;em&gt;38th International Free Electron Laser Conference&lt;/em&gt;&lt;/p&gt;
&lt;h2 id="abstract"&gt;Abstract&lt;/h2&gt;
&lt;p&gt;FEL user facilities often must accommodate requests for a variety of beam parameters. This usually requires skilled operators to tune the machine, reducing the amount of available time for users. In principle, a neural network control policy that is trained on a broad range of operating states could be used to quickly switch between these requests without substantial need for human inter-vention. We present preliminary results from an ongoing study in which a neural network control policy is investi-gated for rapid switching between beam parameters in a compact THz FEL.&lt;/p&gt;</description></item><item><title>Using Neural Network Control Policies For Rapid Switching Between Beam Parameters in a Free Electron Laser</title><link>https://RL4aa.github.io/posts/publications/edelen2017using2/</link><pubDate>Fri, 25 Aug 2017 00:00:00 +0000</pubDate><guid>https://RL4aa.github.io/posts/publications/edelen2017using2/</guid><description>&lt;p&gt;&lt;em&gt;&lt;strong&gt;A. L. Edelen&lt;sup&gt;1&lt;/sup&gt;, S. G. Biedron&lt;sup&gt;2&lt;/sup&gt;, J. P. Edelen&lt;sup&gt;3&lt;/sup&gt;, S. V. Milton&lt;sup&gt;4&lt;/sup&gt;, P. J. M. van der Slot&lt;sup&gt;5&lt;/sup&gt;&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;sup&gt;1&lt;/sup&gt;Colorado State University, &lt;sup&gt;2&lt;/sup&gt;Element Aero, &lt;sup&gt;3&lt;/sup&gt;Fermi National Accelerator Laboratory, &lt;sup&gt;4&lt;/sup&gt;Los Alamos National Laboratory, &lt;sup&gt;5&lt;/sup&gt;University of Twente&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Workshop on Deep Learning for Physical Sciences at the Conference on Neural Information Processing Systems 2017&lt;/em&gt;&lt;/p&gt;
&lt;h2 id="abstract"&gt;Abstract&lt;/h2&gt;
&lt;p&gt;Free Electron Laser (FEL) facilities often must accommodate requests for a varietyof electron beam parameters in order to supply scientific users with appropriatephoton beam characteristics. This usually requires skilled human operators to tunethe machine. In principle, a neural network control policy that is trained on a broadrange of machine operating states could be used to quickly switch between theserequests without substantial need for human intervention. We present preliminaryresults from an ongoing simulation study in which a neural network control policyis investigated for rapid switching between beam parameters in a compact THzFEL that exhibits nonlinear electron beam dynamics. To accomplish this, we firsttrain a feed-forward neural network to mimic a physics-based simulation of theFEL. We then train a neural network control policy by first pre-training it as aninverse model (using supervised learning with a subset of the simulation data) andthen training it more extensively with reinforcement learning. In this case, thereinforcement learning component consists of letting the policy network interactwith the learned system model and backpropagating the cost through the modelnetwork to the controller network.&lt;/p&gt;</description></item><item><title>Orbit Correction Studies Using Neural Networks</title><link>https://RL4aa.github.io/posts/publications/meier2012orbit/</link><pubDate>Sun, 20 May 2012 00:00:00 +0000</pubDate><guid>https://RL4aa.github.io/posts/publications/meier2012orbit/</guid><description>&lt;p&gt;&lt;em&gt;&lt;strong&gt;E. Meier, Y.-R. E. Tan, G. S. LeBlanc&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Australian Synchrotron&lt;/p&gt;
&lt;p&gt;&lt;em&gt;3rd International Particle Accelerator Conference&lt;/em&gt;&lt;/p&gt;
&lt;h2 id="abstract"&gt;Abstract&lt;/h2&gt;
&lt;p&gt;This paper reports the use of neural networks for orbitcorrection at the Australian Synchrotron Storage Ring. Theproposed system uses two neural networks in an actor-criticscheme to model a long term cost function and computeappropriate corrections. The system is entirely based onthe history of the beam position and the actuators, i.e. thecorrector magnets, in the storage ring. This makes the sys-tem auto-tuneable, which has the advantage of avoiding themeasure of a response matrix. The controller will automat-ically maintain an updated BPM corrector response matrix.In future if coupled with some form of orbit response anal-ysis, the system will have the potential to track drifts orchanges to the lattice functions in ‚Äùreal time‚Äù. As a genericand robust orbit correction program it can be used duringcommissioning and in slow orbit feedback. In this study,we present positive initial results of the simulations of thestorage ring in Matlab.&lt;/p&gt;</description></item><item><title>About Us</title><link>https://RL4aa.github.io/aboutus/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://RL4aa.github.io/aboutus/</guid><description>aboutus</description></item></channel></rss>