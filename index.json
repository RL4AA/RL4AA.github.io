[{"content":"Links to the workshop Indico page of the workshop. Github repository for the hands-on tutorial. ","permalink":"https://RL4aa.github.io/posts/rl4aa23/","summary":"Links to the workshop Indico page of the workshop. Github repository for the hands-on tutorial. ","title":"RL4AA'23: 1st Collaboration Workshop on Reinforcement Learning for Autonomous Accelerators"},{"content":"J. Kaiser, O. Stein, A. Eichler. Deutsches Elektronen-Synchrotron DESY. 39th International Conference on Machine Learning.\nAbstract In recent work, it has been shown that reinforcement learning (RL) is capable of solving a variety of problems at sometimes super-human performance levels. But despite continued advances in the field, applying RL to complex real-world control and optimisation problems has proven difficult. In this contribution, we demonstrate how to successfully apply RL to the optimisation of a highly complex real-world machine – specifically a linear particle accelerator – in an only partially observable setting and without requiring training on the real machine. Our method outperforms conventional optimisation algorithms in both the achieved result and time taken as well as already achieving close to human-level performance. We expect that such automation of machine optimisation will push the limits of operability, increase machine availability and lead to a paradigm shift in how such machines are operated, ultimately facilitating advances in a variety of fields, such as science and medicine among many others.\nRead the paper: https://proceedings.mlr.press/v162/kaiser22a.html\nContact: Jan Kaiser\n","permalink":"https://RL4aa.github.io/posts/kaiser2022learningbased/","summary":"J. Kaiser, O. Stein, A. Eichler. Deutsches Elektronen-Synchrotron DESY. 39th International Conference on Machine Learning.\nAbstract In recent work, it has been shown that reinforcement learning (RL) is capable of solving a variety of problems at sometimes super-human performance levels. But despite continued advances in the field, applying RL to complex real-world control and optimisation problems has proven difficult. In this contribution, we demonstrate how to successfully apply RL to the optimisation of a highly complex real-world machine – specifically a linear particle accelerator – in an only partially observable setting and without requiring training on the real machine.","title":"Learning-based Optimisation of Particle Accelerators Under Partial Observability Without Real-World Training"},{"content":"N. Madysa, V. Kain, R. Alemany Fernandez, N. Biancacci, B. Goddard, F. M. Velotti. CERN. 13th Particle Accelerator Conference.\nAbstract High intensities in the Low Energy Ion Ring (LEIR) at CERN are achieved by stacking several multi-turn injec- tions from the pre-accelerator Linac3. Up to seven consec- utive 200 μs long, 200 ms spaced pulses are injected from Linac3 into LEIR. Two inclined septa, one magnetic and one electrostatic, combined with a collapsing horizontal or- bit bump allows a 6-D phase space painting via a linearly ramped mean momentum along the Linac3 pulse and in- jection at high dispersion. The already circulating beam is cooled and dragged longitudinally via electron cooling (e- cooling) into a stacking momentum to free space for the fol- lowing injections. For optimal intensity accumulation, the electron energy and trajectory need to match the ion energy and orbit at the e-cooler section.\nIn this paper, a reinforcement learning (RL) agent is trained to adjust various e-cooler and Linac3 parameters to maximise the intensity at the end of the injection plateau. Variational Auto-Encoders (VAE) are used to compress lon- gitudinal Schottky spectra into a compact latent space rep- resentation as state input for the RL agent. The RL agent is pre-trained on a surrogate model of the LEIR e-cooling dynamics, which in turn is learned from the data collected for the training of the VAE. The performance of the VAE, the surrogate model, and the RL agent is investigated in this paper. An overview of planned tests in the upcoming LEIR runs is given.\nRead the paper: https://jacow.org/ipac2022/papers/tupost040.pdf\nContact: Nico Madysa\n","permalink":"https://RL4aa.github.io/posts/madysa2022automated/","summary":"N. Madysa, V. Kain, R. Alemany Fernandez, N. Biancacci, B. Goddard, F. M. Velotti. CERN. 13th Particle Accelerator Conference.\nAbstract High intensities in the Low Energy Ion Ring (LEIR) at CERN are achieved by stacking several multi-turn injec- tions from the pre-accelerator Linac3. Up to seven consec- utive 200 μs long, 200 ms spaced pulses are injected from Linac3 into LEIR. Two inclined septa, one magnetic and one electrostatic, combined with a collapsing horizontal or- bit bump allows a 6-D phase space painting via a linearly ramped mean momentum along the Linac3 pulse and in- jection at high dispersion.","title":"Automated Intensity Optimisation Using Reinforcement Learning at LEIR"},{"content":"N. Bruchon. University of Trieste. PhD thesis.\nAbstract The research carried out in particle accelerator facilities does not concern only particle and condensed matter physics, although these are the main topics covered in the field. Indeed, since a particle accelerator is composed of many different sub-systems, its proper functioning depends both on each of these parts and their interconnection. It follows that the study, implementation, and improvement of the various sub-systems are fundamental points of investigation too. In particular, an interesting aspect for the automation engineering community is the control of such systems that usually are complex, large, noise-affected, and non-linear.\nThe doctoral project fits into this scope, investigating the introduction of new methods to automatically improve the performance of a specific type of particle accelerators: seeded free-electron lasers. The optimization of such systems is a challenging task, already faced in years by many different approaches in order to find and attain an optimal working point, keeping it optimally tuned despite drift or disturbances. Despite the good results achieved, better ones are always sought for. For this reason, several methods belonging to reinforcement learning, an area of machine learning that is attracting more and more attention in the scientific field, have been applied on FERMI, the free-electron laser facility at Elettra Sincrotrone Trieste. The research activity has been carried out by applying both model-free and model-based techniques belonging to reinforcement learning. Satisfactory preliminary results have been obtained, that present the first step toward a new fully automatic procedure for the alignment of the seed laser to the electron beam.\nIn the meantime, at the Conseil Européen pour la Recherche Nucléaire, CERN, a similar investigation was ongoing. In the last year of the doctoral course, a collaboration to share the knowledge on the topic took place. Some of the results collected on the largest particle physics laboratory in the world are presented in the doctoral dissertation.\nRead the paper: https://arts.units.it/handle/11368/2982117\nContact: Niky Bruchon\n","permalink":"https://RL4aa.github.io/posts/bruchon2021feasiblity/","summary":"N. Bruchon. University of Trieste. PhD thesis.\nAbstract The research carried out in particle accelerator facilities does not concern only particle and condensed matter physics, although these are the main topics covered in the field. Indeed, since a particle accelerator is composed of many different sub-systems, its proper functioning depends both on each of these parts and their interconnection. It follows that the study, implementation, and improvement of the various sub-systems are fundamental points of investigation too.","title":"Feasibility Investigation on Several Reinforcement Learning Techniques to Improve the Performance of the FERMI Free-Electron Laser"},{"content":"X. Pang, S. Thulasidasan, L. Rybarcyk. Apple, Los Alamos National Laboratory. Machine Learning for Engineering Modeling, Simulation, and Design Workshop at Neural Information Processing Systems 2020.\nAbstract We describe an approach to learning optimal control policies for a large, linear particle accelerator using deep reinforcement learning coupled with a high-fidelity physics engine. The framework consists of an AI controller that uses deep neural networks for state and action-space representation and learns optimal policies using reward signals that are provided by the physics simulator. For this work, we only focus on controlling a small section of the entire accelerator. Nevertheless, initial results indicate that we can achieve better-than-human level performance in terms of particle beam current and distribution. The ultimate goal of this line of work is to substantially reduce the tuning time for such facilities by orders of magnitude, and achieve near-autonomous control.\nRead the paper: https://ml4eng.github.io/camera_readys/58.pdf\nContact: Xiaoying Pang\n","permalink":"https://RL4aa.github.io/posts/pang2020autonomous/","summary":"X. Pang, S. Thulasidasan, L. Rybarcyk. Apple, Los Alamos National Laboratory. Machine Learning for Engineering Modeling, Simulation, and Design Workshop at Neural Information Processing Systems 2020.\nAbstract We describe an approach to learning optimal control policies for a large, linear particle accelerator using deep reinforcement learning coupled with a high-fidelity physics engine. The framework consists of an AI controller that uses deep neural networks for state and action-space representation and learns optimal policies using reward signals that are provided by the physics simulator.","title":"Autonomous Control of a Particle Accelerator using Deep Reinforcement Learning"},{"content":"V. Kain, S. Hirlander, B. Goddard, F. M. Velotti, G. Z. Della Porta, N. Bruchon, G. Valentino. CERN, University of Trieste, University of Malta. Physical Review Accelerators and Beams.\nAbstract Numerical optimization algorithms are already established tools to increase and stabilize the performance of particle accelerators. These algorithms have many advantages, are available out of the box, and can be adapted to a wide range of optimization problems in accelerator operation. The next boost in efficiency is expected to come from reinforcement learning algorithms that learn the optimal policy for a certain control problem and hence, once trained, can do without the time-consuming exploration phase needed for numerical optimizers. To investigate this approach, continuous model-free reinforcement learning with up to 16 degrees of freedom was developed and successfully tested at various facilities at CERN. The approach and algorithms used are discussed and the results obtained for trajectory steering at the AWAKE electron line and LINAC4 are presented. The necessary next steps, such as uncertainty aware model-based approaches, and the potential for future applications at particle accelerators are addressed.\nRead the paper: https://doi.org/10.1103/PhysRevAccelBeams.23.124801\nContact: Verena Kain\n","permalink":"https://RL4aa.github.io/posts/kain2020sampleefficient/","summary":"V. Kain, S. Hirlander, B. Goddard, F. M. Velotti, G. Z. Della Porta, N. Bruchon, G. Valentino. CERN, University of Trieste, University of Malta. Physical Review Accelerators and Beams.\nAbstract Numerical optimization algorithms are already established tools to increase and stabilize the performance of particle accelerators. These algorithms have many advantages, are available out of the box, and can be adapted to a wide range of optimization problems in accelerator operation.","title":"Sample-efficient reinforcement learning for CERN accelerator control"},{"content":"N. Bruchon, G. Fenu, G. Gaio, M. Lonza, F. H. O’Shea, F. A. Pellegrino, E. Salvato. University of Trieste, Elettra Sincrotrone Trieste. Electronics.\nAbstract Optimal tuning of particle accelerators is a challenging task. Many different approaches have been proposed in the past to solve two main problems—attainment of an optimal working point and performance recovery after machine drifts. The most classical model-free techniques (e.g., Gradient Ascent or Extremum Seeking algorithms) have some intrinsic limitations. To overcome those limitations, Machine Learning tools, in particular Reinforcement Learning (RL), are attracting more and more attention in the particle accelerator community. We investigate the feasibility of RL model-free approaches to align the seed laser, as well as other service lasers, at FERMI, the free-electron laser facility at Elettra Sincrotrone Trieste. We apply two different techniques—the first, based on the episodic Q-learning with linear function approximation, for performance optimization; the second, based on the continuous Natural Policy Gradient REINFORCE algorithm, for performance recovery. Despite the simplicity of these approaches, we report satisfactory preliminary results, that represent the first step toward a new fully automatic procedure for the alignment of the seed laser to the electron beam. Such an alignment is, at present, performed manually.\nRead the paper: https://www.mdpi.com/2079-9292/9/5/781\nContact: Niky Bruchon\n","permalink":"https://RL4aa.github.io/posts/bruchon2020basic/","summary":"N. Bruchon, G. Fenu, G. Gaio, M. Lonza, F. H. O’Shea, F. A. Pellegrino, E. Salvato. University of Trieste, Elettra Sincrotrone Trieste. Electronics.\nAbstract Optimal tuning of particle accelerators is a challenging task. Many different approaches have been proposed in the past to solve two main problems—attainment of an optimal working point and performance recovery after machine drifts. The most classical model-free techniques (e.g., Gradient Ascent or Extremum Seeking algorithms) have some intrinsic limitations.","title":"Basic Reinforcement Learning Techniques to Control the Intensity of a Seeded Free-Electron Laser"},{"content":"N. Bruchon, G. Fenu, G. Gaio, M. Lonza, F. A. Pellegrino, E. Salvato. University of Trieste. 23rd International Conference on Mechatronics Technology.\nAbstract The optimization of particle accelerators is a challenging task, and many different approaches have been proposed in years, to obtain an optimal tuning of the plant and to keep it optimally tuned despite drifts or disturbances. Indeed, the classical model-free approaches (such as Gradient Ascent or Extremum Seeking algorithms) have intrinsic limitations. To overcome those limitations, Machine Learning techniques, in particular, the Reinforcement Learning, are attracting more and more attention in the particle accelerator community. The purpose of this paper is to apply a Reinforcement Learning model-free approach to the alignment of a seed laser, based on a rather general target function depending on the laser trajectory. The study focuses on the alignment of the lasers at FERMI, the free-electron laser facility at Elettra Sincrotrone Trieste. In particular, we employ Q-learning with linear function approximation and report experimental results obtained in two setups, which are the actual setups where the final application has to be deployed. Despite the simplicity of the approach, we report satisfactory preliminary results, that represent the first step toward a fully automatic procedure for seed laser to the electron beam. Such a superimposition is, at present, performed manually.\nRead the paper: https://ieeexplore.ieee.org/document/8932150\nContact: Niky Bruchon\n","permalink":"https://RL4aa.github.io/posts/bruchon2019toward/","summary":"N. Bruchon, G. Fenu, G. Gaio, M. Lonza, F. A. Pellegrino, E. Salvato. University of Trieste. 23rd International Conference on Mechatronics Technology.\nAbstract The optimization of particle accelerators is a challenging task, and many different approaches have been proposed in years, to obtain an optimal tuning of the plant and to keep it optimally tuned despite drifts or disturbances. Indeed, the classical model-free approaches (such as Gradient Ascent or Extremum Seeking algorithms) have intrinsic limitations.","title":"Toward the Application of Reinforcement Learning to the Intensity Control of a Seeded Free-Electron Laser"}]