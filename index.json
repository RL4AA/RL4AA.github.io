[{"content":"Links to the workshop Indico page of the workshop. Github repository for the hands-on tutorial. ","permalink":"https://RL4aa.github.io/posts/rl4aa23/","summary":"Links to the workshop Indico page of the workshop. Github repository for the hands-on tutorial. ","title":"RL4AA'23: 1st Collaboration Workshop on Reinforcement Learning for Autonomous Accelerators"},{"content":"Jan Kaiser, Oliver Stein, Annika Eichler. Deutsches Elektronen-Synchrotron DESY. 39th International Conference on Machine Learning.\nAbstract In recent work, it has been shown that reinforcement learning (RL) is capable of solving a variety of problems at sometimes super-human performance levels. But despite continued advances in the field, applying RL to complex real-world control and optimisation problems has proven difficult. In this contribution, we demonstrate how to successfully apply RL to the optimisation of a highly complex real-world machine – specifically a linear particle accelerator – in an only partially observable setting and without requiring training on the real machine. Our method outperforms conventional optimisation algorithms in both the achieved result and time taken as well as already achieving close to human-level performance. We expect that such automation of machine optimisation will push the limits of operability, increase machine availability and lead to a paradigm shift in how such machines are operated, ultimately facilitating advances in a variety of fields, such as science and medicine among many others.\nRead the paper: https://proceedings.mlr.press/v162/kaiser22a.html\nContact: Jan Kaiser\n","permalink":"https://RL4aa.github.io/posts/kaiser2022learningbased/","summary":"Jan Kaiser, Oliver Stein, Annika Eichler. Deutsches Elektronen-Synchrotron DESY. 39th International Conference on Machine Learning.\nAbstract In recent work, it has been shown that reinforcement learning (RL) is capable of solving a variety of problems at sometimes super-human performance levels. But despite continued advances in the field, applying RL to complex real-world control and optimisation problems has proven difficult. In this contribution, we demonstrate how to successfully apply RL to the optimisation of a highly complex real-world machine – specifically a linear particle accelerator – in an only partially observable setting and without requiring training on the real machine.","title":"Learning-based Optimisation of Particle Accelerators Under Partial Observability Without Real-World Training"},{"content":"Niky Bruchon. University of Trieste. PhD thesis.\nAbstract The research carried out in particle accelerator facilities does not concern only particle and condensed matter physics, although these are the main topics covered in the field. Indeed, since a particle accelerator is composed of many different sub-systems, its proper functioning depends both on each of these parts and their interconnection. It follows that the study, implementation, and improvement of the various sub-systems are fundamental points of investigation too. In particular, an interesting aspect for the automation engineering community is the control of such systems that usually are complex, large, noise-affected, and non-linear.\nThe doctoral project fits into this scope, investigating the introduction of new methods to automatically improve the performance of a specific type of particle accelerators: seeded free-electron lasers. The optimization of such systems is a challenging task, already faced in years by many different approaches in order to find and attain an optimal working point, keeping it optimally tuned despite drift or disturbances. Despite the good results achieved, better ones are always sought for. For this reason, several methods belonging to reinforcement learning, an area of machine learning that is attracting more and more attention in the scientific field, have been applied on FERMI, the free-electron laser facility at Elettra Sincrotrone Trieste. The research activity has been carried out by applying both model-free and model-based techniques belonging to reinforcement learning. Satisfactory preliminary results have been obtained, that present the first step toward a new fully automatic procedure for the alignment of the seed laser to the electron beam.\nIn the meantime, at the Conseil Européen pour la Recherche Nucléaire, CERN, a similar investigation was ongoing. In the last year of the doctoral course, a collaboration to share the knowledge on the topic took place. Some of the results collected on the largest particle physics laboratory in the world are presented in the doctoral dissertation.\nRead the paper: https://arts.units.it/handle/11368/2982117\nContact: Niky Bruchon\n","permalink":"https://RL4aa.github.io/posts/bruchon2021feasiblity/","summary":"Niky Bruchon. University of Trieste. PhD thesis.\nAbstract The research carried out in particle accelerator facilities does not concern only particle and condensed matter physics, although these are the main topics covered in the field. Indeed, since a particle accelerator is composed of many different sub-systems, its proper functioning depends both on each of these parts and their interconnection. It follows that the study, implementation, and improvement of the various sub-systems are fundamental points of investigation too.","title":"Feasibility Investigation on Several Reinforcement Learning Techniques to Improve the Performance of the FERMI Free-Electron Laser"},{"content":"Xiaoying Pang, Sunil Thulasidasan, Larry Rybarcyk. Apple, Los Alamos National Laboratory. Machine Learning for Engineering Modeling, Simulation, and Design Workshop at Neural Information Processing Systems 2020.\nAbstract We describe an approach to learning optimal control policies for a large, linear particle accelerator using deep reinforcement learning coupled with a high-fidelity physics engine. The framework consists of an AI controller that uses deep neural networks for state and action-space representation and learns optimal policies using reward signals that are provided by the physics simulator. For this work, we only focus on controlling a small section of the entire accelerator. Nevertheless, initial results indicate that we can achieve better-than-human level performance in terms of particle beam current and distribution. The ultimate goal of this line of work is to substantially reduce the tuning time for such facilities by orders of magnitude, and achieve near-autonomous control.\nRead the paper: https://ml4eng.github.io/camera_readys/58.pdf\nContact: Xiaoying Pang\n","permalink":"https://RL4aa.github.io/posts/pang2020autonomous/","summary":"Xiaoying Pang, Sunil Thulasidasan, Larry Rybarcyk. Apple, Los Alamos National Laboratory. Machine Learning for Engineering Modeling, Simulation, and Design Workshop at Neural Information Processing Systems 2020.\nAbstract We describe an approach to learning optimal control policies for a large, linear particle accelerator using deep reinforcement learning coupled with a high-fidelity physics engine. The framework consists of an AI controller that uses deep neural networks for state and action-space representation and learns optimal policies using reward signals that are provided by the physics simulator.","title":"Autonomous Control of a Particle Accelerator using Deep Reinforcement Learning"},{"content":"Verena Kain, Simon Hirlander, Brennan Goddard, Francesco Maria Velotti, Giovanni Zevi Della Porta, Niky Bruchon, and Gianluca Valentino. CERN, University of Trieste, University of Malta. Physical Review Accelerators and Beams.\nAbstract Numerical optimization algorithms are already established tools to increase and stabilize the performance of particle accelerators. These algorithms have many advantages, are available out of the box, and can be adapted to a wide range of optimization problems in accelerator operation. The next boost in efficiency is expected to come from reinforcement learning algorithms that learn the optimal policy for a certain control problem and hence, once trained, can do without the time-consuming exploration phase needed for numerical optimizers. To investigate this approach, continuous model-free reinforcement learning with up to 16 degrees of freedom was developed and successfully tested at various facilities at CERN. The approach and algorithms used are discussed and the results obtained for trajectory steering at the AWAKE electron line and LINAC4 are presented. The necessary next steps, such as uncertainty aware model-based approaches, and the potential for future applications at particle accelerators are addressed.\nRead the paper: https://doi.org/10.1103/PhysRevAccelBeams.23.124801\nContact: Verena Kain\n","permalink":"https://RL4aa.github.io/posts/kain2020sampleefficient/","summary":"Verena Kain, Simon Hirlander, Brennan Goddard, Francesco Maria Velotti, Giovanni Zevi Della Porta, Niky Bruchon, and Gianluca Valentino. CERN, University of Trieste, University of Malta. Physical Review Accelerators and Beams.\nAbstract Numerical optimization algorithms are already established tools to increase and stabilize the performance of particle accelerators. These algorithms have many advantages, are available out of the box, and can be adapted to a wide range of optimization problems in accelerator operation.","title":"Sample-efficient reinforcement learning for CERN accelerator control"},{"content":"Niky Bruchon, Gianfranco Fenu, Giulio Gaio, Marco Lonza, Felice Andrea Pellegrino, Erica Salvato. University of Trieste. 23rd International Conference on Mechatronics Technology.\nAbstract The optimization of particle accelerators is a challenging task, and many different approaches have been proposed in years, to obtain an optimal tuning of the plant and to keep it optimally tuned despite drifts or disturbances. Indeed, the classical model-free approaches (such as Gradient Ascent or Extremum Seeking algorithms) have intrinsic limitations. To overcome those limitations, Machine Learning techniques, in particular, the Reinforcement Learning, are attracting more and more attention in the particle accelerator community. The purpose of this paper is to apply a Reinforcement Learning model-free approach to the alignment of a seed laser, based on a rather general target function depending on the laser trajectory. The study focuses on the alignment of the lasers at FERMI, the free-electron laser facility at Elettra Sincrotrone Trieste. In particular, we employ Q-learning with linear function approximation and report experimental results obtained in two setups, which are the actual setups where the final application has to be deployed. Despite the simplicity of the approach, we report satisfactory preliminary results, that represent the first step toward a fully automatic procedure for seed laser to the electron beam. Such a superimposition is, at present, performed manually.\nRead the paper: https://ieeexplore.ieee.org/document/8932150\nContact: Niky Bruchon\n","permalink":"https://RL4aa.github.io/posts/bruchon2019toward/","summary":"Niky Bruchon, Gianfranco Fenu, Giulio Gaio, Marco Lonza, Felice Andrea Pellegrino, Erica Salvato. University of Trieste. 23rd International Conference on Mechatronics Technology.\nAbstract The optimization of particle accelerators is a challenging task, and many different approaches have been proposed in years, to obtain an optimal tuning of the plant and to keep it optimally tuned despite drifts or disturbances. Indeed, the classical model-free approaches (such as Gradient Ascent or Extremum Seeking algorithms) have intrinsic limitations.","title":"Toward the Application of Reinforcement Learning to the Intensity Control of a Seeded Free-Electron Laser"}]