[{"content":"Good News - save the date! Our first workshop, RL4AA 2023, was very successful. Because of this we are hoping to hold the 2nd RL4AA workshop in spring 2024 in Salzburg, Austria in 05.02. - 07.02.2024. Further details will follow soon.\n","permalink":"https://RL4aa.github.io/posts/announcements/rl4aa24_stay_tuned/","summary":"Good News - save the date! Our first workshop, RL4AA 2023, was very successful. Because of this we are hoping to hold the 2nd RL4AA workshop in spring 2024 in Salzburg, Austria in 05.02. - 07.02.2024. Further details will follow soon.","title":"üìç The 2nd RL4AA workshop 2024 will be held 05.-07. February 2024 in Salzburg"},{"content":"J. Kaiser1, C. Xu2, A. Eichler1, A. Santamaria Garcia2, O. Stein1, E. Br√ºndermann2, W. Kuropka1, H. Dinter1, F. Mayet1, T. Vinatier1, F. Burkart1, H. Schlarb1\n1Deutsches Elektronen-Synchrotron DESY, 2 Karlsruhe Institute of Technology KIT\narXiv\nAbstract Online tuning of real-world plants is a complex optimisation problem that continues to require manual intervention by experienced human operators. Autonomous tuning is a rapidly expanding field of research, where learning-based methods, such as Reinforcement Learning-trained Optimisation (RLO) and Bayesian optimisation (BO), hold great promise for achieving outstanding plant performance and reducing tuning times. Which algorithm to choose in different scenarios, however, remains an open question. Here we present a comparative study using a routine task in a real particle accelerator as an example, showing that RLO generally outperforms BO, but is not always the best choice. Based on the study\u0026rsquo;s results, we provide a clear set of criteria to guide the choice of algorithm for a given tuning task. These can ease the adoption of learning-based autonomous tuning solutions to the operation of complex real-world plants, ultimately improving the availability and pushing the limits of operability of these facilities, thereby enabling scientific and engineering advancements.\nRead the paper: https://arxiv.org/abs/2306.03739\nContact: Jan Kaiser Chenran Xu\n","permalink":"https://RL4aa.github.io/posts/publications/kaiser2023learning/","summary":"J. Kaiser1, C. Xu2, A. Eichler1, A. Santamaria Garcia2, O. Stein1, E. Br√ºndermann2, W. Kuropka1, H. Dinter1, F. Mayet1, T. Vinatier1, F. Burkart1, H. Schlarb1\n1Deutsches Elektronen-Synchrotron DESY, 2 Karlsruhe Institute of Technology KIT\narXiv\nAbstract Online tuning of real-world plants is a complex optimisation problem that continues to require manual intervention by experienced human operators. Autonomous tuning is a rapidly expanding field of research, where learning-based methods, such as Reinforcement Learning-trained Optimisation (RLO) and Bayesian optimisation (BO), hold great promise for achieving outstanding plant performance and reducing tuning times.","title":"Learning to Do or Learning While Doing: Reinforcement Learning and Bayesian Optimisation for Online Continuous Tuning"},{"content":"Good News! The RL4AA community is happy to announce its Discord server! If you are interested in discussing reinforcement learning applied to accelerators, please join for announcements (e.g. new publications), forum discussions, an open chat, and meeting rooms.\nHope to see you there!\nhttps://discord.gg/QtBMqsjWH2\n","permalink":"https://RL4aa.github.io/posts/announcements/discord_server/","summary":"Good News! The RL4AA community is happy to announce its Discord server! If you are interested in discussing reinforcement learning applied to accelerators, please join for announcements (e.g. new publications), forum discussions, an open chat, and meeting rooms.\nHope to see you there!\nhttps://discord.gg/QtBMqsjWH2","title":"The RL4AA Discord server is up!"},{"content":"X. Chen, X. Qi, C. Su, Y. He, Z. Wang, K. Sun, C. Jin, W. Chen, S. Liu, X. Zhao, D. Jia, M. Yi\nChinese Academy of Sciences\narXiv\nAbstract The superconducting linear accelerator is a highly flexiable facility for modern scientific discoveries, necessitating weekly reconfiguration and tuning. Accordingly, minimizing setup time proves essential in affording users with ample experimental time. We propose a trend-based soft actor-critic(TBSAC) beam control method with strong robustness, allowing the agents to be trained in a simulated environment and applied to the real accelerator directly with zero-shot. To validate the effectiveness of our method, two different typical beam control tasks were performed on China Accelerator Facility for Superheavy Elements (CAFe II) and a light particle injector(LPI) respectively. The orbit correction tasks were performed in three cryomodules in CAFe II seperately, the time required for tuning has been reduced to one-tenth of that needed by human experts, and the RMS values of the corrected orbit were all less than 1mm. The other transmission efficiency optimization task was conducted in the LPI, our agent successfully optimized the transmission efficiency of radio-frequency quadrupole(RFQ) to over 85% within 2 minutes. The outcomes of these two experiments offer substantiation that our proposed TBSAC approach can efficiently and effectively accomplish beam commissioning tasks while upholding the same standard as skilled human experts. As such, our method exhibits potential for future applications in other accelerator commissioning fields.\nRead the paper: https://arxiv.org/abs/2305.13869\nContact: Xin Qi, Zhijun Wang\n","permalink":"https://RL4aa.github.io/posts/publications/chen2023trendbased/","summary":"X. Chen, X. Qi, C. Su, Y. He, Z. Wang, K. Sun, C. Jin, W. Chen, S. Liu, X. Zhao, D. Jia, M. Yi\nChinese Academy of Sciences\narXiv\nAbstract The superconducting linear accelerator is a highly flexiable facility for modern scientific discoveries, necessitating weekly reconfiguration and tuning. Accordingly, minimizing setup time proves essential in affording users with ample experimental time. We propose a trend-based soft actor-critic(TBSAC) beam control method with strong robustness, allowing the agents to be trained in a simulated environment and applied to the real accelerator directly with zero-shot.","title":"Trend-Based SAC Beam Control Method with Zero-Shot in Superconducting Linear Accelerator"},{"content":"** Simon Hirlaender, Lukas Lamminger, Giovanni Zevi Della Porta, Verena Kain**\nAbstract Reinforcement learning (RL) is a promising direction in machine learning for the control and optimisation of particle accelerators since it learns directly from experience without needing a model a-priori. However, RL generally suffers from low sample efficiency and thus training from scracth on the machine is often not an option. RL agents are usually trained or pre-tuned on simulators and then transferred to the real environment. In this work we propose a model-based RL approach based on Gaussian processes (GPs) to overcome the sample efficiency limitation. Our RL agent was able to learn to control the trajectory at the CERN AWAKE (Advanced Wakefield Experiment) facility, a problem of 10 degrees of freedom, within a few interactions only. To date, numerical optimises are used to restore or increase and stabilise the performance of accelerators. A major drawback is that they must explore the optimisation space each time they are applied. Our RL approach learns as quickly as numerical optimisers for one optimisation run, but can be used afterwards as single-shot or few-shot controllers. Furthermore, it can also handle safety and time-varying systems and can be used for the online stabilisation of accelerator operation.This approach opens a new avenue for the application of RL in accelerator control and brings it into the realm of everyday applications.\nRead the paper: [https://www.ipac23.org/preproc/doi/jacow-ipac2023-thpl038/index.html]\nContact: Simon Hirlaender\n","permalink":"https://RL4aa.github.io/posts/publications/ultra_fast_rl_awake/","summary":"** Simon Hirlaender, Lukas Lamminger, Giovanni Zevi Della Porta, Verena Kain**\nAbstract Reinforcement learning (RL) is a promising direction in machine learning for the control and optimisation of particle accelerators since it learns directly from experience without needing a model a-priori. However, RL generally suffers from low sample efficiency and thus training from scracth on the machine is often not an option. RL agents are usually trained or pre-tuned on simulators and then transferred to the real environment.","title":"Ultra fast reinforcement learning demonstrated at CERN AWAKE "},{"content":"F. M. Velotti1, B. Goddard1, V. Kain1, R. Ramjiawan1, G. Z. Della Porta1 and S. Hirlaender2\n1CERN, 2University of Salzburg\nMachine Learning: Science and Technology\nAbstract To improve the performance-critical stability and brightness of the electron bunch at injection into the proton-driven plasma wakefield at the AWAKE CERN experiment, automation approaches based on unsupervised machine learning (ML) were developed and deployed. Numerical optimisers were tested together with different model-free reinforcement learning (RL) agents. In order to avoid any bias, RL agents have been trained also using a completely unsupervised state encoding using auto-encoders. To aid hyper-parameter selection, a full synthetic model of the beamline was constructed using a variational auto-encoder trained to generate surrogate data from equipment settings. This paper describes the novel approaches based on deep learning and RL to aid the automatic setup of a low energy line, as the one used to deliver beam to the AWAKE facility. The results obtained with the different ML approaches, including automatic unsupervised feature extraction from images using computer vision are presented. The prospects for operational deployment and wider applicability are discussed.\nRead the paper: https://iopscience.iop.org/article/10.1088/2632-2153/acce21\nContact: Francesco Maria Velotti\n","permalink":"https://RL4aa.github.io/posts/publications/velotti2023towards/","summary":"F. M. Velotti1, B. Goddard1, V. Kain1, R. Ramjiawan1, G. Z. Della Porta1 and S. Hirlaender2\n1CERN, 2University of Salzburg\nMachine Learning: Science and Technology\nAbstract To improve the performance-critical stability and brightness of the electron bunch at injection into the proton-driven plasma wakefield at the AWAKE CERN experiment, automation approaches based on unsupervised machine learning (ML) were developed and deployed. Numerical optimisers were tested together with different model-free reinforcement learning (RL) agents.","title":"Towards automatic setup of 18 MeV electron beamline using machine learning"},{"content":"X. Chen, Y. Jia, X. Qi, Z. Wang, Y. He\nChinese Academy of Sciences\nPhysical Review Accelerators and Beams\nAbstract Recently, reinforcement learning (RL) algorithms have been applied to a wide range of control problems in accelerator commissioning. In order to achieve efficient and fast control, these algorithms need to be highly efficient, so as to minimize the online training time. In this paper, we incorporated the beam position monitor trend into the observation space of the twin delayed deep deterministic policy gradient (TD3) algorithm and trained two different structure agents, one based on physical prior knowledge and the other using the original TD3 network architecture. Both of the agents exhibit strong robustness in the simulated environment. The effectiveness of the agent based on physical prior knowledge has been validated in a real accelerator. Results show that the agent can overcome the difference between simulated and real accelerator environments. Once the training is completed in the simulated environment, the agent can be directly applied to the real accelerator without any online training process. The RL agent is deployed to the medium energy beam transport section of China Accelerator Facility for Superheavy Elements. Fast and automatic orbit correction is being tested with up to ten degrees of freedom. The experimental results show that the agents can correct the orbit to within 1 mm. Moreover, due to the strong robustness of the agent, when a trained agent is applied to different lattices of different particles, the orbit correction can still be completed. Since there are no online data collection and training processes, all online corrections are done within 30 s. This paper shows that, as long as the robustness of the RL algorithm is sufficient, the offline learning agents can be directly applied to online correction, which will greatly improve the efficiency of orbit correction. Such an approach to RL may find promising applications in other areas of accelerator commissioning.\nRead the paper: https://journals.aps.org/prab/abstract/10.1103/PhysRevAccelBeams.26.044601\nContact: Xin Qi, Zhijun Wang\n","permalink":"https://RL4aa.github.io/posts/publications/chen2023orbit/","summary":"X. Chen, Y. Jia, X. Qi, Z. Wang, Y. He\nChinese Academy of Sciences\nPhysical Review Accelerators and Beams\nAbstract Recently, reinforcement learning (RL) algorithms have been applied to a wide range of control problems in accelerator commissioning. In order to achieve efficient and fast control, these algorithms need to be highly efficient, so as to minimize the online training time. In this paper, we incorporated the beam position monitor trend into the observation space of the twin delayed deep deterministic policy gradient (TD3) algorithm and trained two different structure agents, one based on physical prior knowledge and the other using the original TD3 network architecture.","title":"Orbit Correction Based on Improved Reinforcement Learning Algorithm"},{"content":"Reinforcement learning is the most difficult learning paradigms to understand and to efficiently use, but it holds a lot of promise in the field of accelerator physics. The applications of reinforcement learning to accelerators today are not very numerous yet, but the interest of the community is growing considerably. This is how the 1st collaboration workshop on Reinforcement Learning for Autonomous Accelerators (RL4AA'23) came to be! The AI4Accelerators team organized and hosted the workshop at KIT, gathering colleagues involved in reinforcement learning. The workshop offered introductory lectures to reinforcement learning, a Python tutorial that studied the real deployment of such an algorithm in a real accelerator, and guided discussion sessions on the most pressing topics. The contents of the discussion will be published in the form of proceedings later.\nMost importantly, this the RL4AA‚Äô23 event was the official launch of the Reinforcement Learning for Autonomous Accelerators collaboration (RL4AA), which aims to put researchers in contact, create and share valuable resources, and be the spark for interesting ideas and projects together.\nLinks to the workshop Indico page of the workshop. Github repository for the hands-on tutorial. Some event photos ","permalink":"https://RL4aa.github.io/posts/announcements/rl4aa23/","summary":"Reinforcement learning is the most difficult learning paradigms to understand and to efficiently use, but it holds a lot of promise in the field of accelerator physics. The applications of reinforcement learning to accelerators today are not very numerous yet, but the interest of the community is growing considerably. This is how the 1st collaboration workshop on Reinforcement Learning for Autonomous Accelerators (RL4AA'23) came to be! The AI4Accelerators team organized and hosted the workshop at KIT, gathering colleagues involved in reinforcement learning.","title":"RL4AA'23: 1st Collaboration Workshop on Reinforcement Learning for Autonomous Accelerators"},{"content":"D. Meier1, L. V. Ramirez1, J. V√∂lker1, J. Viefhaus1, B. Sick2, G. Hartmann1\n1Helmholtz-Zentrum Berlin, 2University of Kassel\nPhysical Review Accelerators and Beams\nAbstract Superconducting photoelectron injectors are promising for generating highly brilliant pulsed electron beams with high repetition rates and low emittances. Experiments such as ultrafast electron diffraction, experiments at the Terahertz scale, and energy recovery linac applications require such properties. However, optimizing the beam properties is challenging due to the high number of possible machine parameter combinations. This article shows the successful automated optimization of beam properties utilizing an already existing simulation model. To reduce the required computation time, we replace the costly simulation with a faster approximation with a neural network. For optimization, we propose a reinforcement learning approach leveraging the simple computation of the derivative of the approximation. We prove that our approach outperforms standard optimization methods for the required function evaluations given a defined minimum accuracy.\nRead the paper: https://doi.org/10.1103/PhysRevAccelBeams.25.104604\nContact: -\n","permalink":"https://RL4aa.github.io/posts/publications/meier2022optimizing/","summary":"D. Meier1, L. V. Ramirez1, J. V√∂lker1, J. Viefhaus1, B. Sick2, G. Hartmann1\n1Helmholtz-Zentrum Berlin, 2University of Kassel\nPhysical Review Accelerators and Beams\nAbstract Superconducting photoelectron injectors are promising for generating highly brilliant pulsed electron beams with high repetition rates and low emittances. Experiments such as ultrafast electron diffraction, experiments at the Terahertz scale, and energy recovery linac applications require such properties. However, optimizing the beam properties is challenging due to the high number of possible machine parameter combinations.","title":"Optimizing a superconducting radio-frequency gun using deep reinforcement learning"},{"content":"L. Grech1, G. Valentino1, D. Alves2 and Simon Hirlaender3\n1University of Malta, 2CERN, 3University of Salzburg\nFrontiers in Physics\nAbstract The Beam-Based Feedback System (BBFS) was primarily responsible for correcting the beam energy, orbit and tune in the CERN Large Hadron Collider (LHC). A major code renovation of the BBFS was planned and carried out during the LHC Long Shutdown 2 (LS2). This work consists of an explorative study to solve a beam-based control problem, the tune feedback (QFB), utilising state-of-the-art Reinforcement Learning (RL). A simulation environment was created to mimic the operation of the QFB. A series of RL agents were trained, and the best-performing agents were then subjected to a set of well-designed tests. The original feedback controller used in the QFB was reimplemented to compare the performance of the classical approach to the performance of selected RL agents in the test scenarios. Results from the simulated environment show that the RL agent performance can exceed the controller-based paradigm.\nRead the paper: https://www.frontiersin.org/articles/10.3389/fphy.2022.929064/full\nContact: Leander Grech\n","permalink":"https://RL4aa.github.io/posts/publications/grech2022application/","summary":"L. Grech1, G. Valentino1, D. Alves2 and Simon Hirlaender3\n1University of Malta, 2CERN, 3University of Salzburg\nFrontiers in Physics\nAbstract The Beam-Based Feedback System (BBFS) was primarily responsible for correcting the beam energy, orbit and tune in the CERN Large Hadron Collider (LHC). A major code renovation of the BBFS was planned and carried out during the LHC Long Shutdown 2 (LS2). This work consists of an explorative study to solve a beam-based control problem, the tune feedback (QFB), utilising state-of-the-art Reinforcement Learning (RL).","title":"Application of reinforcement learning in the LHC tune feedback"},{"content":"J. Kaiser, O. Stein, A. Eichler\nDeutsches Elektronen-Synchrotron DESY\n39th International Conference on Machine Learning\nAbstract In recent work, it has been shown that reinforcement learning (RL) is capable of solving a variety of problems at sometimes super-human performance levels. But despite continued advances in the field, applying RL to complex real-world control and optimisation problems has proven difficult. In this contribution, we demonstrate how to successfully apply RL to the optimisation of a highly complex real-world machine ‚Äì specifically a linear particle accelerator ‚Äì in an only partially observable setting and without requiring training on the real machine. Our method outperforms conventional optimisation algorithms in both the achieved result and time taken as well as already achieving close to human-level performance. We expect that such automation of machine optimisation will push the limits of operability, increase machine availability and lead to a paradigm shift in how such machines are operated, ultimately facilitating advances in a variety of fields, such as science and medicine among many others.\nRead the paper: https://proceedings.mlr.press/v162/kaiser22a.html\nContact: Jan Kaiser\n","permalink":"https://RL4aa.github.io/posts/publications/kaiser2022learningbased/","summary":"J. Kaiser, O. Stein, A. Eichler\nDeutsches Elektronen-Synchrotron DESY\n39th International Conference on Machine Learning\nAbstract In recent work, it has been shown that reinforcement learning (RL) is capable of solving a variety of problems at sometimes super-human performance levels. But despite continued advances in the field, applying RL to complex real-world control and optimisation problems has proven difficult. In this contribution, we demonstrate how to successfully apply RL to the optimisation of a highly complex real-world machine ‚Äì specifically a linear particle accelerator ‚Äì in an only partially observable setting and without requiring training on the real machine.","title":"Learning-based Optimisation of Particle Accelerators Under Partial Observability Without Real-World Training"},{"content":"N. Madysa, V. Kain, R. Alemany Fernandez, N. Biancacci, B. Goddard, F. M. Velotti\nCERN\n13th Particle Accelerator Conference\nAbstract High intensities in the Low Energy Ion Ring (LEIR) at CERN are achieved by stacking several multi-turn injec- tions from the pre-accelerator Linac3. Up to seven consec- utive 200 Œºs long, 200 ms spaced pulses are injected from Linac3 into LEIR. Two inclined septa, one magnetic and one electrostatic, combined with a collapsing horizontal or- bit bump allows a 6-D phase space painting via a linearly ramped mean momentum along the Linac3 pulse and in- jection at high dispersion. The already circulating beam is cooled and dragged longitudinally via electron cooling (e- cooling) into a stacking momentum to free space for the fol- lowing injections. For optimal intensity accumulation, the electron energy and trajectory need to match the ion energy and orbit at the e-cooler section.\nIn this paper, a reinforcement learning (RL) agent is trained to adjust various e-cooler and Linac3 parameters to maximise the intensity at the end of the injection plateau. Variational Auto-Encoders (VAE) are used to compress lon- gitudinal Schottky spectra into a compact latent space rep- resentation as state input for the RL agent. The RL agent is pre-trained on a surrogate model of the LEIR e-cooling dynamics, which in turn is learned from the data collected for the training of the VAE. The performance of the VAE, the surrogate model, and the RL agent is investigated in this paper. An overview of planned tests in the upcoming LEIR runs is given.\nRead the paper: https://jacow.org/ipac2022/papers/tupost040.pdf\nContact: Nico Madysa\n","permalink":"https://RL4aa.github.io/posts/publications/madysa2022automated/","summary":"N. Madysa, V. Kain, R. Alemany Fernandez, N. Biancacci, B. Goddard, F. M. Velotti\nCERN\n13th Particle Accelerator Conference\nAbstract High intensities in the Low Energy Ion Ring (LEIR) at CERN are achieved by stacking several multi-turn injec- tions from the pre-accelerator Linac3. Up to seven consec- utive 200 Œºs long, 200 ms spaced pulses are injected from Linac3 into LEIR. Two inclined septa, one magnetic and one electrostatic, combined with a collapsing horizontal or- bit bump allows a 6-D phase space painting via a linearly ramped mean momentum along the Linac3 pulse and in- jection at high dispersion.","title":"Automated Intensity Optimisation Using Reinforcement Learning at LEIR"},{"content":"T. Boltz\nKarlsruhe Insitute of Technology KIT\nPhD thesis\nAbstract At the time this thesis is written, the world finds itself amidst and partly in the process of recovering from the COVID-19 pandemic caused by the SARS-Cov-2 virus. One major contribution to the worldwide efforts of bringing this pandemic to an end are the vaccines developed by different research teams all around the globe. Produced in a remarkably short time frame, a crucial first step for the discovery of these vaccines was mapping out the atomic structure of the proteins making up the virus and their interactions. Due to the bright X-rays required in the process, synchrotron light sources play an active role in the ongoing efforts of accomplishing that goal. Synchrotron light sources are particle accelerators that are capable of providing intense electromagnetic radiation by accelerating packages of electrons, called bunches, and forcing them on curved trajectories. Besides the support of research on the SARS-Cov-2 virus, the remarkable properties of synchrotron radiation lead to a multitude of applications in a variety of scientific fields such as materials science, geology, biology and medicine. As a special form of synchrotron radiation, this thesis is concerned with the coherent synchrotron radiation (CSR) generated by short electron bunches in a storage ring. At wavelengths larger than the size of the emitting electron structure, the particles within a bunch radiate coherently. This coherent emission of synchrotron radiation scales with the number of involved particles and can thus enhance the intensity of the emitted radiation by several orders of magnitude. As a consequence, modern synchrotron light sources, such as the Karlsruhe Research Accelerator (KARA) at the Karlsruhe Institute of Technology (KIT), are deliberately operating with short bunch lengths to extend the radiated CSR spectrum to higher frequencies and to increase the intensity of the emitted radiation. Yet, the continuous reduction of the bunch length at high beam intensities eventually leads to complex longitudinal dynamics caused by the self-interaction of the electron bunches with their own emitted CSR. This phenomenon, generally referred to as micro-bunching or micro-wave instability, can lead to the formation of dynamically changing micro-structures within the charge distribution of the electron bunches and thus to a uctuating emission of CSR. Moreover, it can cause oscillations of the bunch length and the energy spread, which can be detrimental to the operation of a synchrotron light source. On the other hand, as electron structures smaller than the full electron bunch, the micro-structures created by the instability lead to an increased emission of CSR at frequencies up to the THz frequency range. The instability can thus also be beneficial for a variety of applications that rely on intense radiation in that particular frequency range.\nOver the past years, the micro-bunching instability has been extensively studied at the KIT storage ring KARA and other synchrotron light sources. Facilitated by the development of novel diagnostics and simulation tools, the instability and the underlying longitudinal beam dynamics were observed and analyzed in great detail and across a large range of machine parameters. Building upon the gained insights and experience with the instability, the work summarized in this thesis takes these efforts one step further by approaching the topic of control over the occurring micro-bunching dynamics. In a careful analysis of the perturbation generated by the CSR self-interaction, an effective method of influencing the formation of micro-structures is identified and the resultant opportunities of exerting control over these dynamics are pursued. As indicated above, the benefits of extensive control over the micro-bunching instability are twofold. A practical method of mitigating the CSR-induced perturbation at an electron storage ring would extend the regime of stable operation to shorter bunch lengths and higher bunch currents. As illustrated in the context of the COVID-19 pandemic, particle accelerators in general and synchrotron light sources in particular are instruments that facilitate basic scientific research in various domains. An extension of the sustainable beam properties that can be provided to external experiments is thus a major benefit. Additionally, successful mitigation of the micro-bunching instability would expand the capabilities to optimize for related beam properties, at existing facilities, but also for future synchrotron light sources. On the other hand, a deliberate and controlled excitation of the micro-structures can amplify the intensity of the CSR emitted in the frequency range corresponding to the spatial extent of the structure and could thus be used to tailor the emission of CSR to dedicated experiments. In an attempt to support these complementary objectives, the presented work is mainly concerned with finding direct ways of interacting with the micro-structure formation process in order to influence the beam dynamics in either direction. For the objective of mitigating the micro-bunching dynamics, which turns out to be the more challenging task, the necessity for dynamic adjustments of the applied control signal naturally motivates the use of reinforcement learning (RL) methods. The general task is thus formalized as an RL problem and different state-of-the-art algorithms are applied to solve the underlying control problem. The pursued approach towards micro-bunching control at electron storage rings is developed and tested on the basis of simulation data and its feasibility verified in first experiments at KARA.\nRead the paper: https://publikationen.bibliothek.kit.edu/1000140271\nContact: -\n","permalink":"https://RL4aa.github.io/posts/publications/boltz2019microbunching/","summary":"T. Boltz\nKarlsruhe Insitute of Technology KIT\nPhD thesis\nAbstract At the time this thesis is written, the world finds itself amidst and partly in the process of recovering from the COVID-19 pandemic caused by the SARS-Cov-2 virus. One major contribution to the worldwide efforts of bringing this pandemic to an end are the vaccines developed by different research teams all around the globe. Produced in a remarkably short time frame, a crucial first step for the discovery of these vaccines was mapping out the atomic structure of the proteins making up the virus and their interactions.","title":"Micro-Bunching Control at Electron Storage Rings with Reinforcement Learning"},{"content":"J. St. John1, C. Herwig1, D. Kafkes1, J. Mitrevski1, W. A. Pellico1, G. N. Perdue1, A. Quintero-Parra1, B. A. Schupbach1, K. Seiya1, N. Tran1, M. Schram2, J. M. Duarte3, Y. Huang4, R. Keller5\n1Fermi National Accelerator Laboratory, 2Thomas Jefferson National Accelerator Laboratory, 3University of California San Diego, 4Pacific Northwest National Laboratory, 5Columbia University\nPhysical Review Accelerators and Beams\nAbstract We describe a method for precisely regulating the gradient magnet power supply (GMPS) at the Fermilab Booster accelerator complex using a neural network trained via reinforcement learning. We demonstrate preliminary results by training a surrogate machine-learning model on real accelerator data to emulate the GMPS, and using this surrogate model in turn to train the neural network for its regulation task. We additionally show how the neural networks to be deployed for control purposes may be compiled to execute on field-programmable gate arrays (FPGAs), and show the first machine-learning based control algorithm implemented on an FPGA for controls at the Fermilab accelerator complex. As there are no surprise latencies on an FPGA, this capability is important for operational stability in complicated environments such as an accelerator facility.\nRead the paper: https://journals.aps.org/prab/abstract/10.1103/PhysRevAccelBeams.24.104601\nContact: Jason St. John\n","permalink":"https://RL4aa.github.io/posts/publications/stjohn2021realtime/","summary":"J. St. John1, C. Herwig1, D. Kafkes1, J. Mitrevski1, W. A. Pellico1, G. N. Perdue1, A. Quintero-Parra1, B. A. Schupbach1, K. Seiya1, N. Tran1, M. Schram2, J. M. Duarte3, Y. Huang4, R. Keller5\n1Fermi National Accelerator Laboratory, 2Thomas Jefferson National Accelerator Laboratory, 3University of California San Diego, 4Pacific Northwest National Laboratory, 5Columbia University\nPhysical Review Accelerators and Beams\nAbstract We describe a method for precisely regulating the gradient magnet power supply (GMPS) at the Fermilab Booster accelerator complex using a neural network trained via reinforcement learning.","title":"Real-time artificial intelligence for accelerator control: A study at the Fermilab Booster"},{"content":"V. Kain1, N. Bruchon1, S. Hirlander1, N. Madysa1, I. Vojskovic1, P. Skowronski1, G. Valentino2\n1CERN, 2University of Malta\n61st ICFA ABDW on High-Intensity and High-Brightness Hadron Beams\nAbstract The CERN H‚àílinear accelerator, LINAC4, served as atest bed for advanced algorithms during the CERN LongShutdown 2 in the years 2019/20. One of the main goals wasto show that reinforcement learning with all its benefits canbe used as a replacement for numerical optimization and asa complement to classical control in the accelerator controlcontext. Many of the algorithms used were prepared before-hand at the electron line of the AWAKE facility to makethe best use of the limited time available at LINAC4. Anoverview of the algorithms and concepts tested at LINAC4and AWAKE will be given and the results discussed.\nRead the paper: https://jacow.org/hb2021/papers/tuec4.pdf\nContact: Verena Kain\n","permalink":"https://RL4aa.github.io/posts/publications/kain2021test/","summary":"V. Kain1, N. Bruchon1, S. Hirlander1, N. Madysa1, I. Vojskovic1, P. Skowronski1, G. Valentino2\n1CERN, 2University of Malta\n61st ICFA ABDW on High-Intensity and High-Brightness Hadron Beams\nAbstract The CERN H‚àílinear accelerator, LINAC4, served as atest bed for advanced algorithms during the CERN LongShutdown 2 in the years 2019/20. One of the main goals wasto show that reinforcement learning with all its benefits canbe used as a replacement for numerical optimization and asa complement to classical control in the accelerator controlcontext.","title":"Test of Machine Learning at the CERN LINAC4"},{"content":"L. Grech\nUniversity of Malta\nPhD thesis\nAbstract The Large Hadron Collider (LHC) at the European Organization for Nuclear Research (CERN) is the largest synchrotron built to date, having a circumference of approx- imately 27km. The LHC is able to accelerate two counter-rotating proton and/or heavy-ion beams up to 7 TeV per charge. These highly energetic beams are contained inside a vacuum chamber with an inner diameter of 80 mm by means of strong mag- netic fields produced by superconducting magnets. A beam cleaning and machine protection system is in place to prevent high-energy halo particles from impacting and heating the superconducting magnets.\nDue to the tight tolerances on the beam trajectory imposed by the beam cleaning and machine protection system, the LHC was the first accelerator to require automatic beam-based feedback control. Until the LHC Run 2, this was implemented by the Beam-Based Feedback System (BBFS) which was mainly responsible for collecting beam measurements from various types of beam instrumentation and calculating the corrections in the current of corrector magnets throughout the whole length of the LHC.\nThroughout the years of LHC operation, some of the original BBFS functionality became deprecated and fell into disuse. Various minor upgrades were performed, how- ever, without a systematic redesign of the BBFS the code became difficult to maintain. The BBFS also suffered from a computational bottleneck during the calculation of the optics matrices used by the BBFS controllers. A feasibility study was performed to assess whether Hardware Acceleration (HA) in the form of Graphical Processing Units (GPUs) would improve the performance. It was found that the use of GPUs was not necessary and as a result the hardware upgrade for the BBFS could be finalised. Following this was the redesign and implementation of a more streamlined BBFS called BFCLHC. This work was done in collaboration with BE-OP-LHC and was designed with a more intuitive interface. A new feature called Function Players was imple- mented at the request of the LHC operators, which allows certain BFCLHC settings to be automated.\nThe Base-Band Q (tune) (BBQ) system is responsible for estimating the values of the horizontal and vertical tunes in both beams of the LHC. It was found that noise harmonics were perturbing the tune estimates and consequently causing the Tune Feedback (QFB) within the BBFS to behave erroneously. In this work, new tune estimation algorithms were developed to improve upon the accuracy of the BBQ system. To aid with this development, a simulation procedure was set up which mimics the frequency spectra obtained by the BBQ system. Two algorithmic approaches were proposed which take into consideration the location of the noise harmonics. A data-driven approach was also attempted where a combination of simulated and real frequency spectra were used to train a neural network to predict the value of the tune from a BBQ spectrum. It was observed that the data-driven approach improves upon the stability of the tune estimates in the presence of noise harmonics.\nThe use of Reinforcement Learning (RL) in beam-based feedback control systems was considered since it offers the possibility of optimal control, regardless of any dy- namical changes in the machine. Since this work was performed during the LHC Long Shutdown 2 (LS2), the control problem present in the BBFS was formulated in a sim- ulation environment called QFBEnv that allowed RL agents to be trained offline. The linear beam optics models used in the standard Proportional-Integral (PI) controllers of the BBFS were used to simulate the response of the LHC. In particular, the perfor- mance of several RL agents in the task of tune correction were assessed in detail and run through different failure scenarios. All the tests performed showed that certain RL agents can achieve a better performance than a standard PI controller, also in the presence of actuator failures. The possibility of using RL in the orbit feedback was also considered and discussed.\nRead the paper: https://www.um.edu.mt/library/oar/handle/123456789/104427\nContact: Leander Grech\n","permalink":"https://RL4aa.github.io/posts/publications/grech2021renovation/","summary":"L. Grech\nUniversity of Malta\nPhD thesis\nAbstract The Large Hadron Collider (LHC) at the European Organization for Nuclear Research (CERN) is the largest synchrotron built to date, having a circumference of approx- imately 27km. The LHC is able to accelerate two counter-rotating proton and/or heavy-ion beams up to 7 TeV per charge. These highly energetic beams are contained inside a vacuum chamber with an inner diameter of 80 mm by means of strong mag- netic fields produced by superconducting magnets.","title":"Renovation of the beam-based feedback systems in the LHC"},{"content":"W. Wang1, M. Caselle1, T. Boltz1, E. Blomley1, M. Brosi1, T. Dritschler1, A. Ebersoldt1, A. Kopmann1, A. Santamaria Garcia1, P. Schreiber1, E. Br√ºndermann1, M. Weber1, A.-S. M√ºller1, Y. Fang2\n1Karlsruhe Insitute of Technology KIT, 2Northwestern Polytechnical University\nIEEE Transactions on Nuclear Science\nAbstract Coherent synchrotron radiation (CSR) is generated when the electron bunch length is in the order of the magnitude of the wavelength of the emitted radiation. The self-interaction of short electron bunches with their own electromagnetic fields changes the longitudinal beam dynamics significantly. Above a certain current threshold, the micro-bunching instability develops, characterized by the appearance of distinguishable substructures in the longitudinal phase space of the bunch. To stabilize the CSR emission, a real-time feedback control loop based on reinforcement learning (RL) is proposed. Informed by the available THz diagnostics, the feedback is designed to act on the radio frequency (RF) system of the storage ring to mitigate the micro-bunching dynamics. To satisfy low-latency requirements given by the longitudinal beam dynamics, the RL controller has been implemented on hardware (FPGA). In this article, a real-time feedback loop architecture and its performance is presented and compared with a software implementation using Keras-RL on CPU/GPU. The results obtained with the CSR simulation Inovesa demonstrate that the functionality of both platforms is equivalent. The training performance of the hardware implementation is similar to software solution, while it outperforms the Keras-RL implementation by an order of magnitude. The presented RL hardware controller is considered as an essential platform for the development of intelligent CSR control systems.\nRead the paper: https://doi.org/10.1109/TNS.2021.3084515\nContact: -\n","permalink":"https://RL4aa.github.io/posts/publications/wang2021accelerated/","summary":"W. Wang1, M. Caselle1, T. Boltz1, E. Blomley1, M. Brosi1, T. Dritschler1, A. Ebersoldt1, A. Kopmann1, A. Santamaria Garcia1, P. Schreiber1, E. Br√ºndermann1, M. Weber1, A.-S. M√ºller1, Y. Fang2\n1Karlsruhe Insitute of Technology KIT, 2Northwestern Polytechnical University\nIEEE Transactions on Nuclear Science\nAbstract Coherent synchrotron radiation (CSR) is generated when the electron bunch length is in the order of the magnitude of the wavelength of the emitted radiation. The self-interaction of short electron bunches with their own electromagnetic fields changes the longitudinal beam dynamics significantly.","title":"Accelerated Deep Reinforcement Learning for Fast Feedback of Beam Dynamics at KARA"},{"content":"A. Eichler1, F. Burkart1, J. Kaiser1, W. Kuropka1, O. Stein1, E. Br√ºndermann2, A. Santamaria Garcia2, C. Xu2\n1Deutsches Elektronen-Synchrotron DESY, 2Karlsruhe Institute of Technology KIT\n12th International Particle Accelerator Conference\nAbstract Reinforcement learning algorithms have risen in pop-ularity in the accelerator physics community in recentyears, showing potential in beam control and in the opti-mization and automation of tasks in accelerator operation.The Helmholtz AI project ‚ÄúMachine Learning Toward Au-tonomous Accelerators‚Äù is a collaboration between DESYand KIT that works on investigating and developing rein-forcement learning applications for the automatic start-upof electron linear accelerators. The work is carried out inparallel at two similar research accelerators: ARES at DESYand FLUTE at KIT, giving the unique opportunity of trans-fer learning between facilities. One of the first steps of thisproject is the establishment of a common interface betweenthe simulations and the machine, in order to test and applyvarious optimization approaches interchangeably betweenthe two accelerators. In this paper we present first results onthe common interface and its application to beam focusingin ARES as well as the idea of laser shaping with spatiallight modulators at FLUTE.\nRead the paper: https://jacow.org/ipac2021/papers/tupab298.pdf\nContact: Annika Eichler\n","permalink":"https://RL4aa.github.io/posts/publications/eichler2021first/","summary":"A. Eichler1, F. Burkart1, J. Kaiser1, W. Kuropka1, O. Stein1, E. Br√ºndermann2, A. Santamaria Garcia2, C. Xu2\n1Deutsches Elektronen-Synchrotron DESY, 2Karlsruhe Institute of Technology KIT\n12th International Particle Accelerator Conference\nAbstract Reinforcement learning algorithms have risen in pop-ularity in the accelerator physics community in recentyears, showing potential in beam control and in the opti-mization and automation of tasks in accelerator operation.The Helmholtz AI project ‚ÄúMachine Learning Toward Au-tonomous Accelerators‚Äù is a collaboration between DESYand KIT that works on investigating and developing rein-forcement learning applications for the automatic start-upof electron linear accelerators.","title":"First Steps Toward an Autonomous Accelerator, A Common Project Between DESY and KIT"},{"content":"A. Ivanov, I. Agapov, A. Eichler, S. Tomin\nDeutsches Elektronen Synchrotron DESY\n12th International Particle Accelerator Conference\nAbstract We propose an approach for incorporating acceleratorphysics models into reinforcement learning agents. The proposed approach is based on the Taylor mapping technique for the simulation of particle dynamics. The resulting computational graph is represented as a polynomial neural network and embedded into the traditional reinforcement learning agents. The application of the model is demonstrated in a nonlinear simulation model of beam transmission. The comparison of the approach with the traditional numerical optimization as well as neural networks-based agents demonstrates better convergence of the proposed technique.\nRead the paper: https://jacow.org/ipac2021/papers/thpab191.pdf\nContact: Andrei Ivanov\n","permalink":"https://RL4aa.github.io/posts/publications/ivanov2021physicsenhanced/","summary":"A. Ivanov, I. Agapov, A. Eichler, S. Tomin\nDeutsches Elektronen Synchrotron DESY\n12th International Particle Accelerator Conference\nAbstract We propose an approach for incorporating acceleratorphysics models into reinforcement learning agents. The proposed approach is based on the Taylor mapping technique for the simulation of particle dynamics. The resulting computational graph is represented as a polynomial neural network and embedded into the traditional reinforcement learning agents. The application of the model is demonstrated in a nonlinear simulation model of beam transmission.","title":"Physics-Enhanced Reinforcement Learning for Optimal Control"},{"content":"N. Bruchon\nUniversity of Trieste\nPhD thesis\nAbstract The research carried out in particle accelerator facilities does not concern only particle and condensed matter physics, although these are the main topics covered in the field. Indeed, since a particle accelerator is composed of many different sub-systems, its proper functioning depends both on each of these parts and their interconnection. It follows that the study, implementation, and improvement of the various sub-systems are fundamental points of investigation too. In particular, an interesting aspect for the automation engineering community is the control of such systems that usually are complex, large, noise-affected, and non-linear.\nThe doctoral project fits into this scope, investigating the introduction of new methods to automatically improve the performance of a specific type of particle accelerators: seeded free-electron lasers. The optimization of such systems is a challenging task, already faced in years by many different approaches in order to find and attain an optimal working point, keeping it optimally tuned despite drift or disturbances. Despite the good results achieved, better ones are always sought for. For this reason, several methods belonging to reinforcement learning, an area of machine learning that is attracting more and more attention in the scientific field, have been applied on FERMI, the free-electron laser facility at Elettra Sincrotrone Trieste. The research activity has been carried out by applying both model-free and model-based techniques belonging to reinforcement learning. Satisfactory preliminary results have been obtained, that present the first step toward a new fully automatic procedure for the alignment of the seed laser to the electron beam.\nIn the meantime, at the Conseil Europ√©en pour la Recherche Nucl√©aire, CERN, a similar investigation was ongoing. In the last year of the doctoral course, a collaboration to share the knowledge on the topic took place. Some of the results collected on the largest particle physics laboratory in the world are presented in the doctoral dissertation.\nRead the paper: https://arts.units.it/handle/11368/2982117\nContact: Niky Bruchon\n","permalink":"https://RL4aa.github.io/posts/publications/bruchon2021feasiblity/","summary":"N. Bruchon\nUniversity of Trieste\nPhD thesis\nAbstract The research carried out in particle accelerator facilities does not concern only particle and condensed matter physics, although these are the main topics covered in the field. Indeed, since a particle accelerator is composed of many different sub-systems, its proper functioning depends both on each of these parts and their interconnection. It follows that the study, implementation, and improvement of the various sub-systems are fundamental points of investigation too.","title":"Feasibility Investigation on Several Reinforcement Learning Techniques to Improve the Performance of the FERMI Free-Electron Laser"},{"content":"F. H. O\u0026rsquo;Shea1, N. Bruchon2, G. Gaio1\n1Elettra Sincrotrone Trieste, 2University of Trieste\nPhysical Review Accelerators and Beams\nAbstract In this article we report on the application of a model-free reinforcement learning method to the optimization of accelerator systems. We simplify a policy gradient algorithm to accelerator control from sophisticated algorithms that have recently been demonstrated to solve complex dynamic problems. After outlining a theoretical basis for the functioning of the algorithm, we explore the small hyperparameter space to develop intuition about said parameters using a simple number-guess environment. Finally, we demonstrate the algorithm optimizing both a free-electron laser and an accelerator-based terahertz source in-situ. The algorithm is applied to different accelerator control systems and optimizes the desired signals in a few hundred steps without any domain knowledge using up to five control parameters. In addition, the algorithm shows modest tolerance to accelerator fault conditions without any special preparation for such conditions.\nRead the paper: https://journals.aps.org/prab/abstract/10.1103/PhysRevAccelBeams.23.122802\nContact: Finn H. O\u0026rsquo;Shea\n","permalink":"https://RL4aa.github.io/posts/publications/oshea202policy/","summary":"F. H. O\u0026rsquo;Shea1, N. Bruchon2, G. Gaio1\n1Elettra Sincrotrone Trieste, 2University of Trieste\nPhysical Review Accelerators and Beams\nAbstract In this article we report on the application of a model-free reinforcement learning method to the optimization of accelerator systems. We simplify a policy gradient algorithm to accelerator control from sophisticated algorithms that have recently been demonstrated to solve complex dynamic problems. After outlining a theoretical basis for the functioning of the algorithm, we explore the small hyperparameter space to develop intuition about said parameters using a simple number-guess environment.","title":"Policy gradient methods for free-electron laser and terahertz source optimization and stabilization at the FERMI free-electron laser at Elettra"},{"content":"S. Hirlaender1, N. Bruchon2\n1University of Salzburg, 2University of Trieste\narXiv\nAbstract Reinforcement learning holds tremendous promise in accelerator controls. The primary goal of this paper is to show how this approach can be utilised on an operational level on accelerator physics problems. Despite the success of model-free reinforcement learning in several domains, sample-efficiency still is a bottle-neck, which might be encompassed by model-based methods. We compare well-suited purely model-based to model-free reinforcement learning applied to the intensity optimisation on the FERMI FEL system. We find that the model-based approach demonstrates higher representational power and sample-efficiency, while the asymptotic performance of the model-free method is slightly superior. The model-based algorithm is implemented in a DYNA-style using an uncertainty aware model, and the model-free algorithm is based on tailored deep Q-learning. In both cases, the algorithms were implemented in a way, which presents increased noise robustness as omnipresent in accelerator control problems.\nRead the paper: https://arxiv.org/abs/2012.09737\nContact: Simon Hirlaender\n","permalink":"https://RL4aa.github.io/posts/publications/hirlaender2020modelfree/","summary":"S. Hirlaender1, N. Bruchon2\n1University of Salzburg, 2University of Trieste\narXiv\nAbstract Reinforcement learning holds tremendous promise in accelerator controls. The primary goal of this paper is to show how this approach can be utilised on an operational level on accelerator physics problems. Despite the success of model-free reinforcement learning in several domains, sample-efficiency still is a bottle-neck, which might be encompassed by model-based methods. We compare well-suited purely model-based to model-free reinforcement learning applied to the intensity optimisation on the FERMI FEL system.","title":"Model-free and Bayesian Ensembling Model-based Deep Reinforcement Learning for Particle Accelerator Control Demonstrated on the FERMI FEL"},{"content":"X. Pang1, S. Thulasidasan2, L. Rybarcyk2\n1Apple, 2Los Alamos National Laboratory\nMachine Learning for Engineering Modeling, Simulation, and Design Workshop at Neural Information Processing Systems 2020\nAbstract We describe an approach to learning optimal control policies for a large, linear particle accelerator using deep reinforcement learning coupled with a high-fidelity physics engine. The framework consists of an AI controller that uses deep neural networks for state and action-space representation and learns optimal policies using reward signals that are provided by the physics simulator. For this work, we only focus on controlling a small section of the entire accelerator. Nevertheless, initial results indicate that we can achieve better-than-human level performance in terms of particle beam current and distribution. The ultimate goal of this line of work is to substantially reduce the tuning time for such facilities by orders of magnitude, and achieve near-autonomous control.\nRead the paper: https://ml4eng.github.io/camera_readys/58.pdf\nContact: Xiaoying Pang\n","permalink":"https://RL4aa.github.io/posts/publications/pang2020autonomous/","summary":"X. Pang1, S. Thulasidasan2, L. Rybarcyk2\n1Apple, 2Los Alamos National Laboratory\nMachine Learning for Engineering Modeling, Simulation, and Design Workshop at Neural Information Processing Systems 2020\nAbstract We describe an approach to learning optimal control policies for a large, linear particle accelerator using deep reinforcement learning coupled with a high-fidelity physics engine. The framework consists of an AI controller that uses deep neural networks for state and action-space representation and learns optimal policies using reward signals that are provided by the physics simulator.","title":"Autonomous Control of a Particle Accelerator using Deep Reinforcement Learning"},{"content":"V. Kain1, S. Hirlander1, B. Goddard1, F. M. Velotti1, G. Z. Della Porta1, N. Bruchon2, G. Valentino3\n1CERN, 2University of Trieste, 3University of Malta\nPhysical Review Accelerators and Beams\nAbstract Numerical optimization algorithms are already established tools to increase and stabilize the performance of particle accelerators. These algorithms have many advantages, are available out of the box, and can be adapted to a wide range of optimization problems in accelerator operation. The next boost in efficiency is expected to come from reinforcement learning algorithms that learn the optimal policy for a certain control problem and hence, once trained, can do without the time-consuming exploration phase needed for numerical optimizers. To investigate this approach, continuous model-free reinforcement learning with up to 16 degrees of freedom was developed and successfully tested at various facilities at CERN. The approach and algorithms used are discussed and the results obtained for trajectory steering at the AWAKE electron line and LINAC4 are presented. The necessary next steps, such as uncertainty aware model-based approaches, and the potential for future applications at particle accelerators are addressed.\nRead the paper: https://doi.org/10.1103/PhysRevAccelBeams.23.124801\nContact: Verena Kain\n","permalink":"https://RL4aa.github.io/posts/publications/kain2020sampleefficient/","summary":"V. Kain1, S. Hirlander1, B. Goddard1, F. M. Velotti1, G. Z. Della Porta1, N. Bruchon2, G. Valentino3\n1CERN, 2University of Trieste, 3University of Malta\nPhysical Review Accelerators and Beams\nAbstract Numerical optimization algorithms are already established tools to increase and stabilize the performance of particle accelerators. These algorithms have many advantages, are available out of the box, and can be adapted to a wide range of optimization problems in accelerator operation.","title":"Sample-efficient reinforcement learning for CERN accelerator control"},{"content":"A. L. Edelen\nColorado State University\nPhD thesis\nAbstract Charged particle accelerators support a wide variety of scientific, industrial, and medical applications. They range in scale and complexity from systems with just a few components for beam acceleration and manipulation, to large scientific user facilities that span many kilometers and have hundreds-to-thousands of individually-controllable components. Specific operational requirements must be met by adjusting the many controllable variables of the accelerator. Meeting these requirements can be challenging, both in terms of the ability to achieve specific beam quality metrics in a reliable fashion and in terms of the time needed to set up and maintain the optimal operating conditions. One avenue toward addressing this challenge is to incorporate techniques from the fields of machine learning (ML) and artificial intelligence (AI) into the way particle accelerators are modeled and controlled. While many promising approaches within AI/ML could beused for particle accelerators, this dissertation focuses on approaches based on neural networks. Neural networks are particularly well-suited to modeling, control, and diagnostic analysis of non-linear systems, as well as systems with large parameter spaces. They are also very appealing for their ability to process high-dimensional data types, such as images and time series (both of which are ubiquitous in particle accelerators). In this work, key studies that demonstrated the potential utility of modern neural network-based approaches to modeling and control of particle accelerators are presented. The context for this work is important: at the start of this work in 2012, there was little interest in AI/ML in the particle accelerator community, and many of the advances in neural networks and deep learning that enabled its present success had not yet been made at that time. As such, this work was both an exploration of possible application areas and a generator of initial demonstrations in these areas, including some of the first applications of modern deep neural networks in particle accelerators.\nRead the paper: https://www.proquest.com/openview/7deab8ec9a2cf877604472c891f4d51c/1\nContact: -\n","permalink":"https://RL4aa.github.io/posts/publications/edelen2020neural/","summary":"A. L. Edelen\nColorado State University\nPhD thesis\nAbstract Charged particle accelerators support a wide variety of scientific, industrial, and medical applications. They range in scale and complexity from systems with just a few components for beam acceleration and manipulation, to large scientific user facilities that span many kilometers and have hundreds-to-thousands of individually-controllable components. Specific operational requirements must be met by adjusting the many controllable variables of the accelerator. Meeting these requirements can be challenging, both in terms of the ability to achieve specific beam quality metrics in a reliable fashion and in terms of the time needed to set up and maintain the optimal operating conditions.","title":"Neural Networks for Modeling and Control of Particle Accelerators"},{"content":"N. Bruchon1, G. Fenu1, G. Gaio2, M. Lonza2, F. H. O‚ÄôShea2, F. A. Pellegrino1, E. Salvato1\n1University of Trieste, 2Elettra Sincrotrone Trieste\nElectronics\nAbstract Optimal tuning of particle accelerators is a challenging task. Many different approaches have been proposed in the past to solve two main problems‚Äîattainment of an optimal working point and performance recovery after machine drifts. The most classical model-free techniques (e.g., Gradient Ascent or Extremum Seeking algorithms) have some intrinsic limitations. To overcome those limitations, Machine Learning tools, in particular Reinforcement Learning (RL), are attracting more and more attention in the particle accelerator community. We investigate the feasibility of RL model-free approaches to align the seed laser, as well as other service lasers, at FERMI, the free-electron laser facility at Elettra Sincrotrone Trieste. We apply two different techniques‚Äîthe first, based on the episodic Q-learning with linear function approximation, for performance optimization; the second, based on the continuous Natural Policy Gradient REINFORCE algorithm, for performance recovery. Despite the simplicity of these approaches, we report satisfactory preliminary results, that represent the first step toward a new fully automatic procedure for the alignment of the seed laser to the electron beam. Such an alignment is, at present, performed manually.\nRead the paper: https://www.mdpi.com/2079-9292/9/5/781\nContact: Niky Bruchon\n","permalink":"https://RL4aa.github.io/posts/publications/bruchon2020basic/","summary":"N. Bruchon1, G. Fenu1, G. Gaio2, M. Lonza2, F. H. O‚ÄôShea2, F. A. Pellegrino1, E. Salvato1\n1University of Trieste, 2Elettra Sincrotrone Trieste\nElectronics\nAbstract Optimal tuning of particle accelerators is a challenging task. Many different approaches have been proposed in the past to solve two main problems‚Äîattainment of an optimal working point and performance recovery after machine drifts. The most classical model-free techniques (e.g., Gradient Ascent or Extremum Seeking algorithms) have some intrinsic limitations.","title":"Basic Reinforcement Learning Techniques to Control the Intensity of a Seeded Free-Electron Laser"},{"content":"N. Bruchon, G. Fenu, G. Gaio, M. Lonza, F. A. Pellegrino, E. Salvato\nUniversity of Trieste\n23rd International Conference on Mechatronics Technology\nAbstract The optimization of particle accelerators is a challenging task, and many different approaches have been proposed in years, to obtain an optimal tuning of the plant and to keep it optimally tuned despite drifts or disturbances. Indeed, the classical model-free approaches (such as Gradient Ascent or Extremum Seeking algorithms) have intrinsic limitations. To overcome those limitations, Machine Learning techniques, in particular, the Reinforcement Learning, are attracting more and more attention in the particle accelerator community. The purpose of this paper is to apply a Reinforcement Learning model-free approach to the alignment of a seed laser, based on a rather general target function depending on the laser trajectory. The study focuses on the alignment of the lasers at FERMI, the free-electron laser facility at Elettra Sincrotrone Trieste. In particular, we employ Q-learning with linear function approximation and report experimental results obtained in two setups, which are the actual setups where the final application has to be deployed. Despite the simplicity of the approach, we report satisfactory preliminary results, that represent the first step toward a fully automatic procedure for seed laser to the electron beam. Such a superimposition is, at present, performed manually.\nRead the paper: https://ieeexplore.ieee.org/document/8932150\nContact: Niky Bruchon\n","permalink":"https://RL4aa.github.io/posts/publications/bruchon2019toward/","summary":"N. Bruchon, G. Fenu, G. Gaio, M. Lonza, F. A. Pellegrino, E. Salvato\nUniversity of Trieste\n23rd International Conference on Mechatronics Technology\nAbstract The optimization of particle accelerators is a challenging task, and many different approaches have been proposed in years, to obtain an optimal tuning of the plant and to keep it optimally tuned despite drifts or disturbances. Indeed, the classical model-free approaches (such as Gradient Ascent or Extremum Seeking algorithms) have intrinsic limitations.","title":"Toward the Application of Reinforcement Learning to the Intensity Control of a Seeded Free-Electron Laser"},{"content":"N. Bruchon1, G. Gaio2, G. Fenu1, M. Lonza2, F. A. Pellegrino1, E. Salvato1\n1University of Trieste, 2Elettra Sincrotrone Trieste\n17th International Conference on Accelerator and Large Experimental Physics Control Systems\nAbstract Reinforcement Learning (RL) is one of the most promis-ing techniques in Machine Learning because of its modestcomputational requirements with respect to other algorithms.RL uses an agent that takes actions within its environmentto maximize a reward related to the goal it is designed toachieve. We have recently used RL as a model-free approachto improve the performance of the FERMI Free ElectronLaser. A number of machine parameters are adjusted tofind the optimum FEL output in terms of intensity and spec-tral quality. In particular we focus on the problem of thealignment of the seed laser with the electron beam, initiallyusing a simplified model and then applying the developedalgorithm on the real machine. This paper reports the resultsobtained and discusses pros and cons of this approach withplans for future applications.\nRead the paper: https://accelconf.web.cern.ch/icalepcs2019/papers/wepha021.pdf\nContact: Niky Bruchon\n","permalink":"https://RL4aa.github.io/posts/publications/bruchon2019freeelectron/","summary":"N. Bruchon1, G. Gaio2, G. Fenu1, M. Lonza2, F. A. Pellegrino1, E. Salvato1\n1University of Trieste, 2Elettra Sincrotrone Trieste\n17th International Conference on Accelerator and Large Experimental Physics Control Systems\nAbstract Reinforcement Learning (RL) is one of the most promis-ing techniques in Machine Learning because of its modestcomputational requirements with respect to other algorithms.RL uses an agent that takes actions within its environmentto maximize a reward related to the goal it is designed toachieve.","title":"Free-electron Laser Optimization with Reinforcement Learning"},{"content":"T. Boltz, M. Brosi, E. Br√ºndermann, B. Haerer, P. Kaiser, C. Pohl, P. Schreiber, M. Yan,T. Asfour, A.-S. M√ºller\nKarlsruhe Insitute of Technology KIT\n10th International Particle Accelerator Conference\nAbstract The operation of ring-based synchrotron light sourceswith short electron bunches increases the emission of co-herent synchrotron radiation (CSR) in the THz frequencyrange. However, the micro-bunching instability resultingfrom self-interaction of the bunch with its own radiationfield limits stable operation with constant intensity of CSRemission to a particular threshold current. Above this thresh-old, the longitudinal charge distribution and thus the emittedradiation vary rapidly and continuously. Therefore, a fastand adaptive feedback system is the appropriate approach tostabilize the dynamics and to overcome the limitations givenby the instability. In this contribution, we discuss first effortstowards a longitudinal feedback design that acts on the RFsystem of the KIT storage ring KARA (Karlsruhe ResearchAccelerator) and aims for stabilization of the emitted THzradiation. Our approach is based on methods of adaptive con-trol that were developed in the field of reinforcement learningand have seen great success in other fields of research overthe past decade. We motivate this particular approach andcomment on different aspects of its implementation.\nRead the paper: http://jacow.org/ipac2019/papers/mopgw017.pdf\nContact: Tobias Boltz\n","permalink":"https://RL4aa.github.io/posts/publications/boltz2019feedback/","summary":"T. Boltz, M. Brosi, E. Br√ºndermann, B. Haerer, P. Kaiser, C. Pohl, P. Schreiber, M. Yan,T. Asfour, A.-S. M√ºller\nKarlsruhe Insitute of Technology KIT\n10th International Particle Accelerator Conference\nAbstract The operation of ring-based synchrotron light sourceswith short electron bunches increases the emission of co-herent synchrotron radiation (CSR) in the THz frequencyrange. However, the micro-bunching instability resultingfrom self-interaction of the bunch with its own radiationfield limits stable operation with constant intensity of CSRemission to a particular threshold current.","title":"Feedback Design for Control of the Micro-Bunching Instability Based on Reinforcement Learning"},{"content":"A. L. Edelen1, S. G. Biedron2, J. P. Edelen3, S. V. Milton4, P. J. M. van der Slot5\n1Colorado State University, 2Element Aero, 3Fermi National Accelerator Laboratory, 4Los Alamos National Laboratory, 5University of Twente\n38th International Free Electron Laser Conference\nAbstract FEL user facilities often must accommodate requests for a variety of beam parameters. This usually requires skilled operators to tune the machine, reducing the amount of available time for users. In principle, a neural network control policy that is trained on a broad range of operating states could be used to quickly switch between these requests without substantial need for human inter-vention. We present preliminary results from an ongoing study in which a neural network control policy is investi-gated for rapid switching between beam parameters in a compact THz FEL.\nRead the paper: http://jacow.org/fel2017/papers/wep031.pdf\nContact: Auralee Edelen\n","permalink":"https://RL4aa.github.io/posts/publications/edelen2017using/","summary":"A. L. Edelen1, S. G. Biedron2, J. P. Edelen3, S. V. Milton4, P. J. M. van der Slot5\n1Colorado State University, 2Element Aero, 3Fermi National Accelerator Laboratory, 4Los Alamos National Laboratory, 5University of Twente\n38th International Free Electron Laser Conference\nAbstract FEL user facilities often must accommodate requests for a variety of beam parameters. This usually requires skilled operators to tune the machine, reducing the amount of available time for users.","title":"Using a neural network control policy for rapid switching between beam parameters in an FEL"},{"content":"A. L. Edelen1, S. G. Biedron2, J. P. Edelen3, S. V. Milton4, P. J. M. van der Slot5\n1Colorado State University, 2Element Aero, 3Fermi National Accelerator Laboratory, 4Los Alamos National Laboratory, 5University of Twente\nWorkshop on Deep Learning for Physical Sciences at the Conference on Neural Information Processing Systems 2017\nAbstract Free Electron Laser (FEL) facilities often must accommodate requests for a varietyof electron beam parameters in order to supply scientific users with appropriatephoton beam characteristics. This usually requires skilled human operators to tunethe machine. In principle, a neural network control policy that is trained on a broadrange of machine operating states could be used to quickly switch between theserequests without substantial need for human intervention. We present preliminaryresults from an ongoing simulation study in which a neural network control policyis investigated for rapid switching between beam parameters in a compact THzFEL that exhibits nonlinear electron beam dynamics. To accomplish this, we firsttrain a feed-forward neural network to mimic a physics-based simulation of theFEL. We then train a neural network control policy by first pre-training it as aninverse model (using supervised learning with a subset of the simulation data) andthen training it more extensively with reinforcement learning. In this case, thereinforcement learning component consists of letting the policy network interactwith the learned system model and backpropagating the cost through the modelnetwork to the controller network.\nRead the paper: https://ml4physicalsciences.github.io/2017/files/nips_dlps_2017_16.pdf\nContact: Auralee Edelen\n","permalink":"https://RL4aa.github.io/posts/publications/edelen2017using2/","summary":"A. L. Edelen1, S. G. Biedron2, J. P. Edelen3, S. V. Milton4, P. J. M. van der Slot5\n1Colorado State University, 2Element Aero, 3Fermi National Accelerator Laboratory, 4Los Alamos National Laboratory, 5University of Twente\nWorkshop on Deep Learning for Physical Sciences at the Conference on Neural Information Processing Systems 2017\nAbstract Free Electron Laser (FEL) facilities often must accommodate requests for a varietyof electron beam parameters in order to supply scientific users with appropriatephoton beam characteristics.","title":"Using Neural Network Control Policies For Rapid Switching Between Beam Parameters in a Free Electron Laser"},{"content":"E. Meier, Y.-R. E. Tan, G. S. LeBlanc\nAustralian Synchrotron\n3rd International Particle Accelerator Conference\nAbstract This paper reports the use of neural networks for orbitcorrection at the Australian Synchrotron Storage Ring. Theproposed system uses two neural networks in an actor-criticscheme to model a long term cost function and computeappropriate corrections. The system is entirely based onthe history of the beam position and the actuators, i.e. thecorrector magnets, in the storage ring. This makes the sys-tem auto-tuneable, which has the advantage of avoiding themeasure of a response matrix. The controller will automat-ically maintain an updated BPM corrector response matrix.In future if coupled with some form of orbit response anal-ysis, the system will have the potential to track drifts orchanges to the lattice functions in ‚Äùreal time‚Äù. As a genericand robust orbit correction program it can be used duringcommissioning and in slow orbit feedback. In this study,we present positive initial results of the simulations of thestorage ring in Matlab.\nRead the paper: https://accelconf.web.cern.ch/IPAC2012/papers/weppp057.pdf\nContact: Evelyne Meier\n","permalink":"https://RL4aa.github.io/posts/publications/meier2012orbit/","summary":"E. Meier, Y.-R. E. Tan, G. S. LeBlanc\nAustralian Synchrotron\n3rd International Particle Accelerator Conference\nAbstract This paper reports the use of neural networks for orbitcorrection at the Australian Synchrotron Storage Ring. Theproposed system uses two neural networks in an actor-criticscheme to model a long term cost function and computeappropriate corrections. The system is entirely based onthe history of the beam position and the actuators, i.e. thecorrector magnets, in the storage ring.","title":"Orbit Correction Studies Using Neural Networks"},{"content":"About us Organizers of the 1st RL4AA workshop and the maintainers of the RL4AA collaboration site.\nAndrea Santamaria Garcia Simon Hirl√§nder Jan Kaiser Chenran Xu Contributing We welcome all enthusiastics and practitioner of reinforcement learning for particle acceelrators to contribute to the website. Please visit the Github Repository for more information.\nAnnounce new publications We try to maintain a comprehensive list of the publications in the RL for accelerator field.\nIf you have a recent publication fitting for the topic, or you find any paper missing, you could start an issue and fill in the essential information.\nWe will update the websites accordingling.\nShare your knowledge We also encourage everyone who has worked in this field to share their knowledge and experience in form of blog posts.\nYou could contact us directly or start an issue in the repository.\n","permalink":"https://RL4aa.github.io/contact/","summary":"contact","title":"Contact Information"}]