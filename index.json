[{"content":"J. Kaiser1, C. Xu2, A. Eichler1, A. Santamaria Garcia2, O. Stein1, E. Bründermann2, W. Kuropka1, H. Dinter1, F. Mayet1, T. Vinatier1, F. Burkart1, H. Schlarb1\n1Deutsches Elektronen-Synchrotron DESY, 2 Karlsruhe Institute of Technology KIT\narXiv\nAbstract Online tuning of real-world plants is a complex optimisation problem that continues to require manual intervention by experienced human operators. Autonomous tuning is a rapidly expanding field of research, where learning-based methods, such as Reinforcement Learning-trained Optimisation (RLO) and Bayesian optimisation (BO), hold great promise for achieving outstanding plant performance and reducing tuning times. Which algorithm to choose in different scenarios, however, remains an open question. Here we present a comparative study using a routine task in a real particle accelerator as an example, showing that RLO generally outperforms BO, but is not always the best choice. Based on the study\u0026rsquo;s results, we provide a clear set of criteria to guide the choice of algorithm for a given tuning task. These can ease the adoption of learning-based autonomous tuning solutions to the operation of complex real-world plants, ultimately improving the availability and pushing the limits of operability of these facilities, thereby enabling scientific and engineering advancements.\nRead the paper: https://arxiv.org/abs/2306.03739\nContact: Jan Kaiser Chenran Xu\n","permalink":"https://RL4aa.github.io/posts/publications/kaiser2023learning/","summary":"J. Kaiser1, C. Xu2, A. Eichler1, A. Santamaria Garcia2, O. Stein1, E. Bründermann2, W. Kuropka1, H. Dinter1, F. Mayet1, T. Vinatier1, F. Burkart1, H. Schlarb1\n1Deutsches Elektronen-Synchrotron DESY, 2 Karlsruhe Institute of Technology KIT\narXiv\nAbstract Online tuning of real-world plants is a complex optimisation problem that continues to require manual intervention by experienced human operators. Autonomous tuning is a rapidly expanding field of research, where learning-based methods, such as Reinforcement Learning-trained Optimisation (RLO) and Bayesian optimisation (BO), hold great promise for achieving outstanding plant performance and reducing tuning times.","title":"Learning to Do or Learning While Doing: Reinforcement Learning and Bayesian Optimisation for Online Continuous Tuning"},{"content":"Good News! The RL4AA community is happy to announce its Discord server! If you are interested in discussing reinforcement learning applied to accelerators, please join for announcements (e.g. new publications), forum discussions, an open chat, and meeting rooms.\nHope to see you there!\nhttps://discord.gg/QtBMqsjWH2\n","permalink":"https://RL4aa.github.io/posts/announcements/discord_server/","summary":"Good News! The RL4AA community is happy to announce its Discord server! If you are interested in discussing reinforcement learning applied to accelerators, please join for announcements (e.g. new publications), forum discussions, an open chat, and meeting rooms.\nHope to see you there!\nhttps://discord.gg/QtBMqsjWH2","title":"The RL4AA Discord server is up!"},{"content":"X. Chen, X. Qi, C. Su, Y. He, Z. Wang, K. Sun, C. Jin, W. Chen, S. Liu, X. Zhao, D. Jia, M. Yi\nChinese Academy of Sciences\narXiv\nAbstract The superconducting linear accelerator is a highly flexiable facility for modern scientific discoveries, necessitating weekly reconfiguration and tuning. Accordingly, minimizing setup time proves essential in affording users with ample experimental time. We propose a trend-based soft actor-critic(TBSAC) beam control method with strong robustness, allowing the agents to be trained in a simulated environment and applied to the real accelerator directly with zero-shot. To validate the effectiveness of our method, two different typical beam control tasks were performed on China Accelerator Facility for Superheavy Elements (CAFe II) and a light particle injector(LPI) respectively. The orbit correction tasks were performed in three cryomodules in CAFe II seperately, the time required for tuning has been reduced to one-tenth of that needed by human experts, and the RMS values of the corrected orbit were all less than 1mm. The other transmission efficiency optimization task was conducted in the LPI, our agent successfully optimized the transmission efficiency of radio-frequency quadrupole(RFQ) to over 85% within 2 minutes. The outcomes of these two experiments offer substantiation that our proposed TBSAC approach can efficiently and effectively accomplish beam commissioning tasks while upholding the same standard as skilled human experts. As such, our method exhibits potential for future applications in other accelerator commissioning fields.\nRead the paper: https://arxiv.org/abs/2305.13869\nContact: Xin Qi, Zhijun Wang\n","permalink":"https://RL4aa.github.io/posts/publications/chen2023trendbased/","summary":"X. Chen, X. Qi, C. Su, Y. He, Z. Wang, K. Sun, C. Jin, W. Chen, S. Liu, X. Zhao, D. Jia, M. Yi\nChinese Academy of Sciences\narXiv\nAbstract The superconducting linear accelerator is a highly flexiable facility for modern scientific discoveries, necessitating weekly reconfiguration and tuning. Accordingly, minimizing setup time proves essential in affording users with ample experimental time. We propose a trend-based soft actor-critic(TBSAC) beam control method with strong robustness, allowing the agents to be trained in a simulated environment and applied to the real accelerator directly with zero-shot.","title":"Trend-Based SAC Beam Control Method with Zero-Shot in Superconducting Linear Accelerator"},{"content":"Good News! Our first workshop, RL4AA 2023, was very successful. Because of this we are hoping to hold the 2nd RL4AA workshop in spring 2024 in Salzburg, Austria.\nSTAY TUNED! \u0026hellip; for further details.\n","permalink":"https://RL4aa.github.io/posts/announcements/rl4aa24_stay_tuned/","summary":"Good News! Our first workshop, RL4AA 2023, was very successful. Because of this we are hoping to hold the 2nd RL4AA workshop in spring 2024 in Salzburg, Austria.\nSTAY TUNED! \u0026hellip; for further details.","title":"The 2nd RL4AA workshop 2024 is being planned - Stay tuned!"},{"content":"F. M. Velotti1, B. Goddard1, V. Kain1, R. Ramjiawan1, G. Z. Della Porta1 and S. Hirlaender2\n1CERN, 2University of Salzburg\nMachine Learning: Science and Technology\nAbstract To improve the performance-critical stability and brightness of the electron bunch at injection into the proton-driven plasma wakefield at the AWAKE CERN experiment, automation approaches based on unsupervised machine learning (ML) were developed and deployed. Numerical optimisers were tested together with different model-free reinforcement learning (RL) agents. In order to avoid any bias, RL agents have been trained also using a completely unsupervised state encoding using auto-encoders. To aid hyper-parameter selection, a full synthetic model of the beamline was constructed using a variational auto-encoder trained to generate surrogate data from equipment settings. This paper describes the novel approaches based on deep learning and RL to aid the automatic setup of a low energy line, as the one used to deliver beam to the AWAKE facility. The results obtained with the different ML approaches, including automatic unsupervised feature extraction from images using computer vision are presented. The prospects for operational deployment and wider applicability are discussed.\nRead the paper: https://iopscience.iop.org/article/10.1088/2632-2153/acce21\nContact: Francesco Maria Velotti\n","permalink":"https://RL4aa.github.io/posts/publications/velotti2023towards/","summary":"F. M. Velotti1, B. Goddard1, V. Kain1, R. Ramjiawan1, G. Z. Della Porta1 and S. Hirlaender2\n1CERN, 2University of Salzburg\nMachine Learning: Science and Technology\nAbstract To improve the performance-critical stability and brightness of the electron bunch at injection into the proton-driven plasma wakefield at the AWAKE CERN experiment, automation approaches based on unsupervised machine learning (ML) were developed and deployed. Numerical optimisers were tested together with different model-free reinforcement learning (RL) agents.","title":"Towards automatic setup of 18 MeV electron beamline using machine learning"},{"content":"X. Chen, Y. Jia, X. Qi, Z. Wang, Y. He\nChinese Academy of Sciences\nPhysical Review Accelerators and Beams\nAbstract Recently, reinforcement learning (RL) algorithms have been applied to a wide range of control problems in accelerator commissioning. In order to achieve efficient and fast control, these algorithms need to be highly efficient, so as to minimize the online training time. In this paper, we incorporated the beam position monitor trend into the observation space of the twin delayed deep deterministic policy gradient (TD3) algorithm and trained two different structure agents, one based on physical prior knowledge and the other using the original TD3 network architecture. Both of the agents exhibit strong robustness in the simulated environment. The effectiveness of the agent based on physical prior knowledge has been validated in a real accelerator. Results show that the agent can overcome the difference between simulated and real accelerator environments. Once the training is completed in the simulated environment, the agent can be directly applied to the real accelerator without any online training process. The RL agent is deployed to the medium energy beam transport section of China Accelerator Facility for Superheavy Elements. Fast and automatic orbit correction is being tested with up to ten degrees of freedom. The experimental results show that the agents can correct the orbit to within 1 mm. Moreover, due to the strong robustness of the agent, when a trained agent is applied to different lattices of different particles, the orbit correction can still be completed. Since there are no online data collection and training processes, all online corrections are done within 30 s. This paper shows that, as long as the robustness of the RL algorithm is sufficient, the offline learning agents can be directly applied to online correction, which will greatly improve the efficiency of orbit correction. Such an approach to RL may find promising applications in other areas of accelerator commissioning.\nRead the paper: https://journals.aps.org/prab/abstract/10.1103/PhysRevAccelBeams.26.044601\nContact: Xin Qi, Zhijun Wang\n","permalink":"https://RL4aa.github.io/posts/publications/chen2023orbit/","summary":"X. Chen, Y. Jia, X. Qi, Z. Wang, Y. He\nChinese Academy of Sciences\nPhysical Review Accelerators and Beams\nAbstract Recently, reinforcement learning (RL) algorithms have been applied to a wide range of control problems in accelerator commissioning. In order to achieve efficient and fast control, these algorithms need to be highly efficient, so as to minimize the online training time. In this paper, we incorporated the beam position monitor trend into the observation space of the twin delayed deep deterministic policy gradient (TD3) algorithm and trained two different structure agents, one based on physical prior knowledge and the other using the original TD3 network architecture.","title":"Orbit Correction Based on Improved Reinforcement Learning Algorithm"},{"content":"Reinforcement learning is the most difficult learning paradigms to understand and to efficiently use, but it holds a lot of promise in the field of accelerator physics. The applications of reinforcement learning to accelerators today are not very numerous yet, but the interest of the community is growing considerably. This is how the 1st collaboration workshop on Reinforcement Learning for Autonomous Accelerators (RL4AA'23) came to be! The AI4Accelerators team organized and hosted the workshop at KIT, gathering colleagues involved in reinforcement learning. The workshop offered introductory lectures to reinforcement learning, a Python tutorial that studied the real deployment of such an algorithm in a real accelerator, and guided discussion sessions on the most pressing topics. The contents of the discussion will be published in the form of proceedings later.\nMost importantly, this the RL4AA’23 event was the official launch of the Reinforcement Learning for Autonomous Accelerators collaboration (RL4AA), which aims to put researchers in contact, create and share valuable resources, and be the spark for interesting ideas and projects together.\nLinks to the workshop Indico page of the workshop. Github repository for the hands-on tutorial. Some event photos ","permalink":"https://RL4aa.github.io/posts/announcements/rl4aa23/","summary":"Reinforcement learning is the most difficult learning paradigms to understand and to efficiently use, but it holds a lot of promise in the field of accelerator physics. The applications of reinforcement learning to accelerators today are not very numerous yet, but the interest of the community is growing considerably. This is how the 1st collaboration workshop on Reinforcement Learning for Autonomous Accelerators (RL4AA'23) came to be! The AI4Accelerators team organized and hosted the workshop at KIT, gathering colleagues involved in reinforcement learning.","title":"RL4AA'23: 1st Collaboration Workshop on Reinforcement Learning for Autonomous Accelerators"},{"content":"D. Meier1, L. V. Ramirez1, J. Völker1, J. Viefhaus1, B. Sick2, G. Hartmann1\n1Helmholtz-Zentrum Berlin, 2University of Kassel\nPhysical Review Accelerators and Beams\nAbstract Superconducting photoelectron injectors are promising for generating highly brilliant pulsed electron beams with high repetition rates and low emittances. Experiments such as ultrafast electron diffraction, experiments at the Terahertz scale, and energy recovery linac applications require such properties. However, optimizing the beam properties is challenging due to the high number of possible machine parameter combinations. This article shows the successful automated optimization of beam properties utilizing an already existing simulation model. To reduce the required computation time, we replace the costly simulation with a faster approximation with a neural network. For optimization, we propose a reinforcement learning approach leveraging the simple computation of the derivative of the approximation. We prove that our approach outperforms standard optimization methods for the required function evaluations given a defined minimum accuracy.\nRead the paper: https://doi.org/10.1103/PhysRevAccelBeams.25.104604\nContact: -\n","permalink":"https://RL4aa.github.io/posts/publications/meier2022optimizing/","summary":"D. Meier1, L. V. Ramirez1, J. Völker1, J. Viefhaus1, B. Sick2, G. Hartmann1\n1Helmholtz-Zentrum Berlin, 2University of Kassel\nPhysical Review Accelerators and Beams\nAbstract Superconducting photoelectron injectors are promising for generating highly brilliant pulsed electron beams with high repetition rates and low emittances. Experiments such as ultrafast electron diffraction, experiments at the Terahertz scale, and energy recovery linac applications require such properties. However, optimizing the beam properties is challenging due to the high number of possible machine parameter combinations.","title":"Optimizing a superconducting radio-frequency gun using deep reinforcement learning"},{"content":"L. Grech1, G. Valentino1, D. Alves2 and Simon Hirlaender3\n1University of Malta, 2CERN, 3University of Salzburg\nFrontiers in Physics\nAbstract The Beam-Based Feedback System (BBFS) was primarily responsible for correcting the beam energy, orbit and tune in the CERN Large Hadron Collider (LHC). A major code renovation of the BBFS was planned and carried out during the LHC Long Shutdown 2 (LS2). This work consists of an explorative study to solve a beam-based control problem, the tune feedback (QFB), utilising state-of-the-art Reinforcement Learning (RL). A simulation environment was created to mimic the operation of the QFB. A series of RL agents were trained, and the best-performing agents were then subjected to a set of well-designed tests. The original feedback controller used in the QFB was reimplemented to compare the performance of the classical approach to the performance of selected RL agents in the test scenarios. Results from the simulated environment show that the RL agent performance can exceed the controller-based paradigm.\nRead the paper: https://www.frontiersin.org/articles/10.3389/fphy.2022.929064/full\nContact: Leander Grech\n","permalink":"https://RL4aa.github.io/posts/publications/grech2022application/","summary":"L. Grech1, G. Valentino1, D. Alves2 and Simon Hirlaender3\n1University of Malta, 2CERN, 3University of Salzburg\nFrontiers in Physics\nAbstract The Beam-Based Feedback System (BBFS) was primarily responsible for correcting the beam energy, orbit and tune in the CERN Large Hadron Collider (LHC). A major code renovation of the BBFS was planned and carried out during the LHC Long Shutdown 2 (LS2). This work consists of an explorative study to solve a beam-based control problem, the tune feedback (QFB), utilising state-of-the-art Reinforcement Learning (RL).","title":"Application of reinforcement learning in the LHC tune feedback"},{"content":"J. Kaiser, O. Stein, A. Eichler\nDeutsches Elektronen-Synchrotron DESY\n39th International Conference on Machine Learning\nAbstract In recent work, it has been shown that reinforcement learning (RL) is capable of solving a variety of problems at sometimes super-human performance levels. But despite continued advances in the field, applying RL to complex real-world control and optimisation problems has proven difficult. In this contribution, we demonstrate how to successfully apply RL to the optimisation of a highly complex real-world machine – specifically a linear particle accelerator – in an only partially observable setting and without requiring training on the real machine. Our method outperforms conventional optimisation algorithms in both the achieved result and time taken as well as already achieving close to human-level performance. We expect that such automation of machine optimisation will push the limits of operability, increase machine availability and lead to a paradigm shift in how such machines are operated, ultimately facilitating advances in a variety of fields, such as science and medicine among many others.\nRead the paper: https://proceedings.mlr.press/v162/kaiser22a.html\nContact: Jan Kaiser\n","permalink":"https://RL4aa.github.io/posts/publications/kaiser2022learningbased/","summary":"J. Kaiser, O. Stein, A. Eichler\nDeutsches Elektronen-Synchrotron DESY\n39th International Conference on Machine Learning\nAbstract In recent work, it has been shown that reinforcement learning (RL) is capable of solving a variety of problems at sometimes super-human performance levels. But despite continued advances in the field, applying RL to complex real-world control and optimisation problems has proven difficult. In this contribution, we demonstrate how to successfully apply RL to the optimisation of a highly complex real-world machine – specifically a linear particle accelerator – in an only partially observable setting and without requiring training on the real machine.","title":"Learning-based Optimisation of Particle Accelerators Under Partial Observability Without Real-World Training"},{"content":"N. Madysa, V. Kain, R. Alemany Fernandez, N. Biancacci, B. Goddard, F. M. Velotti\nCERN\n13th Particle Accelerator Conference\nAbstract High intensities in the Low Energy Ion Ring (LEIR) at CERN are achieved by stacking several multi-turn injec- tions from the pre-accelerator Linac3. Up to seven consec- utive 200 μs long, 200 ms spaced pulses are injected from Linac3 into LEIR. Two inclined septa, one magnetic and one electrostatic, combined with a collapsing horizontal or- bit bump allows a 6-D phase space painting via a linearly ramped mean momentum along the Linac3 pulse and in- jection at high dispersion. The already circulating beam is cooled and dragged longitudinally via electron cooling (e- cooling) into a stacking momentum to free space for the fol- lowing injections. For optimal intensity accumulation, the electron energy and trajectory need to match the ion energy and orbit at the e-cooler section.\nIn this paper, a reinforcement learning (RL) agent is trained to adjust various e-cooler and Linac3 parameters to maximise the intensity at the end of the injection plateau. Variational Auto-Encoders (VAE) are used to compress lon- gitudinal Schottky spectra into a compact latent space rep- resentation as state input for the RL agent. The RL agent is pre-trained on a surrogate model of the LEIR e-cooling dynamics, which in turn is learned from the data collected for the training of the VAE. The performance of the VAE, the surrogate model, and the RL agent is investigated in this paper. An overview of planned tests in the upcoming LEIR runs is given.\nRead the paper: https://jacow.org/ipac2022/papers/tupost040.pdf\nContact: Nico Madysa\n","permalink":"https://RL4aa.github.io/posts/publications/madysa2022automated/","summary":"N. Madysa, V. Kain, R. Alemany Fernandez, N. Biancacci, B. Goddard, F. M. Velotti\nCERN\n13th Particle Accelerator Conference\nAbstract High intensities in the Low Energy Ion Ring (LEIR) at CERN are achieved by stacking several multi-turn injec- tions from the pre-accelerator Linac3. Up to seven consec- utive 200 μs long, 200 ms spaced pulses are injected from Linac3 into LEIR. Two inclined septa, one magnetic and one electrostatic, combined with a collapsing horizontal or- bit bump allows a 6-D phase space painting via a linearly ramped mean momentum along the Linac3 pulse and in- jection at high dispersion.","title":"Automated Intensity Optimisation Using Reinforcement Learning at LEIR"},{"content":"J. St. John1, C. Herwig1, D. Kafkes1, J. Mitrevski1, W. A. Pellico1, G. N. Perdue1, A. Quintero-Parra1, B. A. Schupbach1, K. Seiya1, N. Tran1, M. Schram2, J. M. Duarte3, Y. Huang4, R. Keller5\n1Fermi National Accelerator Laboratory, 2Thomas Jefferson National Accelerator Laboratory, 3University of California San Diego, 4Pacific Northwest National Laboratory, 5Columbia University\nPhysical Review Accelerators and Beams\nAbstract We describe a method for precisely regulating the gradient magnet power supply (GMPS) at the Fermilab Booster accelerator complex using a neural network trained via reinforcement learning. We demonstrate preliminary results by training a surrogate machine-learning model on real accelerator data to emulate the GMPS, and using this surrogate model in turn to train the neural network for its regulation task. We additionally show how the neural networks to be deployed for control purposes may be compiled to execute on field-programmable gate arrays (FPGAs), and show the first machine-learning based control algorithm implemented on an FPGA for controls at the Fermilab accelerator complex. As there are no surprise latencies on an FPGA, this capability is important for operational stability in complicated environments such as an accelerator facility.\nRead the paper: https://journals.aps.org/prab/abstract/10.1103/PhysRevAccelBeams.24.104601\nContact: Jason St. John\n","permalink":"https://RL4aa.github.io/posts/publications/stjohn2021realtime/","summary":"J. St. John1, C. Herwig1, D. Kafkes1, J. Mitrevski1, W. A. Pellico1, G. N. Perdue1, A. Quintero-Parra1, B. A. Schupbach1, K. Seiya1, N. Tran1, M. Schram2, J. M. Duarte3, Y. Huang4, R. Keller5\n1Fermi National Accelerator Laboratory, 2Thomas Jefferson National Accelerator Laboratory, 3University of California San Diego, 4Pacific Northwest National Laboratory, 5Columbia University\nPhysical Review Accelerators and Beams\nAbstract We describe a method for precisely regulating the gradient magnet power supply (GMPS) at the Fermilab Booster accelerator complex using a neural network trained via reinforcement learning.","title":"Real-time artificial intelligence for accelerator control: A study at the Fermilab Booster"},{"content":"V. Kain1, N. Bruchon1, S. Hirlander1, N. Madysa1, I. Vojskovic1, P. Skowronski1, G. Valentino2\n1CERN, 2University of Malta\n61st ICFA ABDW on High-Intensity and High-Brightness Hadron Beams\nAbstract The CERN H−linear accelerator, LINAC4, served as atest bed for advanced algorithms during the CERN LongShutdown 2 in the years 2019/20. One of the main goals wasto show that reinforcement learning with all its benefits canbe used as a replacement for numerical optimization and asa complement to classical control in the accelerator controlcontext. Many of the algorithms used were prepared before-hand at the electron line of the AWAKE facility to makethe best use of the limited time available at LINAC4. Anoverview of the algorithms and concepts tested at LINAC4and AWAKE will be given and the results discussed.\nRead the paper: https://jacow.org/hb2021/papers/tuec4.pdf\nContact: Verena Kain\n","permalink":"https://RL4aa.github.io/posts/publications/kain2021test/","summary":"V. Kain1, N. Bruchon1, S. Hirlander1, N. Madysa1, I. Vojskovic1, P. Skowronski1, G. Valentino2\n1CERN, 2University of Malta\n61st ICFA ABDW on High-Intensity and High-Brightness Hadron Beams\nAbstract The CERN H−linear accelerator, LINAC4, served as atest bed for advanced algorithms during the CERN LongShutdown 2 in the years 2019/20. One of the main goals wasto show that reinforcement learning with all its benefits canbe used as a replacement for numerical optimization and asa complement to classical control in the accelerator controlcontext.","title":"Test of Machine Learning at the CERN LINAC4"},{"content":"L. Grech\nUniversity of Malta\nPhD thesis\nAbstract The Large Hadron Collider (LHC) at the European Organization for Nuclear Research (CERN) is the largest synchrotron built to date, having a circumference of approx- imately 27km. The LHC is able to accelerate two counter-rotating proton and/or heavy-ion beams up to 7 TeV per charge. These highly energetic beams are contained inside a vacuum chamber with an inner diameter of 80 mm by means of strong mag- netic fields produced by superconducting magnets. A beam cleaning and machine protection system is in place to prevent high-energy halo particles from impacting and heating the superconducting magnets.\nDue to the tight tolerances on the beam trajectory imposed by the beam cleaning and machine protection system, the LHC was the first accelerator to require automatic beam-based feedback control. Until the LHC Run 2, this was implemented by the Beam-Based Feedback System (BBFS) which was mainly responsible for collecting beam measurements from various types of beam instrumentation and calculating the corrections in the current of corrector magnets throughout the whole length of the LHC.\nThroughout the years of LHC operation, some of the original BBFS functionality became deprecated and fell into disuse. Various minor upgrades were performed, how- ever, without a systematic redesign of the BBFS the code became difficult to maintain. The BBFS also suffered from a computational bottleneck during the calculation of the optics matrices used by the BBFS controllers. A feasibility study was performed to assess whether Hardware Acceleration (HA) in the form of Graphical Processing Units (GPUs) would improve the performance. It was found that the use of GPUs was not necessary and as a result the hardware upgrade for the BBFS could be finalised. Following this was the redesign and implementation of a more streamlined BBFS called BFCLHC. This work was done in collaboration with BE-OP-LHC and was designed with a more intuitive interface. A new feature called Function Players was imple- mented at the request of the LHC operators, which allows certain BFCLHC settings to be automated.\nThe Base-Band Q (tune) (BBQ) system is responsible for estimating the values of the horizontal and vertical tunes in both beams of the LHC. It was found that noise harmonics were perturbing the tune estimates and consequently causing the Tune Feedback (QFB) within the BBFS to behave erroneously. In this work, new tune estimation algorithms were developed to improve upon the accuracy of the BBQ system. To aid with this development, a simulation procedure was set up which mimics the frequency spectra obtained by the BBQ system. Two algorithmic approaches were proposed which take into consideration the location of the noise harmonics. A data-driven approach was also attempted where a combination of simulated and real frequency spectra were used to train a neural network to predict the value of the tune from a BBQ spectrum. It was observed that the data-driven approach improves upon the stability of the tune estimates in the presence of noise harmonics.\nThe use of Reinforcement Learning (RL) in beam-based feedback control systems was considered since it offers the possibility of optimal control, regardless of any dy- namical changes in the machine. Since this work was performed during the LHC Long Shutdown 2 (LS2), the control problem present in the BBFS was formulated in a sim- ulation environment called QFBEnv that allowed RL agents to be trained offline. The linear beam optics models used in the standard Proportional-Integral (PI) controllers of the BBFS were used to simulate the response of the LHC. In particular, the perfor- mance of several RL agents in the task of tune correction were assessed in detail and run through different failure scenarios. All the tests performed showed that certain RL agents can achieve a better performance than a standard PI controller, also in the presence of actuator failures. The possibility of using RL in the orbit feedback was also considered and discussed.\nRead the paper: https://www.um.edu.mt/library/oar/handle/123456789/104427\nContact: Leander Grech\n","permalink":"https://RL4aa.github.io/posts/publications/grech2021renovation/","summary":"L. Grech\nUniversity of Malta\nPhD thesis\nAbstract The Large Hadron Collider (LHC) at the European Organization for Nuclear Research (CERN) is the largest synchrotron built to date, having a circumference of approx- imately 27km. The LHC is able to accelerate two counter-rotating proton and/or heavy-ion beams up to 7 TeV per charge. These highly energetic beams are contained inside a vacuum chamber with an inner diameter of 80 mm by means of strong mag- netic fields produced by superconducting magnets.","title":"Renovation of the beam-based feedback systems in the LHC"},{"content":"A. Eichler1, F. Burkart1, J. Kaiser1, W. Kuropka1, O. Stein1, E. Bründermann2, A. Santamaria Garcia2, C. Xu2\n1Deutsches Elektronen-Synchrotron DESY, 2Karlsruhe Institute of Technology KIT\n12th International Particle Accelerator Conference\nAbstract Reinforcement learning algorithms have risen in pop-ularity in the accelerator physics community in recentyears, showing potential in beam control and in the opti-mization and automation of tasks in accelerator operation.The Helmholtz AI project “Machine Learning Toward Au-tonomous Accelerators” is a collaboration between DESYand KIT that works on investigating and developing rein-forcement learning applications for the automatic start-upof electron linear accelerators. The work is carried out inparallel at two similar research accelerators: ARES at DESYand FLUTE at KIT, giving the unique opportunity of trans-fer learning between facilities. One of the first steps of thisproject is the establishment of a common interface betweenthe simulations and the machine, in order to test and applyvarious optimization approaches interchangeably betweenthe two accelerators. In this paper we present first results onthe common interface and its application to beam focusingin ARES as well as the idea of laser shaping with spatiallight modulators at FLUTE.\nRead the paper: https://jacow.org/ipac2021/papers/tupab298.pdf\nContact: Annika Eichler\n","permalink":"https://RL4aa.github.io/posts/publications/eichler2021first/","summary":"A. Eichler1, F. Burkart1, J. Kaiser1, W. Kuropka1, O. Stein1, E. Bründermann2, A. Santamaria Garcia2, C. Xu2\n1Deutsches Elektronen-Synchrotron DESY, 2Karlsruhe Institute of Technology KIT\n12th International Particle Accelerator Conference\nAbstract Reinforcement learning algorithms have risen in pop-ularity in the accelerator physics community in recentyears, showing potential in beam control and in the opti-mization and automation of tasks in accelerator operation.The Helmholtz AI project “Machine Learning Toward Au-tonomous Accelerators” is a collaboration between DESYand KIT that works on investigating and developing rein-forcement learning applications for the automatic start-upof electron linear accelerators.","title":"First Steps Toward an Autonomous Accelerator, A Common Project Between DESY and KIT"},{"content":"A. Ivanov, I. Agapov, A. Eichler, S. Tomin\nDeutsches Elektronen Synchrotron DESY\n12th International Particle Accelerator Conference\nAbstract We propose an approach for incorporating acceleratorphysics models into reinforcement learning agents. The proposed approach is based on the Taylor mapping technique for the simulation of particle dynamics. The resulting computational graph is represented as a polynomial neural network and embedded into the traditional reinforcement learning agents. The application of the model is demonstrated in a nonlinear simulation model of beam transmission. The comparison of the approach with the traditional numerical optimization as well as neural networks-based agents demonstrates better convergence of the proposed technique.\nRead the paper: https://jacow.org/ipac2021/papers/thpab191.pdf\nContact: Andrei Ivanov\n","permalink":"https://RL4aa.github.io/posts/publications/ivanov2021physicsenhanced/","summary":"A. Ivanov, I. Agapov, A. Eichler, S. Tomin\nDeutsches Elektronen Synchrotron DESY\n12th International Particle Accelerator Conference\nAbstract We propose an approach for incorporating acceleratorphysics models into reinforcement learning agents. The proposed approach is based on the Taylor mapping technique for the simulation of particle dynamics. The resulting computational graph is represented as a polynomial neural network and embedded into the traditional reinforcement learning agents. The application of the model is demonstrated in a nonlinear simulation model of beam transmission.","title":"Physics-Enhanced Reinforcement Learning for Optimal Control"},{"content":"N. Bruchon\nUniversity of Trieste\nPhD thesis\nAbstract The research carried out in particle accelerator facilities does not concern only particle and condensed matter physics, although these are the main topics covered in the field. Indeed, since a particle accelerator is composed of many different sub-systems, its proper functioning depends both on each of these parts and their interconnection. It follows that the study, implementation, and improvement of the various sub-systems are fundamental points of investigation too. In particular, an interesting aspect for the automation engineering community is the control of such systems that usually are complex, large, noise-affected, and non-linear.\nThe doctoral project fits into this scope, investigating the introduction of new methods to automatically improve the performance of a specific type of particle accelerators: seeded free-electron lasers. The optimization of such systems is a challenging task, already faced in years by many different approaches in order to find and attain an optimal working point, keeping it optimally tuned despite drift or disturbances. Despite the good results achieved, better ones are always sought for. For this reason, several methods belonging to reinforcement learning, an area of machine learning that is attracting more and more attention in the scientific field, have been applied on FERMI, the free-electron laser facility at Elettra Sincrotrone Trieste. The research activity has been carried out by applying both model-free and model-based techniques belonging to reinforcement learning. Satisfactory preliminary results have been obtained, that present the first step toward a new fully automatic procedure for the alignment of the seed laser to the electron beam.\nIn the meantime, at the Conseil Européen pour la Recherche Nucléaire, CERN, a similar investigation was ongoing. In the last year of the doctoral course, a collaboration to share the knowledge on the topic took place. Some of the results collected on the largest particle physics laboratory in the world are presented in the doctoral dissertation.\nRead the paper: https://arts.units.it/handle/11368/2982117\nContact: Niky Bruchon\n","permalink":"https://RL4aa.github.io/posts/publications/bruchon2021feasiblity/","summary":"N. Bruchon\nUniversity of Trieste\nPhD thesis\nAbstract The research carried out in particle accelerator facilities does not concern only particle and condensed matter physics, although these are the main topics covered in the field. Indeed, since a particle accelerator is composed of many different sub-systems, its proper functioning depends both on each of these parts and their interconnection. It follows that the study, implementation, and improvement of the various sub-systems are fundamental points of investigation too.","title":"Feasibility Investigation on Several Reinforcement Learning Techniques to Improve the Performance of the FERMI Free-Electron Laser"},{"content":"F. H. O\u0026rsquo;Shea1, N. Bruchon2, G. Gaio1\n1Elettra Sincrotrone Trieste, 2University of Trieste\nPhysical Review Accelerators and Beams\nAbstract In this article we report on the application of a model-free reinforcement learning method to the optimization of accelerator systems. We simplify a policy gradient algorithm to accelerator control from sophisticated algorithms that have recently been demonstrated to solve complex dynamic problems. After outlining a theoretical basis for the functioning of the algorithm, we explore the small hyperparameter space to develop intuition about said parameters using a simple number-guess environment. Finally, we demonstrate the algorithm optimizing both a free-electron laser and an accelerator-based terahertz source in-situ. The algorithm is applied to different accelerator control systems and optimizes the desired signals in a few hundred steps without any domain knowledge using up to five control parameters. In addition, the algorithm shows modest tolerance to accelerator fault conditions without any special preparation for such conditions.\nRead the paper: https://journals.aps.org/prab/abstract/10.1103/PhysRevAccelBeams.23.122802\nContact: Finn H. O\u0026rsquo;Shea\n","permalink":"https://RL4aa.github.io/posts/publications/oshea202policy/","summary":"F. H. O\u0026rsquo;Shea1, N. Bruchon2, G. Gaio1\n1Elettra Sincrotrone Trieste, 2University of Trieste\nPhysical Review Accelerators and Beams\nAbstract In this article we report on the application of a model-free reinforcement learning method to the optimization of accelerator systems. We simplify a policy gradient algorithm to accelerator control from sophisticated algorithms that have recently been demonstrated to solve complex dynamic problems. After outlining a theoretical basis for the functioning of the algorithm, we explore the small hyperparameter space to develop intuition about said parameters using a simple number-guess environment.","title":"Policy gradient methods for free-electron laser and terahertz source optimization and stabilization at the FERMI free-electron laser at Elettra"},{"content":"S. Hirlaender, N. Bruchon\nUniversity of Salzburg, University of Trieste\narXiv\nAbstract Reinforcement learning holds tremendous promise in accelerator controls. The primary goal of this paper is to show how this approach can be utilised on an operational level on accelerator physics problems. Despite the success of model-free reinforcement learning in several domains, sample-efficiency still is a bottle-neck, which might be encompassed by model-based methods. We compare well-suited purely model-based to model-free reinforcement learning applied to the intensity optimisation on the FERMI FEL system. We find that the model-based approach demonstrates higher representational power and sample-efficiency, while the asymptotic performance of the model-free method is slightly superior. The model-based algorithm is implemented in a DYNA-style using an uncertainty aware model, and the model-free algorithm is based on tailored deep Q-learning. In both cases, the algorithms were implemented in a way, which presents increased noise robustness as omnipresent in accelerator control problems.\nRead the paper: https://arxiv.org/abs/2012.09737\nContact: Simon Hirlaender\n","permalink":"https://RL4aa.github.io/posts/publications/hirlaender2020modelfree/","summary":"S. Hirlaender, N. Bruchon\nUniversity of Salzburg, University of Trieste\narXiv\nAbstract Reinforcement learning holds tremendous promise in accelerator controls. The primary goal of this paper is to show how this approach can be utilised on an operational level on accelerator physics problems. Despite the success of model-free reinforcement learning in several domains, sample-efficiency still is a bottle-neck, which might be encompassed by model-based methods. We compare well-suited purely model-based to model-free reinforcement learning applied to the intensity optimisation on the FERMI FEL system.","title":"Model-free and Bayesian Ensembling Model-based Deep Reinforcement Learning for Particle Accelerator Control Demonstrated on the FERMI FEL"},{"content":"X. Pang1, S. Thulasidasan2, L. Rybarcyk2\n1Apple, 2Los Alamos National Laboratory\nMachine Learning for Engineering Modeling, Simulation, and Design Workshop at Neural Information Processing Systems 2020\nAbstract We describe an approach to learning optimal control policies for a large, linear particle accelerator using deep reinforcement learning coupled with a high-fidelity physics engine. The framework consists of an AI controller that uses deep neural networks for state and action-space representation and learns optimal policies using reward signals that are provided by the physics simulator. For this work, we only focus on controlling a small section of the entire accelerator. Nevertheless, initial results indicate that we can achieve better-than-human level performance in terms of particle beam current and distribution. The ultimate goal of this line of work is to substantially reduce the tuning time for such facilities by orders of magnitude, and achieve near-autonomous control.\nRead the paper: https://ml4eng.github.io/camera_readys/58.pdf\nContact: Xiaoying Pang\n","permalink":"https://RL4aa.github.io/posts/publications/pang2020autonomous/","summary":"X. Pang1, S. Thulasidasan2, L. Rybarcyk2\n1Apple, 2Los Alamos National Laboratory\nMachine Learning for Engineering Modeling, Simulation, and Design Workshop at Neural Information Processing Systems 2020\nAbstract We describe an approach to learning optimal control policies for a large, linear particle accelerator using deep reinforcement learning coupled with a high-fidelity physics engine. The framework consists of an AI controller that uses deep neural networks for state and action-space representation and learns optimal policies using reward signals that are provided by the physics simulator.","title":"Autonomous Control of a Particle Accelerator using Deep Reinforcement Learning"},{"content":"V. Kain1, S. Hirlander1, B. Goddard1, F. M. Velotti1, G. Z. Della Porta1, N. Bruchon2, G. Valentino3\n1CERN, 2University of Trieste, 3University of Malta\nPhysical Review Accelerators and Beams\nAbstract Numerical optimization algorithms are already established tools to increase and stabilize the performance of particle accelerators. These algorithms have many advantages, are available out of the box, and can be adapted to a wide range of optimization problems in accelerator operation. The next boost in efficiency is expected to come from reinforcement learning algorithms that learn the optimal policy for a certain control problem and hence, once trained, can do without the time-consuming exploration phase needed for numerical optimizers. To investigate this approach, continuous model-free reinforcement learning with up to 16 degrees of freedom was developed and successfully tested at various facilities at CERN. The approach and algorithms used are discussed and the results obtained for trajectory steering at the AWAKE electron line and LINAC4 are presented. The necessary next steps, such as uncertainty aware model-based approaches, and the potential for future applications at particle accelerators are addressed.\nRead the paper: https://doi.org/10.1103/PhysRevAccelBeams.23.124801\nContact: Verena Kain\n","permalink":"https://RL4aa.github.io/posts/publications/kain2020sampleefficient/","summary":"V. Kain1, S. Hirlander1, B. Goddard1, F. M. Velotti1, G. Z. Della Porta1, N. Bruchon2, G. Valentino3\n1CERN, 2University of Trieste, 3University of Malta\nPhysical Review Accelerators and Beams\nAbstract Numerical optimization algorithms are already established tools to increase and stabilize the performance of particle accelerators. These algorithms have many advantages, are available out of the box, and can be adapted to a wide range of optimization problems in accelerator operation.","title":"Sample-efficient reinforcement learning for CERN accelerator control"},{"content":"N. Bruchon1, G. Fenu1, G. Gaio2, M. Lonza2, F. H. O’Shea2, F. A. Pellegrino1, E. Salvato1\n1University of Trieste, 2Elettra Sincrotrone Trieste\nElectronics\nAbstract Optimal tuning of particle accelerators is a challenging task. Many different approaches have been proposed in the past to solve two main problems—attainment of an optimal working point and performance recovery after machine drifts. The most classical model-free techniques (e.g., Gradient Ascent or Extremum Seeking algorithms) have some intrinsic limitations. To overcome those limitations, Machine Learning tools, in particular Reinforcement Learning (RL), are attracting more and more attention in the particle accelerator community. We investigate the feasibility of RL model-free approaches to align the seed laser, as well as other service lasers, at FERMI, the free-electron laser facility at Elettra Sincrotrone Trieste. We apply two different techniques—the first, based on the episodic Q-learning with linear function approximation, for performance optimization; the second, based on the continuous Natural Policy Gradient REINFORCE algorithm, for performance recovery. Despite the simplicity of these approaches, we report satisfactory preliminary results, that represent the first step toward a new fully automatic procedure for the alignment of the seed laser to the electron beam. Such an alignment is, at present, performed manually.\nRead the paper: https://www.mdpi.com/2079-9292/9/5/781\nContact: Niky Bruchon\n","permalink":"https://RL4aa.github.io/posts/publications/bruchon2020basic/","summary":"N. Bruchon1, G. Fenu1, G. Gaio2, M. Lonza2, F. H. O’Shea2, F. A. Pellegrino1, E. Salvato1\n1University of Trieste, 2Elettra Sincrotrone Trieste\nElectronics\nAbstract Optimal tuning of particle accelerators is a challenging task. Many different approaches have been proposed in the past to solve two main problems—attainment of an optimal working point and performance recovery after machine drifts. The most classical model-free techniques (e.g., Gradient Ascent or Extremum Seeking algorithms) have some intrinsic limitations.","title":"Basic Reinforcement Learning Techniques to Control the Intensity of a Seeded Free-Electron Laser"},{"content":"N. Bruchon, G. Fenu, G. Gaio, M. Lonza, F. A. Pellegrino, E. Salvato\nUniversity of Trieste\n23rd International Conference on Mechatronics Technology\nAbstract The optimization of particle accelerators is a challenging task, and many different approaches have been proposed in years, to obtain an optimal tuning of the plant and to keep it optimally tuned despite drifts or disturbances. Indeed, the classical model-free approaches (such as Gradient Ascent or Extremum Seeking algorithms) have intrinsic limitations. To overcome those limitations, Machine Learning techniques, in particular, the Reinforcement Learning, are attracting more and more attention in the particle accelerator community. The purpose of this paper is to apply a Reinforcement Learning model-free approach to the alignment of a seed laser, based on a rather general target function depending on the laser trajectory. The study focuses on the alignment of the lasers at FERMI, the free-electron laser facility at Elettra Sincrotrone Trieste. In particular, we employ Q-learning with linear function approximation and report experimental results obtained in two setups, which are the actual setups where the final application has to be deployed. Despite the simplicity of the approach, we report satisfactory preliminary results, that represent the first step toward a fully automatic procedure for seed laser to the electron beam. Such a superimposition is, at present, performed manually.\nRead the paper: https://ieeexplore.ieee.org/document/8932150\nContact: Niky Bruchon\n","permalink":"https://RL4aa.github.io/posts/publications/bruchon2019toward/","summary":"N. Bruchon, G. Fenu, G. Gaio, M. Lonza, F. A. Pellegrino, E. Salvato\nUniversity of Trieste\n23rd International Conference on Mechatronics Technology\nAbstract The optimization of particle accelerators is a challenging task, and many different approaches have been proposed in years, to obtain an optimal tuning of the plant and to keep it optimally tuned despite drifts or disturbances. Indeed, the classical model-free approaches (such as Gradient Ascent or Extremum Seeking algorithms) have intrinsic limitations.","title":"Toward the Application of Reinforcement Learning to the Intensity Control of a Seeded Free-Electron Laser"},{"content":"T. Boltz, M. Brosi, E. Bründermann, B. Haerer, P. Kaiser, C. Pohl, P. Schreiber, M. Yan,T. Asfour, A.-S. Müller\nKarlsruhe Insitute of Technology KIT\n10th International Particle Accelerator Conference\nAbstract The operation of ring-based synchrotron light sourceswith short electron bunches increases the emission of co-herent synchrotron radiation (CSR) in the THz frequencyrange. However, the micro-bunching instability resultingfrom self-interaction of the bunch with its own radiationfield limits stable operation with constant intensity of CSRemission to a particular threshold current. Above this thresh-old, the longitudinal charge distribution and thus the emittedradiation vary rapidly and continuously. Therefore, a fastand adaptive feedback system is the appropriate approach tostabilize the dynamics and to overcome the limitations givenby the instability. In this contribution, we discuss first effortstowards a longitudinal feedback design that acts on the RFsystem of the KIT storage ring KARA (Karlsruhe ResearchAccelerator) and aims for stabilization of the emitted THzradiation. Our approach is based on methods of adaptive con-trol that were developed in the field of reinforcement learningand have seen great success in other fields of research overthe past decade. We motivate this particular approach andcomment on different aspects of its implementation.\nRead the paper: http://jacow.org/ipac2019/papers/mopgw017.pdf\nContact: Tobias Boltz\n","permalink":"https://RL4aa.github.io/posts/publications/boltz2019feedback/","summary":"T. Boltz, M. Brosi, E. Bründermann, B. Haerer, P. Kaiser, C. Pohl, P. Schreiber, M. Yan,T. Asfour, A.-S. Müller\nKarlsruhe Insitute of Technology KIT\n10th International Particle Accelerator Conference\nAbstract The operation of ring-based synchrotron light sourceswith short electron bunches increases the emission of co-herent synchrotron radiation (CSR) in the THz frequencyrange. However, the micro-bunching instability resultingfrom self-interaction of the bunch with its own radiationfield limits stable operation with constant intensity of CSRemission to a particular threshold current.","title":"Feedback Design for Control of the Micro-Bunching Instability Based on Reinforcement Learning"},{"content":"A. L. Edelen1, S. G. Biedron2, J. P. Edelen3, S. V. Milton4, P. J. M. van der Slot5\n1Colorado State University, 2Element Aero, 3Fermi National Accelerator Laboratory, 4Los Alamos National Laboratory, 5University of Twente\n38th International Free Electron Laser Conference\nAbstract FEL user facilities often must accommodate requests for a variety of beam parameters. This usually requires skilled operators to tune the machine, reducing the amount of available time for users. In principle, a neural network control policy that is trained on a broad range of operating states could be used to quickly switch between these requests without substantial need for human inter-vention. We present preliminary results from an ongoing study in which a neural network control policy is investi-gated for rapid switching between beam parameters in a compact THz FEL.\nRead the paper: http://jacow.org/fel2017/papers/wep031.pdf\nContact: Auralee Edelen\n","permalink":"https://RL4aa.github.io/posts/publications/edelen2017using/","summary":"A. L. Edelen1, S. G. Biedron2, J. P. Edelen3, S. V. Milton4, P. J. M. van der Slot5\n1Colorado State University, 2Element Aero, 3Fermi National Accelerator Laboratory, 4Los Alamos National Laboratory, 5University of Twente\n38th International Free Electron Laser Conference\nAbstract FEL user facilities often must accommodate requests for a variety of beam parameters. This usually requires skilled operators to tune the machine, reducing the amount of available time for users.","title":"Using a neural network control policy for rapid switching between beam parameters in an FEL"},{"content":"About us Organizers of the 1st RL4AA workshop and the maintainers of the RL4AA collaboration site.\nAndrea Santamaria Garcia Simon Hirländer Jan Kaiser Chenran Xu Contributing We welcome all enthusiastics and practitioner of reinforcement learning for particle acceelrators to contribute to the website. Please visit the Github Repository for more information.\nAnnounce new publications We try to maintain a comprehensive list of the publications in the RL for accelerator field.\nIf you have a recent publication fitting for the topic, or you find any paper missing, you could start an issue and fill in the essential information.\nWe will update the websites accordingling.\nShare your knowledge We also encourage everyone who has worked in this field to share their knowledge and experience in form of blog posts.\nYou could contact us directly or start an issue in the repository.\n","permalink":"https://RL4aa.github.io/contact/","summary":"contact","title":"Contact Information"}]