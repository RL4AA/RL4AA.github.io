<!doctype html><html lang=en dir=auto><head><meta name=generator content="Hugo 0.152.1"><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>RL4AA Collaboration | Homepage</title><meta name=description content="Reinforcement Learning for Autonomous Accelerator Collaboration - https://github.com/RL4AA"><meta name=author content="RL4AA Collaboration"><link rel=canonical href=https://RL4aa.github.io/><link crossorigin=anonymous href=/assets/css/stylesheet.a090830a421002426baafbd314e38f149d77b4c48a12ee9312700d770b27fb26.css integrity="sha256-oJCDCkIQAkJrqvvTFOOPFJ13tMSKEu6TEnANdwsn+yY=" rel="preload stylesheet" as=style><link rel=icon href=https://RL4aa.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://RL4aa.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://RL4aa.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://RL4aa.github.io/apple-touch-icon.png><link rel=mask-icon href=https://RL4aa.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate type=application/rss+xml href=https://RL4aa.github.io/index.xml title=rss><link rel=alternate type=application/json href=https://RL4aa.github.io/index.json title=json><link rel=alternate hreflang=en href=https://RL4aa.github.io/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><meta property="og:url" content="https://RL4aa.github.io/"><meta property="og:site_name" content="RL4AA Collaboration | Homepage"><meta property="og:title" content="RL4AA Collaboration | Homepage"><meta property="og:description" content="Reinforcement Learning for Autonomous Accelerator Collaboration - https://github.com/RL4AA"><meta property="og:locale" content="en-us"><meta property="og:type" content="website"><meta name=twitter:card content="summary"><meta name=twitter:title content="RL4AA Collaboration | Homepage"><meta name=twitter:description content="Reinforcement Learning for Autonomous Accelerator Collaboration - https://github.com/RL4AA"><script type=application/ld+json>{"@context":"https://schema.org","@type":"Organization","name":"RL4AA Collaboration | Homepage","url":"https://RL4aa.github.io/","description":"Reinforcement Learning for Autonomous Accelerator Collaboration - https://github.com/RL4AA","logo":"https://RL4aa.github.io/favicon.ico","sameAs":["https://github.com/RL4AA","https://discord.gg/QtBMqsjWH2"]}</script></head><body class=list id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://RL4aa.github.io/ accesskey=h title="RL4AA Collaboration | Homepage (Alt + H)"><img src=https://RL4aa.github.io/imgs/rl4aa_logo.png alt aria-label=logo height=30>RL4AA Collaboration | Homepage</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://RL4aa.github.io/categories/announcements title=Announcements><span>Announcements</span></a></li><li><a href=https://RL4aa.github.io/categories/publications title=Publications><span>Publications</span></a></li><li><a href=https://RL4aa.github.io/tags/ title=Tags><span>Tags</span></a></li><li><a href=https://RL4aa.github.io/archives/ title=Archive><span>Archive</span></a></li><li><a href=https://RL4aa.github.io/contact/ title=Contact><span>Contact</span></a></li></ul></nav></header><main class=main><article class="first-entry home-info"><header class=entry-header><h1>RL4AA Collaboration</h1></header><div class=entry-content><p>Collaboration on Reinforcement Learning for Autonomous Accelerators</p><p> </p><h2 id=find-your-way-around-the-site>Find your way around the site</h2><ul><li><strong><a href=/categories/announcements>Announcements</a></strong> of new events and information</li><li><strong><a href=/categories/publications/>Publications</a></strong> about RL applied for particle accelerators</li><li><strong><a href=/contact/>Contact</a></strong> us to contribute or collaborate</li><li><strong><a href=/search/>Search</a></strong> for keywords directly</li><li><strong><a href=/archives>Archive</a></strong> to see all the posts</li></ul></div><footer class=entry-footer><div class=social-icons><a href=https://github.com/RL4AA target=_blank rel="noopener noreferrer me" title=Github><svg viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M9 19c-5 1.5-5-2.5-7-3m14 6v-3.87a3.37 3.37.0 00-.94-2.61c3.14-.35 6.44-1.54 6.44-7A5.44 5.44.0 0020 4.77 5.07 5.07.0 0019.91 1S18.73.65 16 2.48a13.38 13.38.0 00-7 0C6.27.65 5.09 1 5.09 1A5.07 5.07.0 005 4.77 5.44 5.44.0 003.5 8.55c0 5.42 3.3 6.61 6.44 7A3.37 3.37.0 009 18.13V22"/></svg>
</a><a href=https://discord.gg/QtBMqsjWH2 target=_blank rel="noopener noreferrer me" title=Discord><svg viewBox="0 0 127.14 96.36" fill="currentColor" stroke="none" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M107.7 8.07A105.15 105.15.0 0081.47.0a72.06 72.06.0 00-3.36 6.83A97.68 97.68.0 0049 6.83 72.37 72.37.0 0045.64.0 105.89 105.89.0 0019.39 8.09C2.79 32.65-1.71 56.6.54 80.21h0A105.73 105.73.0 0032.71 96.36 77.7 77.7.0 0039.6 85.25a68.42 68.42.0 01-10.85-5.18c.91-.66 1.8-1.34 2.66-2a75.57 75.57.0 0064.32.0c.87.71 1.76 1.39 2.66 2a68.68 68.68.0 01-10.87 5.19 77 77 0 006.89 11.1A105.25 105.25.0 00126.6 80.22h0C129.24 52.84 122.09 29.11 107.7 8.07zM42.45 65.69C36.18 65.69 31 60 31 53s5-12.74 11.43-12.74S54 46 53.89 53 48.84 65.69 42.45 65.69zm42.24.0C78.41 65.69 73.25 60 73.25 53s5-12.74 11.44-12.74S96.23 46 96.12 53 91.08 65.69 84.69 65.69z"/></svg></a></div></footer></article><article class=post-entry><figure class=entry-cover><img loading=lazy src=https://RL4aa.github.io/imgs/pexels-wolfgang-weiser-467045605-16916258-small-webp.webp alt="Hamburg Speicherstadt."></figure><header class=entry-header><h2 class=entry-hint-parent>Announcing RL4AA'25 taking place 2 - 4 April 2025 in Hamburg, Germany</h2></header><div class=entry-content><p>We are delighted to announce the “3rd International Workshop on Reinforcement Learning for Autonomous Accelerators” RL4AA’25! To be held 2 - 4 April 2025 at DESY in Hamburg, Germany. Following the very successful RL4AA'23 and RL4AA'24 workshops, the goal of this workshop is to exchange experiences and ideas about RL in the context of particle accelerators amongst both experts and beginners.
We have an exciting workshop program lined up! Two keynotes by RL experts Jan Peters (University of Darmstadt) The second speaker will be announced soon. Hands-on RL challenge Contributed talks Posters Introduction to RL More details: Workshop website: https://rl4aa.github.io/RL4AA25/ Indico link: https://indico.scc.kit.edu/event/4216/ Call for abstracts: open until 24 January 2025 Registration: Open right now! Registration deadline: 7 March 2025 Thanks to generous sponsorships, there are no registration fees! Workshop: 2 - 4 April 2025, Hamburg, Germany Please join us if you have worked in reinforcement learning or you are simply interested and would like to start! This workshop is intended to foster discussions and to start interesting projects together.
...</p></div><footer class=entry-footer><span title='2024-10-17 00:00:00 +0000 UTC'>October 17, 2024</span>&nbsp;·&nbsp;<span>298 words</span>&nbsp;·&nbsp;<span>RL4AA Collaboration</span></footer><a class=entry-link aria-label="post link to Announcing RL4AA'25 taking place 2 - 4 April 2025 in Hamburg, Germany" href=https://RL4aa.github.io/posts/announcements/rl4aa25_announcement/></a></article><article class=post-entry><figure class=entry-cover><img loading=lazy src=https://RL4aa.github.io/imgs/rl4aa24/rl4aa24-salzburg-2024_53510444597_o.jpg alt="RL4AA'24 workshop group photo"></figure><header class=entry-header><h2 class=entry-hint-parent>Successful RL4AA'24 workshop in Salzburg: Thanks everyone for joining!</h2></header><div class=entry-content><p>From 5 to 7 February 2024, IDA Lab at the Paris Lodron University of Salzburg kindly hosted the RL4AA community for the 2nd workshop on Reinforcement Learning for Autonomous Accelerators (RL4AA'24).
With over 50 participants from more than 10 different countries, we are excited to see that our community is growing and that the interest in reinforcement learning (for particle accelerators) is increasing. In a total of 19 talks, we got to hear about the latest developments and impressive results in the field.
...</p></div><footer class=entry-footer><span title='2024-02-16 00:00:00 +0000 UTC'>February 16, 2024</span>&nbsp;·&nbsp;<span>339 words</span>&nbsp;·&nbsp;<span>RL4AA Collaboration</span></footer><a class=entry-link aria-label="post link to Successful RL4AA'24 workshop in Salzburg: Thanks everyone for joining!" href=https://RL4aa.github.io/posts/announcements/rl4aa24_after/></a></article><article class=post-entry><figure class=entry-cover><img loading=lazy src=https://RL4aa.github.io/imgs/rl4aa24_poster.png alt="RL4AA'24 flyer."></figure><header class=entry-header><h2 class=entry-hint-parent>RL4AA'24 call for abstracts extended to 5 January. Exciting keynote speakers announced. Register now!</h2></header><div class=entry-content><p>The call for abstracts for the RL4AA'24 workshop, taking place 05 - 07 February 2024 in Salzburg, Austria, has been extended. Register for the workshop and submit your abstract until 5 January 2024!
We are also excited to announce our keynote speakers:
Antonin Raffin (German Aerospace Center) Felix Berkenkamp (Bosch Center for AI) For more information on the workshop, please see the official workshop website: https://rl4aa.github.io/RL4AA24/
To get directly to registration, please visit: https://indico.scc.kit.edu/event/3746/
...</p></div><footer class=entry-footer><span title='2023-12-22 00:00:00 +0000 UTC'>December 22, 2023</span>&nbsp;·&nbsp;<span>95 words</span>&nbsp;·&nbsp;<span>RL4AA Collaboration</span></footer><a class=entry-link aria-label="post link to RL4AA'24 call for abstracts extended to 5 January. Exciting keynote speakers announced. Register now!" href=https://RL4aa.github.io/posts/announcements/rl4aa24_last_chance/></a></article><article class=post-entry><figure class=entry-cover><img loading=lazy src=https://RL4aa.github.io/imgs/salzburg_stock_photo_2.png alt=Salzburg.></figure><header class=entry-header><h2 class=entry-hint-parent>Registration is now open for RL4AA'24 taking place 05 - 07 February 2024 in Salzburg, Austria</h2></header><div class=entry-content><p>Announcing RL4AA'24 - Registration is open! Following up on the very successful RL4AA'23 workshop in Karlsruhe earlier this year, we are excited to announce the 2nd RL4AA workshop RL4AA'24, which will be held in Salzburg, Austria, from 05 - 07 February 2024. The workshop will be hosted at the Paris Lodron University of Salzburg. We are looking forward to an exciting workshop with many interesting talks and discussions on reinforcement learning for autonomous particle accelerators and hope to see you all in Salzburg!
...</p></div><footer class=entry-footer><span title='2023-08-18 00:00:00 +0000 UTC'>August 18, 2023</span>&nbsp;·&nbsp;<span>96 words</span>&nbsp;·&nbsp;<span>RL4AA Collaboration</span></footer><a class=entry-link aria-label="post link to Registration is now open for RL4AA'24 taking place 05 - 07 February 2024 in Salzburg, Austria" href=https://RL4aa.github.io/posts/announcements/rl4aa24_announcement/></a></article><article class=post-entry><figure class=entry-cover><img loading=lazy src=https://RL4aa.github.io/imgs/kaiser2023learning.png alt="Simplified 3D illustration of the considered section of the ARES particle accelerator."></figure><header class=entry-header><h2 class=entry-hint-parent>Learning to Do or Learning While Doing: Reinforcement Learning and Bayesian Optimisation for Online Continuous Tuning</h2></header><div class=entry-content><p>J. Kaiser1, C. Xu2, A. Eichler1, A. Santamaria Garcia2, O. Stein1, E. Bründermann2, W. Kuropka1, H. Dinter1, F. Mayet1, T. Vinatier1, F. Burkart1, H. Schlarb1
1Deutsches Elektronen-Synchrotron DESY, 2 Karlsruhe Institute of Technology KIT
arXiv
Abstract Online tuning of real-world plants is a complex optimisation problem that continues to require manual intervention by experienced human operators. Autonomous tuning is a rapidly expanding field of research, where learning-based methods, such as Reinforcement Learning-trained Optimisation (RLO) and Bayesian optimisation (BO), hold great promise for achieving outstanding plant performance and reducing tuning times. Which algorithm to choose in different scenarios, however, remains an open question. Here we present a comparative study using a routine task in a real particle accelerator as an example, showing that RLO generally outperforms BO, but is not always the best choice. Based on the study’s results, we provide a clear set of criteria to guide the choice of algorithm for a given tuning task. These can ease the adoption of learning-based autonomous tuning solutions to the operation of complex real-world plants, ultimately improving the availability and pushing the limits of operability of these facilities, thereby enabling scientific and engineering advancements.
...</p></div><footer class=entry-footer><span title='2023-06-06 00:00:00 +0000 UTC'>June 6, 2023</span>&nbsp;·&nbsp;<span>201 words</span>&nbsp;·&nbsp;<span>RL4AA Collaboration</span></footer><a class=entry-link aria-label="post link to Learning to Do or Learning While Doing: Reinforcement Learning and Bayesian Optimisation for Online Continuous Tuning" href=https://RL4aa.github.io/posts/publications/kaiser2023learning/></a></article><article class=post-entry><figure class=entry-cover><img loading=lazy src=https://RL4aa.github.io/imgs/discord_logo.png alt="Discord logo."></figure><header class=entry-header><h2 class=entry-hint-parent>The RL4AA Discord server is up!</h2></header><div class=entry-content><p>Good News! The RL4AA community is happy to announce its Discord server! If you are interested in discussing reinforcement learning applied to accelerators, please join for announcements (e.g. new publications), forum discussions, an open chat, and meeting rooms.
Hope to see you there!
https://discord.gg/QtBMqsjWH2</p></div><footer class=entry-footer><span title='2023-06-02 00:00:00 +0000 UTC'>June 2, 2023</span>&nbsp;·&nbsp;<span>44 words</span>&nbsp;·&nbsp;<span>RL4AA Collaboration</span></footer><a class=entry-link aria-label="post link to The RL4AA Discord server is up!" href=https://RL4aa.github.io/posts/announcements/discord_server/></a></article><article class=post-entry><figure class=entry-cover><img loading=lazy src=https://RL4aa.github.io/imgs/chen2023trendbased.png alt="Overview of the training loop and the structure of simulated environment"></figure><header class=entry-header><h2 class=entry-hint-parent>Trend-Based SAC Beam Control Method with Zero-Shot in Superconducting Linear Accelerator</h2></header><div class=entry-content><p>X. Chen, X. Qi, C. Su, Y. He, Z. Wang, K. Sun, C. Jin, W. Chen, S. Liu, X. Zhao, D. Jia, M. Yi
Chinese Academy of Sciences
arXiv
Abstract The superconducting linear accelerator is a highly flexiable facility for modern scientific discoveries, necessitating weekly reconfiguration and tuning. Accordingly, minimizing setup time proves essential in affording users with ample experimental time. We propose a trend-based soft actor-critic(TBSAC) beam control method with strong robustness, allowing the agents to be trained in a simulated environment and applied to the real accelerator directly with zero-shot. To validate the effectiveness of our method, two different typical beam control tasks were performed on China Accelerator Facility for Superheavy Elements (CAFe II) and a light particle injector(LPI) respectively. The orbit correction tasks were performed in three cryomodules in CAFe II seperately, the time required for tuning has been reduced to one-tenth of that needed by human experts, and the RMS values of the corrected orbit were all less than 1mm. The other transmission efficiency optimization task was conducted in the LPI, our agent successfully optimized the transmission efficiency of radio-frequency quadrupole(RFQ) to over 85% within 2 minutes. The outcomes of these two experiments offer substantiation that our proposed TBSAC approach can efficiently and effectively accomplish beam commissioning tasks while upholding the same standard as skilled human experts. As such, our method exhibits potential for future applications in other accelerator commissioning fields.
...</p></div><footer class=entry-footer><span title='2023-05-23 00:00:00 +0000 UTC'>May 23, 2023</span>&nbsp;·&nbsp;<span>244 words</span>&nbsp;·&nbsp;<span>RL4AA Collaboration</span></footer><a class=entry-link aria-label="post link to Trend-Based SAC Beam Control Method with Zero-Shot in Superconducting Linear Accelerator" href=https://RL4aa.github.io/posts/publications/chen2023trendbased/></a></article><article class=post-entry><figure class=entry-cover><img loading=lazy src=https://RL4aa.github.io/imgs/THPL038_f2.png alt="Overview of the steering method."></figure><header class=entry-header><h2 class=entry-hint-parent>Ultra fast reinforcement learning demonstrated at CERN AWAKE</h2></header><div class=entry-content><p>** Simon Hirlaender, Lukas Lamminger, Giovanni Zevi Della Porta, Verena Kain**
Abstract Reinforcement learning (RL) is a promising direction in machine learning for the control and optimisation of particle accelerators since it learns directly from experience without needing a model a-priori. However, RL generally suffers from low sample efficiency and thus training from scracth on the machine is often not an option. RL agents are usually trained or pre-tuned on simulators and then transferred to the real environment. In this work we propose a model-based RL approach based on Gaussian processes (GPs) to overcome the sample efficiency limitation. Our RL agent was able to learn to control the trajectory at the CERN AWAKE (Advanced Wakefield Experiment) facility, a problem of 10 degrees of freedom, within a few interactions only. To date, numerical optimises are used to restore or increase and stabilise the performance of accelerators. A major drawback is that they must explore the optimisation space each time they are applied. Our RL approach learns as quickly as numerical optimisers for one optimisation run, but can be used afterwards as single-shot or few-shot controllers. Furthermore, it can also handle safety and time-varying systems and can be used for the online stabilisation of accelerator operation.This approach opens a new avenue for the application of RL in accelerator control and brings it into the realm of everyday applications.
...</p></div><footer class=entry-footer><span title='2023-05-01 00:00:00 +0000 UTC'>May 1, 2023</span>&nbsp;·&nbsp;<span>233 words</span>&nbsp;·&nbsp;<span>RL4AA Collaboration</span></footer><a class=entry-link aria-label="post link to Ultra fast reinforcement learning demonstrated at CERN AWAKE " href=https://RL4aa.github.io/posts/publications/ultra_fast_rl_awake/></a></article><article class=post-entry><figure class=entry-cover><img loading=lazy src=https://RL4aa.github.io/imgs/salzburg_stock_photo.jpg alt=Salzburg.></figure><header class=entry-header><h2 class=entry-hint-parent>The 2nd RL4AA workshop 2024 will be held 05 - 07 February 2024 in Salzburg</h2></header><div class=entry-content><p>Good News - save the date! Our first workshop, RL4AA 2023, was very successful. Because of this, we are hoping to hold the 2nd RL4AA workshop in spring 2024 in Salzburg, Austria in 05 - 07 February 2024. Further details will follow soon.</p></div><footer class=entry-footer><span title='2023-04-27 00:00:00 +0000 UTC'>April 27, 2023</span>&nbsp;·&nbsp;<span>43 words</span>&nbsp;·&nbsp;<span>RL4AA Collaboration</span></footer><a class=entry-link aria-label="post link to The 2nd RL4AA workshop 2024 will be held 05 - 07 February 2024 in Salzburg" href=https://RL4aa.github.io/posts/announcements/rl4aa24_stay_tuned/></a></article><article class=post-entry><figure class=entry-cover><img loading=lazy src=https://RL4aa.github.io/imgs/velotti2023towards.png alt="AWAKE beamline showing location of the matching devices (actions) and the observation BTV."></figure><header class=entry-header><h2 class=entry-hint-parent>Towards automatic setup of 18 MeV electron beamline using machine learning</h2></header><div class=entry-content><p>F. M. Velotti1, B. Goddard1, V. Kain1, R. Ramjiawan1, G. Z. Della Porta1 and S. Hirlaender2
1CERN, 2University of Salzburg
Machine Learning: Science and Technology
Abstract To improve the performance-critical stability and brightness of the electron bunch at injection into the proton-driven plasma wakefield at the AWAKE CERN experiment, automation approaches based on unsupervised machine learning (ML) were developed and deployed. Numerical optimisers were tested together with different model-free reinforcement learning (RL) agents. In order to avoid any bias, RL agents have been trained also using a completely unsupervised state encoding using auto-encoders. To aid hyper-parameter selection, a full synthetic model of the beamline was constructed using a variational auto-encoder trained to generate surrogate data from equipment settings. This paper describes the novel approaches based on deep learning and RL to aid the automatic setup of a low energy line, as the one used to deliver beam to the AWAKE facility. The results obtained with the different ML approaches, including automatic unsupervised feature extraction from images using computer vision are presented. The prospects for operational deployment and wider applicability are discussed.
...</p></div><footer class=entry-footer><span title='2023-04-27 00:00:00 +0000 UTC'>April 27, 2023</span>&nbsp;·&nbsp;<span>189 words</span>&nbsp;·&nbsp;<span>RL4AA Collaboration</span></footer><a class=entry-link aria-label="post link to Towards automatic setup of 18 MeV electron beamline using machine learning" href=https://RL4aa.github.io/posts/publications/velotti2023towards/></a></article><footer class=page-footer><nav class=pagination><a class=next href=https://RL4aa.github.io/page/2/>Next&nbsp;&nbsp;»</a></nav></footer></main><footer class=footer><span>&copy; 2025 <a href=https://RL4aa.github.io/>RL4AA Collaboration | Homepage</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>