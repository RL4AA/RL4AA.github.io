<!doctype html><html lang=en dir=auto><head><meta name=generator content="Hugo 0.110.0"><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>RL4AA Collaboration | Homepage</title><meta name=description content><meta name=author content="RL4AA Collaboration"><link rel=canonical href=https://RL4aa.github.io/><link crossorigin=anonymous href=/assets/css/stylesheet.6a98292fb8fa8cf0f3ba4042d4b75515c04267550f3ad49ff6271b5af9562443.css integrity="sha256-apgpL7j6jPDzukBC1LdVFcBCZ1UPOtSf9icbWvlWJEM=" rel="preload stylesheet" as=style><link rel=icon href=https://RL4aa.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://RL4aa.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://RL4aa.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://RL4aa.github.io/apple-touch-icon.png><link rel=mask-icon href=https://RL4aa.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate type=application/rss+xml href=https://RL4aa.github.io/index.xml><link rel=alternate type=application/json href=https://RL4aa.github.io/index.json><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><meta property="og:title" content="RL4AA Collaboration | Homepage"><meta property="og:description" content><meta property="og:type" content="website"><meta property="og:url" content="https://RL4aa.github.io/"><meta name=twitter:card content="summary"><meta name=twitter:title content="RL4AA Collaboration | Homepage"><meta name=twitter:description content><script type=application/ld+json>{"@context":"https://schema.org","@type":"Organization","name":"RL4AA Collaboration | Homepage","url":"https://RL4aa.github.io/","description":"","thumbnailUrl":"https://RL4aa.github.io/favicon.ico","sameAs":["https://github.com/RL4AA"]}</script></head><body class=list id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://RL4aa.github.io/ accesskey=h title="RL4AA Collaboration | Homepage (Alt + H)">RL4AA Collaboration | Homepage</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://RL4aa.github.io/categories/announcements title=Announcements><span>Announcements</span></a></li><li><a href=https://RL4aa.github.io/categories/publications title=Publications><span>Publications</span></a></li><li><a href=https://RL4aa.github.io/tags/ title=Tags><span>Tags</span></a></li><li><a href=https://RL4aa.github.io/archives/ title=Archive><span>Archive</span></a></li><li><a href=https://RL4aa.github.io/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li></ul></nav></header><main class=main><article class="first-entry home-info"><header class=entry-header><h1>RL4AA Collaboration</h1></header><div class=entry-content>Collaboration on Reinforcement Learning for Autonomous Accelerator</div><footer class=entry-footer><div class=social-icons><a href=https://github.com/RL4AA target=_blank rel="noopener noreferrer me" title=Github><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M9 19c-5 1.5-5-2.5-7-3m14 6v-3.87a3.37 3.37.0 00-.94-2.61c3.14-.35 6.44-1.54 6.44-7A5.44 5.44.0 0020 4.77 5.07 5.07.0 0019.91 1S18.73.65 16 2.48a13.38 13.38.0 00-7 0C6.27.65 5.09 1 5.09 1A5.07 5.07.0 005 4.77 5.44 5.44.0 003.5 8.55c0 5.42 3.3 6.61 6.44 7A3.37 3.37.0 009 18.13V22"/></svg></a></div></footer></article><article class=post-entry><figure class=entry-cover><img loading=lazy src=https://RL4aa.github.io/imgs/rl4aa23_group_photo.jpeg alt="RL4AA23workshop photo"></figure><header class=entry-header><h2>RL4AA'23: 1st Collaboration Workshop on Reinforcement Learning for Autonomous Accelerators</h2></header><div class=entry-content><p>Links to the workshop Indico page of the workshop. Github repository for the hands-on tutorial.</p></div><footer class=entry-footer><span title='2023-02-21 00:00:00 +0000 UTC'>February 21, 2023</span>&nbsp;·&nbsp;15 words&nbsp;·&nbsp;RL4AA Collaboration</footer><a class=entry-link aria-label="post link to RL4AA'23: 1st Collaboration Workshop on Reinforcement Learning for Autonomous Accelerators" href=https://RL4aa.github.io/posts/rl4aa23/></a></article><article class=post-entry><figure class=entry-cover><img loading=lazy src=https://RL4aa.github.io/imgs/kaiser2022learningbased.png alt="Reinforcement learning loop for the ARES experimental area."></figure><header class=entry-header><h2>Learning-based Optimisation of Particle Accelerators Under Partial Observability Without Real-World Training</h2></header><div class=entry-content><p>J. Kaiser, O. Stein, A. Eichler. Deutsches Elektronen-Synchrotron DESY. 39th International Conference on Machine Learning.
Abstract In recent work, it has been shown that reinforcement learning (RL) is capable of solving a variety of problems at sometimes super-human performance levels. But despite continued advances in the field, applying RL to complex real-world control and optimisation problems has proven difficult. In this contribution, we demonstrate how to successfully apply RL to the optimisation of a highly complex real-world machine – specifically a linear particle accelerator – in an only partially observable setting and without requiring training on the real machine....</p></div><footer class=entry-footer><span title='2022-07-22 00:00:00 +0000 UTC'>July 22, 2022</span>&nbsp;·&nbsp;174 words&nbsp;·&nbsp;RL4AA Collaboration</footer><a class=entry-link aria-label="post link to Learning-based Optimisation of Particle Accelerators Under Partial Observability Without Real-World Training" href=https://RL4aa.github.io/posts/kaiser2022learningbased/></a></article><article class=post-entry><figure class=entry-cover><img loading=lazy src=https://RL4aa.github.io/imgs/madysa2022automated.png alt="Success rate of the various algorithms over initial beam intensity."></figure><header class=entry-header><h2>Automated Intensity Optimisation Using Reinforcement Learning at LEIR</h2></header><div class=entry-content><p>N. Madysa, V. Kain, R. Alemany Fernandez, N. Biancacci, B. Goddard, F. M. Velotti. CERN. 13th Particle Accelerator Conference.
Abstract High intensities in the Low Energy Ion Ring (LEIR) at CERN are achieved by stacking several multi-turn injec- tions from the pre-accelerator Linac3. Up to seven consec- utive 200 μs long, 200 ms spaced pulses are injected from Linac3 into LEIR. Two inclined septa, one magnetic and one electrostatic, combined with a collapsing horizontal or- bit bump allows a 6-D phase space painting via a linearly ramped mean momentum along the Linac3 pulse and in- jection at high dispersion....</p></div><footer class=entry-footer><span title='2022-06-12 00:00:00 +0000 UTC'>June 12, 2022</span>&nbsp;·&nbsp;264 words&nbsp;·&nbsp;RL4AA Collaboration</footer><a class=entry-link aria-label="post link to Automated Intensity Optimisation Using Reinforcement Learning at LEIR" href=https://RL4aa.github.io/posts/madysa2022automated/></a></article><article class=post-entry><figure class=entry-cover><img loading=lazy src=https://RL4aa.github.io/imgs/bruchon2021feasibility.png alt="Simple scheme of the FERMI FEL seed laser alignment set up."></figure><header class=entry-header><h2>Feasibility Investigation on Several Reinforcement Learning Techniques to Improve the Performance of the FERMI Free-Electron Laser</h2></header><div class=entry-content><p>N. Bruchon. University of Trieste. PhD thesis.
Abstract The research carried out in particle accelerator facilities does not concern only particle and condensed matter physics, although these are the main topics covered in the field. Indeed, since a particle accelerator is composed of many different sub-systems, its proper functioning depends both on each of these parts and their interconnection. It follows that the study, implementation, and improvement of the various sub-systems are fundamental points of investigation too....</p></div><footer class=entry-footer><span title='2021-03-18 00:00:00 +0000 UTC'>March 18, 2021</span>&nbsp;·&nbsp;322 words&nbsp;·&nbsp;RL4AA Collaboration</footer><a class=entry-link aria-label="post link to Feasibility Investigation on Several Reinforcement Learning Techniques to Improve the Performance of the FERMI Free-Electron Laser" href=https://RL4aa.github.io/posts/bruchon2021feasiblity/></a></article><article class=post-entry><figure class=entry-cover><img loading=lazy src=https://RL4aa.github.io/imgs/pang2020autonomous.png alt="Policy network maps states to actions."></figure><header class=entry-header><h2>Autonomous Control of a Particle Accelerator using Deep Reinforcement Learning</h2></header><div class=entry-content><p>X. Pang, S. Thulasidasan, L. Rybarcyk. Apple, Los Alamos National Laboratory. Machine Learning for Engineering Modeling, Simulation, and Design Workshop at Neural Information Processing Systems 2020.
Abstract We describe an approach to learning optimal control policies for a large, linear particle accelerator using deep reinforcement learning coupled with a high-fidelity physics engine. The framework consists of an AI controller that uses deep neural networks for state and action-space representation and learns optimal policies using reward signals that are provided by the physics simulator....</p></div><footer class=entry-footer><span title='2020-12-12 00:00:00 +0000 UTC'>December 12, 2020</span>&nbsp;·&nbsp;150 words&nbsp;·&nbsp;RL4AA Collaboration</footer><a class=entry-link aria-label="post link to Autonomous Control of a Particle Accelerator using Deep Reinforcement Learning" href=https://RL4aa.github.io/posts/pang2020autonomous/></a></article><article class=post-entry><figure class=entry-cover><img loading=lazy src=https://RL4aa.github.io/imgs/kain2020sampleefficient.png alt="The RL paradigm as applied to particle accelerator control, showing the example of trajectory correction."></figure><header class=entry-header><h2>Sample-efficient reinforcement learning for CERN accelerator control</h2></header><div class=entry-content><p>V. Kain, S. Hirlander, B. Goddard, F. M. Velotti, G. Z. Della Porta, N. Bruchon, G. Valentino. CERN, University of Trieste, University of Malta. Physical Review Accelerators and Beams.
Abstract Numerical optimization algorithms are already established tools to increase and stabilize the performance of particle accelerators. These algorithms have many advantages, are available out of the box, and can be adapted to a wide range of optimization problems in accelerator operation....</p></div><footer class=entry-footer><span title='2020-12-01 00:00:00 +0000 UTC'>December 1, 2020</span>&nbsp;·&nbsp;185 words&nbsp;·&nbsp;RL4AA Collaboration</footer><a class=entry-link aria-label="post link to Sample-efficient reinforcement learning for CERN accelerator control" href=https://RL4aa.github.io/posts/kain2020sampleefficient/></a></article><article class=post-entry><figure class=entry-cover><img loading=lazy src=https://RL4aa.github.io/imgs/bruchon2020basic.png alt="Simple scheme of the FERMI FEL seed laser alignment set up."></figure><header class=entry-header><h2>Basic Reinforcement Learning Techniques to Control the Intensity of a Seeded Free-Electron Laser</h2></header><div class=entry-content><p>N. Bruchon, G. Fenu, G. Gaio, M. Lonza, F. H. O’Shea, F. A. Pellegrino, E. Salvato. University of Trieste, Elettra Sincrotrone Trieste. Electronics.
Abstract Optimal tuning of particle accelerators is a challenging task. Many different approaches have been proposed in the past to solve two main problems—attainment of an optimal working point and performance recovery after machine drifts. The most classical model-free techniques (e.g., Gradient Ascent or Extremum Seeking algorithms) have some intrinsic limitations....</p></div><footer class=entry-footer><span title='2020-05-09 00:00:00 +0000 UTC'>May 9, 2020</span>&nbsp;·&nbsp;206 words&nbsp;·&nbsp;RL4AA Collaboration</footer><a class=entry-link aria-label="post link to Basic Reinforcement Learning Techniques to Control the Intensity of a Seeded Free-Electron Laser" href=https://RL4aa.github.io/posts/bruchon2020basic/></a></article><article class=post-entry><figure class=entry-cover><img loading=lazy src=https://RL4aa.github.io/imgs/bruchon2019toward.png alt="Simple scheme of the EOS laser alignment set up."></figure><header class=entry-header><h2>Toward the Application of Reinforcement Learning to the Intensity Control of a Seeded Free-Electron Laser</h2></header><div class=entry-content><p>N. Bruchon, G. Fenu, G. Gaio, M. Lonza, F. A. Pellegrino, E. Salvato. University of Trieste. 23rd International Conference on Mechatronics Technology.
Abstract The optimization of particle accelerators is a challenging task, and many different approaches have been proposed in years, to obtain an optimal tuning of the plant and to keep it optimally tuned despite drifts or disturbances. Indeed, the classical model-free approaches (such as Gradient Ascent or Extremum Seeking algorithms) have intrinsic limitations....</p></div><footer class=entry-footer><span title='2019-10-23 00:00:00 +0000 UTC'>October 23, 2019</span>&nbsp;·&nbsp;222 words&nbsp;·&nbsp;RL4AA Collaboration</footer><a class=entry-link aria-label="post link to Toward the Application of Reinforcement Learning to the Intensity Control of a Seeded Free-Electron Laser" href=https://RL4aa.github.io/posts/bruchon2019toward/></a></article></main><footer class=footer><span>&copy; 2023 <a href=https://RL4aa.github.io/>RL4AA Collaboration | Homepage</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>