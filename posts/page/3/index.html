<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Posts | RL4AA Collaboration | Homepage</title><meta name=keywords content><meta name=description content="Posts - RL4AA Collaboration | Homepage"><meta name=author content="RL4AA Collaboration"><link rel=canonical href=https://RL4aa.github.io/posts/><link crossorigin=anonymous href=/assets/css/stylesheet.a090830a421002426baafbd314e38f149d77b4c48a12ee9312700d770b27fb26.css integrity="sha256-oJCDCkIQAkJrqvvTFOOPFJ13tMSKEu6TEnANdwsn+yY=" rel="preload stylesheet" as=style><link rel=icon href=https://RL4aa.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://RL4aa.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://RL4aa.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://RL4aa.github.io/apple-touch-icon.png><link rel=mask-icon href=https://RL4aa.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate type=application/rss+xml href=https://RL4aa.github.io/posts/index.xml title=rss><link rel=alternate hreflang=en href=https://RL4aa.github.io/posts/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><meta property="og:url" content="https://RL4aa.github.io/posts/"><meta property="og:site_name" content="RL4AA Collaboration | Homepage"><meta property="og:title" content="Posts"><meta property="og:description" content="Reinforcement Learning for Autonomous Accelerator Collaboration - https://github.com/RL4AA"><meta property="og:locale" content="en-us"><meta property="og:type" content="website"><meta name=twitter:card content="summary"><meta name=twitter:title content="Posts"><meta name=twitter:description content="Reinforcement Learning for Autonomous Accelerator Collaboration - https://github.com/RL4AA"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://RL4aa.github.io/posts/"}]}</script></head><body class=list id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://RL4aa.github.io/ accesskey=h title="RL4AA Collaboration | Homepage (Alt + H)"><img src=https://RL4aa.github.io/imgs/rl4aa_logo.png alt aria-label=logo height=30>RL4AA Collaboration | Homepage</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://RL4aa.github.io/categories/announcements title=Announcements><span>Announcements</span></a></li><li><a href=https://RL4aa.github.io/categories/publications title=Publications><span>Publications</span></a></li><li><a href=https://RL4aa.github.io/tags/ title=Tags><span>Tags</span></a></li><li><a href=https://RL4aa.github.io/archives/ title=Archive><span>Archive</span></a></li><li><a href=https://RL4aa.github.io/aboutus/ title="About Us"><span>About Us</span></a></li></ul></nav></header><main class=main><header class=page-header><div class=breadcrumbs><a href=https://RL4aa.github.io/>Home</a></div><h1>Posts</h1></header><article class=post-entry><figure class=entry-cover><img loading=lazy src=https://RL4aa.github.io/imgs/grech2021renovation.png alt="Best PPO agent. Action is deterministic."></figure><header class=entry-header><h2 class=entry-hint-parent>Renovation of the beam-based feedback systems in the LHC</h2></header><div class=entry-content><p>L. Grech
University of Malta
PhD thesis
Abstract The Large Hadron Collider (LHC) at the European Organization for Nuclear Research (CERN) is the largest synchrotron built to date, having a circumference of approx- imately 27km. The LHC is able to accelerate two counter-rotating proton and/or heavy-ion beams up to 7 TeV per charge. These highly energetic beams are contained inside a vacuum chamber with an inner diameter of 80 mm by means of strong mag- netic fields produced by superconducting magnets. A beam cleaning and machine protection system is in place to prevent high-energy halo particles from impacting and heating the superconducting magnets.
...</p></div><footer class=entry-footer><span title='2021-09-01 00:00:00 +0000 UTC'>September 1, 2021</span>&nbsp;·&nbsp;<span>655 words</span>&nbsp;·&nbsp;<span>RL4AA Collaboration</span></footer><a class=entry-link aria-label="post link to Renovation of the beam-based feedback systems in the LHC" href=https://RL4aa.github.io/posts/publications/grech2021renovation/></a></article><article class=post-entry><figure class=entry-cover><img loading=lazy src=https://RL4aa.github.io/imgs/wang2021accelerated.png alt="Hardware solution  for RL control."></figure><header class=entry-header><h2 class=entry-hint-parent>Accelerated Deep Reinforcement Learning for Fast Feedback of Beam Dynamics at KARA</h2></header><div class=entry-content><p>W. Wang1, M. Caselle1, T. Boltz1, E. Blomley1, M. Brosi1, T. Dritschler1, A. Ebersoldt1, A. Kopmann1, A. Santamaria Garcia1, P. Schreiber1, E. Bründermann1, M. Weber1, A.-S. Müller1, Y. Fang2
1Karlsruhe Insitute of Technology KIT, 2Northwestern Polytechnical University
IEEE Transactions on Nuclear Science
Abstract Coherent synchrotron radiation (CSR) is generated when the electron bunch length is in the order of the magnitude of the wavelength of the emitted radiation. The self-interaction of short electron bunches with their own electromagnetic fields changes the longitudinal beam dynamics significantly. Above a certain current threshold, the micro-bunching instability develops, characterized by the appearance of distinguishable substructures in the longitudinal phase space of the bunch. To stabilize the CSR emission, a real-time feedback control loop based on reinforcement learning (RL) is proposed. Informed by the available THz diagnostics, the feedback is designed to act on the radio frequency (RF) system of the storage ring to mitigate the micro-bunching dynamics. To satisfy low-latency requirements given by the longitudinal beam dynamics, the RL controller has been implemented on hardware (FPGA). In this article, a real-time feedback loop architecture and its performance is presented and compared with a software implementation using Keras-RL on CPU/GPU. The results obtained with the CSR simulation Inovesa demonstrate that the functionality of both platforms is equivalent. The training performance of the hardware implementation is similar to software solution, while it outperforms the Keras-RL implementation by an order of magnitude. The presented RL hardware controller is considered as an essential platform for the development of intelligent CSR control systems.
...</p></div><footer class=entry-footer><span title='2021-05-27 00:00:00 +0000 UTC'>May 27, 2021</span>&nbsp;·&nbsp;<span>260 words</span>&nbsp;·&nbsp;<span>RL4AA Collaboration</span></footer><a class=entry-link aria-label="post link to Accelerated Deep Reinforcement Learning for Fast Feedback of Beam Dynamics at KARA" href=https://RL4aa.github.io/posts/publications/wang2021accelerated/></a></article><article class=post-entry><figure class=entry-cover><img loading=lazy src=https://RL4aa.github.io/imgs/eichler2021first.png alt="RL environment for beam optimisation in theARES EA."></figure><header class=entry-header><h2 class=entry-hint-parent>First Steps Toward an Autonomous Accelerator, A Common Project Between DESY and KIT</h2></header><div class=entry-content><p>A. Eichler1, F. Burkart1, J. Kaiser1, W. Kuropka1, O. Stein1, E. Bründermann2, A. Santamaria Garcia2, C. Xu2
1Deutsches Elektronen-Synchrotron DESY, 2Karlsruhe Institute of Technology KIT
12th International Particle Accelerator Conference
Abstract Reinforcement learning algorithms have risen in pop-ularity in the accelerator physics community in recentyears, showing potential in beam control and in the opti-mization and automation of tasks in accelerator operation.The Helmholtz AI project “Machine Learning Toward Au-tonomous Accelerators” is a collaboration between DESYand KIT that works on investigating and developing rein-forcement learning applications for the automatic start-upof electron linear accelerators. The work is carried out inparallel at two similar research accelerators: ARES at DESYand FLUTE at KIT, giving the unique opportunity of trans-fer learning between facilities. One of the first steps of thisproject is the establishment of a common interface betweenthe simulations and the machine, in order to test and applyvarious optimization approaches interchangeably betweenthe two accelerators. In this paper we present first results onthe common interface and its application to beam focusingin ARES as well as the idea of laser shaping with spatiallight modulators at FLUTE.
...</p></div><footer class=entry-footer><span title='2021-05-24 00:00:00 +0000 UTC'>May 24, 2021</span>&nbsp;·&nbsp;<span>185 words</span>&nbsp;·&nbsp;<span>RL4AA Collaboration</span></footer><a class=entry-link aria-label="post link to First Steps Toward an Autonomous Accelerator, A Common Project Between DESY and KIT" href=https://RL4aa.github.io/posts/publications/eichler2021first/></a></article><article class=post-entry><figure class=entry-cover><img loading=lazy src=https://RL4aa.github.io/imgs/ivanov2021physicsenhanced.png alt="Reinforcement learning agent joint with the physics-based polynomial neural network."></figure><header class=entry-header><h2 class=entry-hint-parent>Physics-Enhanced Reinforcement Learning for Optimal Control</h2></header><div class=entry-content><p>A. Ivanov, I. Agapov, A. Eichler, S. Tomin
Deutsches Elektronen Synchrotron DESY
12th International Particle Accelerator Conference
Abstract We propose an approach for incorporating acceleratorphysics models into reinforcement learning agents. The proposed approach is based on the Taylor mapping technique for the simulation of particle dynamics. The resulting computational graph is represented as a polynomial neural network and embedded into the traditional reinforcement learning agents. The application of the model is demonstrated in a nonlinear simulation model of beam transmission. The comparison of the approach with the traditional numerical optimization as well as neural networks-based agents demonstrates better convergence of the proposed technique.
...</p></div><footer class=entry-footer><span title='2021-05-21 00:00:00 +0000 UTC'>May 21, 2021</span>&nbsp;·&nbsp;<span>110 words</span>&nbsp;·&nbsp;<span>RL4AA Collaboration</span></footer><a class=entry-link aria-label="post link to Physics-Enhanced Reinforcement Learning for Optimal Control" href=https://RL4aa.github.io/posts/publications/ivanov2021physicsenhanced/></a></article><article class=post-entry><figure class=entry-cover><img loading=lazy src=https://RL4aa.github.io/imgs/bruchon2021feasibility.png alt="Simple scheme of the FERMI FEL seed laser alignment set up."></figure><header class=entry-header><h2 class=entry-hint-parent>Feasibility Investigation on Several Reinforcement Learning Techniques to Improve the Performance of the FERMI Free-Electron Laser</h2></header><div class=entry-content><p>N. Bruchon
University of Trieste
PhD thesis
Abstract The research carried out in particle accelerator facilities does not concern only particle and condensed matter physics, although these are the main topics covered in the field. Indeed, since a particle accelerator is composed of many different sub-systems, its proper functioning depends both on each of these parts and their interconnection. It follows that the study, implementation, and improvement of the various sub-systems are fundamental points of investigation too. In particular, an interesting aspect for the automation engineering community is the control of such systems that usually are complex, large, noise-affected, and non-linear.
...</p></div><footer class=entry-footer><span title='2021-03-18 00:00:00 +0000 UTC'>March 18, 2021</span>&nbsp;·&nbsp;<span>322 words</span>&nbsp;·&nbsp;<span>RL4AA Collaboration</span></footer><a class=entry-link aria-label="post link to Feasibility Investigation on Several Reinforcement Learning Techniques to Improve the Performance of the FERMI Free-Electron Laser" href=https://RL4aa.github.io/posts/publications/bruchon2021feasiblity/></a></article><article class=post-entry><figure class=entry-cover><img loading=lazy src=https://RL4aa.github.io/imgs/oshea2020policy.png alt="Plot of the reward received by the agent versus step number."></figure><header class=entry-header><h2 class=entry-hint-parent>Policy gradient methods for free-electron laser and terahertz source optimization and stabilization at the FERMI free-electron laser at Elettra</h2></header><div class=entry-content><p>F. H. O’Shea1, N. Bruchon2, G. Gaio1
1Elettra Sincrotrone Trieste, 2University of Trieste
Physical Review Accelerators and Beams
Abstract In this article we report on the application of a model-free reinforcement learning method to the optimization of accelerator systems. We simplify a policy gradient algorithm to accelerator control from sophisticated algorithms that have recently been demonstrated to solve complex dynamic problems. After outlining a theoretical basis for the functioning of the algorithm, we explore the small hyperparameter space to develop intuition about said parameters using a simple number-guess environment. Finally, we demonstrate the algorithm optimizing both a free-electron laser and an accelerator-based terahertz source in-situ. The algorithm is applied to different accelerator control systems and optimizes the desired signals in a few hundred steps without any domain knowledge using up to five control parameters. In addition, the algorithm shows modest tolerance to accelerator fault conditions without any special preparation for such conditions.
...</p></div><footer class=entry-footer><span title='2020-12-21 00:00:00 +0000 UTC'>December 21, 2020</span>&nbsp;·&nbsp;<span>160 words</span>&nbsp;·&nbsp;<span>RL4AA Collaboration</span></footer><a class=entry-link aria-label="post link to Policy gradient methods for free-electron laser and terahertz source optimization and stabilization at the FERMI free-electron laser at Elettra" href=https://RL4aa.github.io/posts/publications/oshea202policy/></a></article><article class=post-entry><figure class=entry-cover><img loading=lazy src=https://RL4aa.github.io/imgs/hirlaender2020modelfree.png alt="A schematic overview of theAE-DYNAapproach used in this paper."></figure><header class=entry-header><h2 class=entry-hint-parent>Model-free and Bayesian Ensembling Model-based Deep Reinforcement Learning for Particle Accelerator Control Demonstrated on the FERMI FEL</h2></header><div class=entry-content><p>S. Hirlaender1, N. Bruchon2
1University of Salzburg, 2University of Trieste
arXiv
Abstract Reinforcement learning holds tremendous promise in accelerator controls. The primary goal of this paper is to show how this approach can be utilised on an operational level on accelerator physics problems. Despite the success of model-free reinforcement learning in several domains, sample-efficiency still is a bottle-neck, which might be encompassed by model-based methods. We compare well-suited purely model-based to model-free reinforcement learning applied to the intensity optimisation on the FERMI FEL system. We find that the model-based approach demonstrates higher representational power and sample-efficiency, while the asymptotic performance of the model-free method is slightly superior. The model-based algorithm is implemented in a DYNA-style using an uncertainty aware model, and the model-free algorithm is based on tailored deep Q-learning. In both cases, the algorithms were implemented in a way, which presents increased noise robustness as omnipresent in accelerator control problems.
...</p></div><footer class=entry-footer><span title='2020-12-17 00:00:00 +0000 UTC'>December 17, 2020</span>&nbsp;·&nbsp;<span>158 words</span>&nbsp;·&nbsp;<span>RL4AA Collaboration</span></footer><a class=entry-link aria-label="post link to Model-free and Bayesian Ensembling Model-based Deep Reinforcement Learning for Particle Accelerator Control Demonstrated on the FERMI FEL" href=https://RL4aa.github.io/posts/publications/hirlaender2020modelfree/></a></article><article class=post-entry><figure class=entry-cover><img loading=lazy src=https://RL4aa.github.io/imgs/pang2020autonomous.png alt="Policy network maps states to actions."></figure><header class=entry-header><h2 class=entry-hint-parent>Autonomous Control of a Particle Accelerator using Deep Reinforcement Learning</h2></header><div class=entry-content><p>X. Pang1, S. Thulasidasan2, L. Rybarcyk2
1Apple, 2Los Alamos National Laboratory
Machine Learning for Engineering Modeling, Simulation, and Design Workshop at Neural Information Processing Systems 2020
Abstract We describe an approach to learning optimal control policies for a large, linear particle accelerator using deep reinforcement learning coupled with a high-fidelity physics engine. The framework consists of an AI controller that uses deep neural networks for state and action-space representation and learns optimal policies using reward signals that are provided by the physics simulator. For this work, we only focus on controlling a small section of the entire accelerator. Nevertheless, initial results indicate that we can achieve better-than-human level performance in terms of particle beam current and distribution. The ultimate goal of this line of work is to substantially reduce the tuning time for such facilities by orders of magnitude, and achieve near-autonomous control.
...</p></div><footer class=entry-footer><span title='2020-12-12 00:00:00 +0000 UTC'>December 12, 2020</span>&nbsp;·&nbsp;<span>150 words</span>&nbsp;·&nbsp;<span>RL4AA Collaboration</span></footer><a class=entry-link aria-label="post link to Autonomous Control of a Particle Accelerator using Deep Reinforcement Learning" href=https://RL4aa.github.io/posts/publications/pang2020autonomous/></a></article><article class=post-entry><figure class=entry-cover><img loading=lazy src=https://RL4aa.github.io/imgs/kain2020sampleefficient.png alt="The RL paradigm as applied to particle accelerator control, showing the example of trajectory correction."></figure><header class=entry-header><h2 class=entry-hint-parent>Sample-efficient reinforcement learning for CERN accelerator control</h2></header><div class=entry-content><p>V. Kain1, S. Hirlander1, B. Goddard1, F. M. Velotti1, G. Z. Della Porta1, N. Bruchon2, G. Valentino3
1CERN, 2University of Trieste, 3University of Malta
Physical Review Accelerators and Beams
Abstract Numerical optimization algorithms are already established tools to increase and stabilize the performance of particle accelerators. These algorithms have many advantages, are available out of the box, and can be adapted to a wide range of optimization problems in accelerator operation. The next boost in efficiency is expected to come from reinforcement learning algorithms that learn the optimal policy for a certain control problem and hence, once trained, can do without the time-consuming exploration phase needed for numerical optimizers. To investigate this approach, continuous model-free reinforcement learning with up to 16 degrees of freedom was developed and successfully tested at various facilities at CERN. The approach and algorithms used are discussed and the results obtained for trajectory steering at the AWAKE electron line and LINAC4 are presented. The necessary next steps, such as uncertainty aware model-based approaches, and the potential for future applications at particle accelerators are addressed.
...</p></div><footer class=entry-footer><span title='2020-12-01 00:00:00 +0000 UTC'>December 1, 2020</span>&nbsp;·&nbsp;<span>185 words</span>&nbsp;·&nbsp;<span>RL4AA Collaboration</span></footer><a class=entry-link aria-label="post link to Sample-efficient reinforcement learning for CERN accelerator control" href=https://RL4aa.github.io/posts/publications/kain2020sampleefficient/></a></article><article class=post-entry><figure class=entry-cover><img loading=lazy src=https://RL4aa.github.io/imgs/edelen2020neural.png alt="Schematic for neural network control policy updates."></figure><header class=entry-header><h2 class=entry-hint-parent>Neural Networks for Modeling and Control of Particle Accelerators</h2></header><div class=entry-content><p>A. L. Edelen
Colorado State University
PhD thesis
Abstract Charged particle accelerators support a wide variety of scientific, industrial, and medical applications. They range in scale and complexity from systems with just a few components for beam acceleration and manipulation, to large scientific user facilities that span many kilometers and have hundreds-to-thousands of individually-controllable components. Specific operational requirements must be met by adjusting the many controllable variables of the accelerator. Meeting these requirements can be challenging, both in terms of the ability to achieve specific beam quality metrics in a reliable fashion and in terms of the time needed to set up and maintain the optimal operating conditions. One avenue toward addressing this challenge is to incorporate techniques from the fields of machine learning (ML) and artificial intelligence (AI) into the way particle accelerators are modeled and controlled. While many promising approaches within AI/ML could beused for particle accelerators, this dissertation focuses on approaches based on neural networks. Neural networks are particularly well-suited to modeling, control, and diagnostic analysis of non-linear systems, as well as systems with large parameter spaces. They are also very appealing for their ability to process high-dimensional data types, such as images and time series (both of which are ubiquitous in particle accelerators). In this work, key studies that demonstrated the potential utility of modern neural network-based approaches to modeling and control of particle accelerators are presented. The context for this work is important: at the start of this work in 2012, there was little interest in AI/ML in the particle accelerator community, and many of the advances in neural networks and deep learning that enabled its present success had not yet been made at that time. As such, this work was both an exploration of possible application areas and a generator of initial demonstrations in these areas, including some of the first applications of modern deep neural networks in particle accelerators.
...</p></div><footer class=entry-footer><span title='2020-07-01 00:00:00 +0000 UTC'>July 1, 2020</span>&nbsp;·&nbsp;<span>322 words</span>&nbsp;·&nbsp;<span>RL4AA Collaboration</span></footer><a class=entry-link aria-label="post link to Neural Networks for Modeling and Control of Particle Accelerators" href=https://RL4aa.github.io/posts/publications/edelen2020neural/></a></article><footer class=page-footer><nav class=pagination><a class=prev href=https://RL4aa.github.io/posts/page/2/>«&nbsp;Prev&nbsp;
</a><a class=next href=https://RL4aa.github.io/posts/page/4/>Next&nbsp;&nbsp;»</a></nav></footer></main><footer class=footer><span>&copy; 2025 <a href=https://RL4aa.github.io/>RL4AA Collaboration | Homepage</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>