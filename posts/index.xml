<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/"><channel><title>Posts on RL4AA Collaboration | Homepage</title><link>https://RL4aa.github.io/posts/</link><description>Recent content in Posts on RL4AA Collaboration | Homepage</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Tue, 21 Feb 2023 00:00:00 +0000</lastBuildDate><atom:link href="https://RL4aa.github.io/posts/index.xml" rel="self" type="application/rss+xml"/><item><title>RL4AA'23: 1st Collaboration Workshop on Reinforcement Learning for Autonomous Accelerators</title><link>https://RL4aa.github.io/posts/rl4aa23/</link><pubDate>Tue, 21 Feb 2023 00:00:00 +0000</pubDate><guid>https://RL4aa.github.io/posts/rl4aa23/</guid><description>Links to the workshop Indico page of the workshop. Github repository for the hands-on tutorial.</description></item><item><title>Learning-based Optimisation of Particle Accelerators Under Partial Observability Without Real-World Training</title><link>https://RL4aa.github.io/posts/kaiser2022learningbased/</link><pubDate>Fri, 22 Jul 2022 00:00:00 +0000</pubDate><guid>https://RL4aa.github.io/posts/kaiser2022learningbased/</guid><description>J. Kaiser, O. Stein, A. Eichler. Deutsches Elektronen-Synchrotron DESY. 39th International Conference on Machine Learning.
Abstract In recent work, it has been shown that reinforcement learning (RL) is capable of solving a variety of problems at sometimes super-human performance levels. But despite continued advances in the field, applying RL to complex real-world control and optimisation problems has proven difficult. In this contribution, we demonstrate how to successfully apply RL to the optimisation of a highly complex real-world machine – specifically a linear particle accelerator – in an only partially observable setting and without requiring training on the real machine.</description></item><item><title>Automated Intensity Optimisation Using Reinforcement Learning at LEIR</title><link>https://RL4aa.github.io/posts/madysa2022automated/</link><pubDate>Sun, 12 Jun 2022 00:00:00 +0000</pubDate><guid>https://RL4aa.github.io/posts/madysa2022automated/</guid><description>N. Madysa, V. Kain, R. Alemany Fernandez, N. Biancacci, B. Goddard, F. M. Velotti. CERN. 13th Particle Accelerator Conference.
Abstract High intensities in the Low Energy Ion Ring (LEIR) at CERN are achieved by stacking several multi-turn injec- tions from the pre-accelerator Linac3. Up to seven consec- utive 200 μs long, 200 ms spaced pulses are injected from Linac3 into LEIR. Two inclined septa, one magnetic and one electrostatic, combined with a collapsing horizontal or- bit bump allows a 6-D phase space painting via a linearly ramped mean momentum along the Linac3 pulse and in- jection at high dispersion.</description></item><item><title>Real-time artificial intelligence for accelerator control: A study at the Fermilab Booster</title><link>https://RL4aa.github.io/posts/stjohn2021realtime/</link><pubDate>Mon, 18 Oct 2021 00:00:00 +0000</pubDate><guid>https://RL4aa.github.io/posts/stjohn2021realtime/</guid><description>J. St. John, C. Herwig, D. Kafkes, J. Mitrevski, W. A. Pellico, G. N. Perdue, A. Quintero-Parra, B. A. Schupbach, K. Seiya, N. Tran, M. Schram, J. M. Duarte, Y. Huang, R. Keller. Fermi National Accelerator Laboratory, Thomas Jefferson National Accelerator Laboratory, University of California San Diego, Pacific Northwest National Laboratory, Columbia University. Physical Review Accelerators and Beams.
Abstract We describe a method for precisely regulating the gradient magnet power supply (GMPS) at the Fermilab Booster accelerator complex using a neural network trained via reinforcement learning.</description></item><item><title>First Steps Toward an Autonomous Accelerator, A Common Project Between DESY and KIT</title><link>https://RL4aa.github.io/posts/eichler2021first/</link><pubDate>Mon, 24 May 2021 00:00:00 +0000</pubDate><guid>https://RL4aa.github.io/posts/eichler2021first/</guid><description>A. Eichler, F. Burkart, J. Kaiser, W. Kuropka, O. Stein, E. Bründermann, A. Santamaria Garcia, C. Xu. Deutsches Elektronen-Synchrotron DESY, Karlsruhe Institute of Technology KIT. 12th International Particle Accelerator Conference.
Abstract Reinforcement learning algorithms have risen in pop-ularity in the accelerator physics community in recentyears, showing potential in beam control and in the opti-mization and automation of tasks in accelerator operation.The Helmholtz AI project “Machine Learning Toward Au-tonomous Accelerators” is a collaboration between DESYand KIT that works on investigating and developing rein-forcement learning applications for the automatic start-upof electron linear accelerators.</description></item><item><title>Physics-Enhanced Reinforcement Learning for Optimal Control</title><link>https://RL4aa.github.io/posts/ivanov2021physicsenhanced/</link><pubDate>Fri, 21 May 2021 00:00:00 +0000</pubDate><guid>https://RL4aa.github.io/posts/ivanov2021physicsenhanced/</guid><description>A. Ivanov, I. Agapov, A. Eichler, S. Tomin. Deutsches Elektronen Synchrotron DESY. 12th International Particle Accelerator Conference.
Abstract We propose an approach for incorporating acceleratorphysics models into reinforcement learning agents. The pro-posed approach is based on the Taylor mapping technique forthe simulation of particle dynamics. The resulting computa-tional graph is represented as a polynomial neural networkand embedded into the traditional reinforcement learningagents. The application of the model is demonstrated ina nonlinear simulation model of beam transmission.</description></item><item><title>Feasibility Investigation on Several Reinforcement Learning Techniques to Improve the Performance of the FERMI Free-Electron Laser</title><link>https://RL4aa.github.io/posts/bruchon2021feasiblity/</link><pubDate>Thu, 18 Mar 2021 00:00:00 +0000</pubDate><guid>https://RL4aa.github.io/posts/bruchon2021feasiblity/</guid><description>N. Bruchon. University of Trieste. PhD thesis.
Abstract The research carried out in particle accelerator facilities does not concern only particle and condensed matter physics, although these are the main topics covered in the field. Indeed, since a particle accelerator is composed of many different sub-systems, its proper functioning depends both on each of these parts and their interconnection. It follows that the study, implementation, and improvement of the various sub-systems are fundamental points of investigation too.</description></item><item><title>Policy gradient methods for free-electron laser and terahertz source optimization and stabilization at the FERMI free-electron laser at Elettra</title><link>https://RL4aa.github.io/posts/oshea202policy/</link><pubDate>Mon, 21 Dec 2020 00:00:00 +0000</pubDate><guid>https://RL4aa.github.io/posts/oshea202policy/</guid><description>F. H. O&amp;rsquo;Shea, N. Bruchon, G. Gaio. Elettra Sincrotrone Trieste, University of Trieste. Physical Review Accelerators and Beams.
Abstract In this article we report on the application of a model-free reinforcement learning method to the optimization of accelerator systems. We simplify a policy gradient algorithm to accelerator control from sophisticated algorithms that have recently been demonstrated to solve complex dynamic problems. After outlining a theoretical basis for the functioning of the algorithm, we explore the small hyperparameter space to develop intuition about said parameters using a simple number-guess environment.</description></item><item><title>Model-free and Bayesian Ensembling Model-based Deep Reinforcement Learning for Particle Accelerator Control Demonstrated on the FERMI FEL</title><link>https://RL4aa.github.io/posts/hirlaender2020modelfree/</link><pubDate>Thu, 17 Dec 2020 00:00:00 +0000</pubDate><guid>https://RL4aa.github.io/posts/hirlaender2020modelfree/</guid><description>S. Hirlaender, N. Bruchon. University of Salzburg, University of Trieste. arXiv.
Abstract Reinforcement learning holds tremendous promise in accelerator controls. The primary goal of this paper is to show how this approach can be utilised on an operational level on accelerator physics problems. Despite the success of model-free reinforcement learning in several domains, sample-efficiency still is a bottle-neck, which might be encompassed by model-based methods. We compare well-suited purely model-based to model-free reinforcement learning applied to the intensity optimisation on the FERMI FEL system.</description></item><item><title>Autonomous Control of a Particle Accelerator using Deep Reinforcement Learning</title><link>https://RL4aa.github.io/posts/pang2020autonomous/</link><pubDate>Sat, 12 Dec 2020 00:00:00 +0000</pubDate><guid>https://RL4aa.github.io/posts/pang2020autonomous/</guid><description>X. Pang, S. Thulasidasan, L. Rybarcyk. Apple, Los Alamos National Laboratory. Machine Learning for Engineering Modeling, Simulation, and Design Workshop at Neural Information Processing Systems 2020.
Abstract We describe an approach to learning optimal control policies for a large, linear particle accelerator using deep reinforcement learning coupled with a high-fidelity physics engine. The framework consists of an AI controller that uses deep neural networks for state and action-space representation and learns optimal policies using reward signals that are provided by the physics simulator.</description></item><item><title>Sample-efficient reinforcement learning for CERN accelerator control</title><link>https://RL4aa.github.io/posts/kain2020sampleefficient/</link><pubDate>Tue, 01 Dec 2020 00:00:00 +0000</pubDate><guid>https://RL4aa.github.io/posts/kain2020sampleefficient/</guid><description>V. Kain, S. Hirlander, B. Goddard, F. M. Velotti, G. Z. Della Porta, N. Bruchon, G. Valentino. CERN, University of Trieste, University of Malta. Physical Review Accelerators and Beams.
Abstract Numerical optimization algorithms are already established tools to increase and stabilize the performance of particle accelerators. These algorithms have many advantages, are available out of the box, and can be adapted to a wide range of optimization problems in accelerator operation.</description></item><item><title>Basic Reinforcement Learning Techniques to Control the Intensity of a Seeded Free-Electron Laser</title><link>https://RL4aa.github.io/posts/bruchon2020basic/</link><pubDate>Sat, 09 May 2020 00:00:00 +0000</pubDate><guid>https://RL4aa.github.io/posts/bruchon2020basic/</guid><description>N. Bruchon, G. Fenu, G. Gaio, M. Lonza, F. H. O’Shea, F. A. Pellegrino, E. Salvato. University of Trieste, Elettra Sincrotrone Trieste. Electronics.
Abstract Optimal tuning of particle accelerators is a challenging task. Many different approaches have been proposed in the past to solve two main problems—attainment of an optimal working point and performance recovery after machine drifts. The most classical model-free techniques (e.g., Gradient Ascent or Extremum Seeking algorithms) have some intrinsic limitations.</description></item><item><title>Toward the Application of Reinforcement Learning to the Intensity Control of a Seeded Free-Electron Laser</title><link>https://RL4aa.github.io/posts/bruchon2019toward/</link><pubDate>Wed, 23 Oct 2019 00:00:00 +0000</pubDate><guid>https://RL4aa.github.io/posts/bruchon2019toward/</guid><description>N. Bruchon, G. Fenu, G. Gaio, M. Lonza, F. A. Pellegrino, E. Salvato. University of Trieste. 23rd International Conference on Mechatronics Technology.
Abstract The optimization of particle accelerators is a challenging task, and many different approaches have been proposed in years, to obtain an optimal tuning of the plant and to keep it optimally tuned despite drifts or disturbances. Indeed, the classical model-free approaches (such as Gradient Ascent or Extremum Seeking algorithms) have intrinsic limitations.</description></item><item><title>Feedback Design for Control of the Micro-Bunching Instability Based on Reinforcement Learning</title><link>https://RL4aa.github.io/posts/boltz2019feedback/</link><pubDate>Sun, 19 May 2019 00:00:00 +0000</pubDate><guid>https://RL4aa.github.io/posts/boltz2019feedback/</guid><description>T. Boltz, M. Brosi, E. Bründermann, B. Haerer, P. Kaiser, C. Pohl, P. Schreiber, M. Yan,T. Asfour, A.-S. Müller. Karlsruhe Insitute of Technology KIT. 10th International Particle Accelerator Conference.
Abstract The operation of ring-based synchrotron light sourceswith short electron bunches increases the emission of co-herent synchrotron radiation (CSR) in the THz frequencyrange. However, the micro-bunching instability resultingfrom self-interaction of the bunch with its own radiationfield limits stable operation with constant intensity of CSRemission to a particular threshold current.</description></item></channel></rss>